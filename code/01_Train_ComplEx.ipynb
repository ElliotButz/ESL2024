{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQDup52LepGf"
      },
      "source": [
        "# Imports, install and mount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIJjGMol68d"
      },
      "source": [
        "<!--  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IdMkskBXNLUF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (5.2.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "# ! pip install cuda\n",
        "# ! pip install torch_geometric\n",
        "# ! pip install nxontology\n",
        "# ! pip install tensordict\n",
        "# ! pip install pandas\n",
        "# ! pip install tensorflow\n",
        "# ! pip install scipy\n",
        "# ! pip install matplotlib\n",
        "\n",
        "# ! pip3 install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import ComplEx\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zsSHTMaQjRm"
      },
      "source": [
        "# Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZUA9xMQQjRm",
        "outputId": "074d9313-168f-444d-c817-4bb87dc9d0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# ComplEx embeddings :\n",
        "\n",
        "hidden_channels = 220\n",
        "batch_size = 4096\n",
        "epochs = 1000\n",
        "\n",
        "params_save_name = f\"PARAMS_ComplEx_HC_6_times_{hidden_channels}_on_full_Os_GO\"\n",
        "params_save_path = \"/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
        "\n",
        "datasets_save_path = '/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/dataset_BS_'+str(batch_size)\n",
        "val_path = datasets_save_path + 'VAL' +  '.pickle'\n",
        "test_path = datasets_save_path + 'TEST' +  '.pickle'\n",
        "train_path = datasets_save_path + 'TRAIN' +  '.pickle'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "print(device)\n",
        "\n",
        "# wandb.init(\n",
        "#     settings=wandb.Settings(start_method=\"fork\"),\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"ComplEx on Os_to_GO_iric\",\n",
        "    \n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"architecture\": \"ComplEx\",\n",
        "#     \"dataset\": \"genes_to_phenotypes_iric.tsv\",\n",
        "#     \"epochs\": epochs,\n",
        "#     'hidden_channels' : hidden_channels,\n",
        "#     'batch_size' : batch_size\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dataset_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mData\u001b[38;5;241m.\u001b[39mfrom_dict(pickle\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/dataset_TEST.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(dataset_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;66;03m#Â Should lead to a  '.pickle' file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                  )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mData:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dataset_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mData\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/storage.py:371\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1268\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1266\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1267\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1268\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1272\u001b[0m offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1205\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1201\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1205\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1206\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1207\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1208\u001b[0m     deserialized_objects[root_key] \u001b[38;5;241m=\u001b[39m typed_storage\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "def save_dataset(dataset: torch_geometric.data.data.Data, save_path: str):\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(dataset.to_dict(), f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_dataset(dataset_path: str #Â Should lead to a  '.pickle' file\n",
        "                 )-> torch_geometric.data.data.Data:\n",
        "    with open(dataset_path, 'rb') as f:\n",
        "        return torch_geometric.data.data.Data.from_dict(pickle.load(f))\n",
        "    torch.load with map_location=torch.device('cpu')\n",
        "    \n",
        "load_dataset('/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/dataset_TEST.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVemQRTkczW7"
      },
      "source": [
        "## Building init vars for Data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Zh3HoEc6cU",
        "outputId": "eb445852-a342-46d2-98a4-ce447df0c8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EDGE INDEX : \n",
            " tensor([[24385, 24385, 24385,  ..., 13194,  3131,  3131],\n",
            "        [26614, 25106,  6930,  ..., 26477,  6836, 11218]])\n",
            "\n",
            "EDGES ATTRIBUTES : \n",
            " tensor([0, 0, 0,  ..., 0, 0, 0])\n",
            "\n",
            "Dataset looks valid ? \n",
            " True\n"
          ]
        }
      ],
      "source": [
        "# # Initial nodes states :\n",
        "# x = torch.ones(len(entity_set), 1)  # Chaque nÅud a 1 pour Ã©tat initial\n",
        "# print('X : \\n',x)\n",
        "\n",
        "# Edges index\n",
        "heads = list(iric['mapped_subject'])\n",
        "tails = list(iric['mapped_object'])\n",
        "edge_index = torch.tensor([heads,tails], dtype=torch.long)\n",
        "print('\\nEDGE INDEX : \\n',edge_index)\n",
        "\n",
        "# edges states\n",
        "edge_attributes = torch.tensor(iric['mapped_predicate'])\n",
        "print('\\nEDGES ATTRIBUTES : \\n',edge_attributes)\n",
        "\n",
        "iric_pyg = Data(\n",
        "                num_nodes = len(entity_set),\n",
        "                edge_index = edge_index,\n",
        "                edge_attr = edge_attributes\n",
        "                )\n",
        "\n",
        "print(\"\\nDataset looks valid ? \\n\",iric_pyg.validate(raise_on_error=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yVnwVs4h0HE"
      },
      "source": [
        "## Setting up datas and model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWLKtXxCoBK"
      },
      "source": [
        "## Splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liA7PxgDac4i",
        "outputId": "a38fc686-f884-4147-8f6e-3101f3cc41ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch_geometric.data.data.Data'>\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "transform = RandomLinkSplit(\n",
        "                            num_val = 0.1,\n",
        "                            num_test = 0.1,\n",
        "                            is_undirected=False,\n",
        "                            add_negative_train_samples=False,\n",
        "                            )\n",
        "\n",
        "train_data, val_data, test_data = transform(iric_pyg)\n",
        "\n",
        "# print(f\"Hole Dataset :\\n {iric_pyg}\\n\\nTrain:\\n{train_data}\\n\\nTest :\\n{test_data}\\n\\nValidation :\\n{val_data}\")\n",
        "# # Il ne faut pas regarder num_edges parce que RLS cache les arÃªtes mais ne les sort pas du graph.\n",
        "# # print(f\"Number of edges in datasets : \\n  Hole Dataset : {iric_pyg.num_edges}\\n\\n  Train: {train_data.num_edges}\\n\\n  Test : {test_data.num_edges}\\n\\n  Validation : {val_data.num_edges}\")\n",
        "# print(f\"Number of edges in datasets : \\n  Train: {list(train_data.edge_label.size())[0]}\\n\\n  Test : {list(test_data.edge_label.size())[0]}\\n\\n  Validation : {list(val_data.edge_label.size())[0]}\")\n",
        "\n",
        "train_data = train_data.to(device)\n",
        "val_data = val_data.to(device)\n",
        "test_data = test_data.to(device)\n",
        "\n",
        "print(type(train_data))\n",
        "\n",
        "# print('\\n\\n',train_data.num_nodes)\n",
        "# print(train_data.num_edge_types)\n",
        "# print(train_data.__dict__)\n",
        "# print(train_data.edge_index[0].size())\n",
        "# print(train_data.edge_index[1].size())\n",
        "# print(train_data.edge_attr.size())\n",
        "# print(train_data.edge_attr)\n",
        "# print(train_data.num_nodes)\n",
        "# print(train_data.edge_index.size()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataset(dataset: torch_geometric.data.data.Data, save_path: str):\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(dataset.to_dict(), f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_dataset(dataset_path: str #Â Should lead to a  '.pickle' file\n",
        "                 )-> torch_geometric.data.data.Data:\n",
        "    with open(dataset_path, 'rb') as f:\n",
        "        return torch_geometric.data.data.Data.from_dict(pickle.load(f))\n",
        "    \n",
        "save_dataset(dataset=val_data, save_path=val_path)\n",
        "save_dataset(dataset=test_data, save_path=test_path)\n",
        "save_dataset(dataset=train_data, save_path=train_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmfPJcuTi3Ul"
      },
      "source": [
        "## Iniating models and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cQ3Oi0vsQjRr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4096\n"
          ]
        }
      ],
      "source": [
        "#Â Initiating models\n",
        "\n",
        "complex_model = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "#Â Initiaing loader\n",
        "head_index = train_data.edge_index[0]\n",
        "tail_index = train_data.edge_index[1]\n",
        "rel_type = train_data.edge_attr\n",
        "\n",
        "loader = complex_model.loader(\n",
        "    head_index = head_index,\n",
        "    tail_index = tail_index,\n",
        "    rel_type = rel_type,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# initiating optimizers\n",
        "complex_optimizer = optim.Adam(complex_model.parameters())\n",
        "\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(data, model):\n",
        "    model.eval()\n",
        "    return model.test(\n",
        "        head_index=test_data.edge_index[0],\n",
        "        tail_index=test_data.edge_index[1],\n",
        "        rel_type=test_data.edge_attr,\n",
        "        batch_size=batch_size,\n",
        "        k=10,\n",
        "    )\n",
        "\n",
        "def train(model, optimizer):\n",
        "    model.train()\n",
        "    total_loss = total_examples = 0\n",
        "    for head_index, rel_type, tail_index in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(head_index, rel_type, tail_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * head_index.numel()\n",
        "        total_examples += head_index.numel()\n",
        "    return total_loss / total_examples\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "    plt.plot(loss_list, label='Loss')\n",
        "    plt.title('Evolution des Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_log(loss_list):\n",
        "    plt.semilogy(loss_list, label='Loss')\n",
        "    plt.title('Evolution des Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def running_mean(list,\n",
        "                 half_window: int # Number of elements that the function will consider\n",
        "                                  # ahead and behind the position X to calculate running mean at X.\n",
        "                 ):\n",
        "    running_means = []\n",
        "\n",
        "    for i in range(0,len(list)):\n",
        "\n",
        "        left_bound = max(0,i-half_window)\n",
        "        right_bound = min(len(list)-1, i + half_window)\n",
        "        sublist = list[left_bound:right_bound+1]\n",
        "        running_means.append(sum(sublist)/len(sublist))\n",
        "\n",
        "    return running_means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv6TIchaQjRr",
        "outputId": "12dd55ba-9b78-4b6a-d457-d4bb97cc8688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Loss: 0.6931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 152324/152324 [15:32<00:00, 163.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Val Mean Rank: 8673.97 Val MRR: 0.0084, Val Hits@10: 0.0152\n",
            "Epoch: 001, Loss: 0.6931\n",
            "Epoch: 002, Loss: 0.6919\n",
            "Epoch: 003, Loss: 0.6790\n",
            "Epoch: 004, Loss: 0.6337\n",
            "Epoch: 005, Loss: 0.5650\n",
            "Epoch: 006, Loss: 0.5067\n",
            "Epoch: 007, Loss: 0.4716\n",
            "Epoch: 008, Loss: 0.4505\n",
            "Epoch: 009, Loss: 0.4338\n",
            "Epoch: 010, Loss: 0.4214\n",
            "Epoch: 011, Loss: 0.4078\n",
            "Epoch: 012, Loss: 0.3963\n",
            "Epoch: 013, Loss: 0.3798\n",
            "Epoch: 014, Loss: 0.3634\n",
            "Epoch: 015, Loss: 0.3454\n",
            "Epoch: 016, Loss: 0.3260\n",
            "Epoch: 017, Loss: 0.3041\n",
            "Epoch: 018, Loss: 0.2823\n",
            "Epoch: 019, Loss: 0.2609\n",
            "Epoch: 020, Loss: 0.2403\n",
            "Epoch: 021, Loss: 0.2203\n",
            "Epoch: 022, Loss: 0.2039\n",
            "Epoch: 023, Loss: 0.1869\n",
            "Epoch: 024, Loss: 0.1727\n",
            "Epoch: 025, Loss: 0.1618\n",
            "Epoch: 026, Loss: 0.1496\n",
            "Epoch: 027, Loss: 0.1387\n",
            "Epoch: 028, Loss: 0.1305\n",
            "Epoch: 029, Loss: 0.1216\n",
            "Epoch: 030, Loss: 0.1159\n",
            "Epoch: 031, Loss: 0.1104\n",
            "Epoch: 032, Loss: 0.1032\n",
            "Epoch: 033, Loss: 0.0997\n",
            "Epoch: 034, Loss: 0.0942\n",
            "Epoch: 035, Loss: 0.0915\n",
            "Epoch: 036, Loss: 0.0873\n",
            "Epoch: 037, Loss: 0.0825\n",
            "Epoch: 038, Loss: 0.0806\n",
            "Epoch: 039, Loss: 0.0781\n",
            "Epoch: 040, Loss: 0.0760\n",
            "Epoch: 041, Loss: 0.0736\n",
            "Epoch: 042, Loss: 0.0713\n",
            "Epoch: 043, Loss: 0.0698\n",
            "Epoch: 044, Loss: 0.0681\n",
            "Epoch: 045, Loss: 0.0657\n",
            "Epoch: 046, Loss: 0.0663\n",
            "Epoch: 047, Loss: 0.0647\n",
            "Epoch: 048, Loss: 0.0631\n",
            "Epoch: 049, Loss: 0.0619\n",
            "Epoch: 050, Loss: 0.0602\n",
            "Epoch: 051, Loss: 0.0598\n",
            "Epoch: 052, Loss: 0.0577\n",
            "Epoch: 053, Loss: 0.0579\n",
            "Epoch: 054, Loss: 0.0575\n",
            "Epoch: 055, Loss: 0.0555\n",
            "Epoch: 056, Loss: 0.0551\n",
            "Epoch: 057, Loss: 0.0548\n",
            "Epoch: 058, Loss: 0.0542\n",
            "Epoch: 059, Loss: 0.0535\n",
            "Epoch: 060, Loss: 0.0527\n",
            "Epoch: 061, Loss: 0.0529\n",
            "Epoch: 062, Loss: 0.0519\n",
            "Epoch: 063, Loss: 0.0513\n",
            "Epoch: 064, Loss: 0.0512\n",
            "Epoch: 065, Loss: 0.0504\n",
            "Epoch: 066, Loss: 0.0503\n",
            "Epoch: 067, Loss: 0.0501\n",
            "Epoch: 068, Loss: 0.0488\n",
            "Epoch: 069, Loss: 0.0496\n",
            "Epoch: 070, Loss: 0.0480\n",
            "Epoch: 071, Loss: 0.0475\n",
            "Epoch: 072, Loss: 0.0484\n",
            "Epoch: 073, Loss: 0.0486\n",
            "Epoch: 074, Loss: 0.0462\n",
            "Epoch: 075, Loss: 0.0468\n",
            "Epoch: 076, Loss: 0.0452\n",
            "Epoch: 077, Loss: 0.0470\n",
            "Epoch: 078, Loss: 0.0472\n",
            "Epoch: 079, Loss: 0.0463\n",
            "Epoch: 080, Loss: 0.0466\n",
            "Epoch: 081, Loss: 0.0460\n",
            "Epoch: 082, Loss: 0.0446\n",
            "Epoch: 083, Loss: 0.0445\n",
            "Epoch: 084, Loss: 0.0448\n",
            "Epoch: 085, Loss: 0.0459\n",
            "Epoch: 086, Loss: 0.0451\n",
            "Epoch: 087, Loss: 0.0449\n",
            "Epoch: 088, Loss: 0.0464\n",
            "Epoch: 089, Loss: 0.0442\n",
            "Epoch: 090, Loss: 0.0444\n",
            "Epoch: 091, Loss: 0.0439\n",
            "Epoch: 092, Loss: 0.0436\n",
            "Epoch: 093, Loss: 0.0430\n",
            "Epoch: 094, Loss: 0.0431\n",
            "Epoch: 095, Loss: 0.0429\n",
            "Epoch: 096, Loss: 0.0432\n",
            "Epoch: 097, Loss: 0.0432\n",
            "Epoch: 098, Loss: 0.0430\n",
            "Epoch: 099, Loss: 0.0428\n",
            "Epoch: 100, Loss: 0.0424\n",
            "Epoch: 101, Loss: 0.0425\n",
            "Epoch: 102, Loss: 0.0414\n",
            "Epoch: 103, Loss: 0.0414\n",
            "Epoch: 104, Loss: 0.0414\n",
            "Epoch: 105, Loss: 0.0440\n",
            "Epoch: 106, Loss: 0.0418\n",
            "Epoch: 107, Loss: 0.0414\n",
            "Epoch: 108, Loss: 0.0419\n",
            "Epoch: 109, Loss: 0.0413\n",
            "Epoch: 110, Loss: 0.0409\n",
            "Epoch: 111, Loss: 0.0412\n",
            "Epoch: 112, Loss: 0.0417\n",
            "Epoch: 113, Loss: 0.0421\n",
            "Epoch: 114, Loss: 0.0427\n",
            "Epoch: 115, Loss: 0.0416\n",
            "Epoch: 116, Loss: 0.0415\n",
            "Epoch: 117, Loss: 0.0412\n",
            "Epoch: 118, Loss: 0.0411\n",
            "Epoch: 119, Loss: 0.0405\n",
            "Epoch: 120, Loss: 0.0427\n",
            "Epoch: 121, Loss: 0.0397\n",
            "Epoch: 122, Loss: 0.0420\n",
            "Epoch: 123, Loss: 0.0396\n",
            "Epoch: 124, Loss: 0.0405\n",
            "Epoch: 125, Loss: 0.0403\n",
            "Epoch: 126, Loss: 0.0397\n",
            "Epoch: 127, Loss: 0.0399\n",
            "Epoch: 128, Loss: 0.0402\n",
            "Epoch: 129, Loss: 0.0401\n",
            "Epoch: 130, Loss: 0.0399\n",
            "Epoch: 131, Loss: 0.0420\n",
            "Epoch: 132, Loss: 0.0394\n",
            "Epoch: 133, Loss: 0.0410\n",
            "Epoch: 134, Loss: 0.0401\n",
            "Epoch: 135, Loss: 0.0410\n",
            "Epoch: 136, Loss: 0.0418\n",
            "Epoch: 137, Loss: 0.0395\n",
            "Epoch: 138, Loss: 0.0403\n",
            "Epoch: 139, Loss: 0.0399\n",
            "Epoch: 140, Loss: 0.0396\n",
            "Epoch: 141, Loss: 0.0395\n",
            "Epoch: 142, Loss: 0.0398\n",
            "Epoch: 143, Loss: 0.0396\n",
            "Epoch: 144, Loss: 0.0395\n",
            "Epoch: 145, Loss: 0.0398\n",
            "Epoch: 146, Loss: 0.0394\n",
            "Epoch: 147, Loss: 0.0408\n",
            "Epoch: 148, Loss: 0.0393\n",
            "Epoch: 149, Loss: 0.0385\n",
            "Epoch: 150, Loss: 0.0385\n",
            "Epoch: 151, Loss: 0.0394\n",
            "Epoch: 152, Loss: 0.0391\n",
            "Epoch: 153, Loss: 0.0386\n",
            "Epoch: 154, Loss: 0.0383\n",
            "Epoch: 155, Loss: 0.0392\n",
            "Epoch: 156, Loss: 0.0395\n",
            "Epoch: 157, Loss: 0.0392\n",
            "Epoch: 158, Loss: 0.0390\n",
            "Epoch: 159, Loss: 0.0381\n",
            "Epoch: 160, Loss: 0.0380\n",
            "Epoch: 161, Loss: 0.0393\n",
            "Epoch: 162, Loss: 0.0394\n",
            "Epoch: 163, Loss: 0.0386\n",
            "Epoch: 164, Loss: 0.0395\n",
            "Epoch: 165, Loss: 0.0365\n",
            "Epoch: 166, Loss: 0.0391\n",
            "Epoch: 167, Loss: 0.0392\n",
            "Epoch: 168, Loss: 0.0388\n",
            "Epoch: 169, Loss: 0.0389\n",
            "Epoch: 170, Loss: 0.0385\n",
            "Epoch: 171, Loss: 0.0396\n",
            "Epoch: 172, Loss: 0.0379\n",
            "Epoch: 173, Loss: 0.0381\n",
            "Epoch: 174, Loss: 0.0380\n",
            "Epoch: 175, Loss: 0.0388\n",
            "Epoch: 176, Loss: 0.0383\n",
            "Epoch: 177, Loss: 0.0380\n",
            "Epoch: 178, Loss: 0.0384\n",
            "Epoch: 179, Loss: 0.0390\n",
            "Epoch: 180, Loss: 0.0381\n",
            "Epoch: 181, Loss: 0.0380\n",
            "Epoch: 182, Loss: 0.0376\n",
            "Epoch: 183, Loss: 0.0387\n",
            "Epoch: 184, Loss: 0.0374\n",
            "Epoch: 185, Loss: 0.0390\n",
            "Epoch: 186, Loss: 0.0378\n",
            "Epoch: 187, Loss: 0.0383\n",
            "Epoch: 188, Loss: 0.0396\n",
            "Epoch: 189, Loss: 0.0384\n",
            "Epoch: 190, Loss: 0.0387\n",
            "Epoch: 191, Loss: 0.0381\n",
            "Epoch: 192, Loss: 0.0381\n",
            "Epoch: 193, Loss: 0.0382\n",
            "Epoch: 194, Loss: 0.0368\n",
            "Epoch: 195, Loss: 0.0384\n",
            "Epoch: 196, Loss: 0.0379\n",
            "Epoch: 197, Loss: 0.0385\n",
            "Epoch: 198, Loss: 0.0376\n",
            "Epoch: 199, Loss: 0.0375\n",
            "Epoch: 200, Loss: 0.0375\n",
            "Epoch: 201, Loss: 0.0382\n",
            "Epoch: 202, Loss: 0.0374\n",
            "Epoch: 203, Loss: 0.0363\n",
            "Epoch: 204, Loss: 0.0381\n",
            "Epoch: 205, Loss: 0.0379\n",
            "Epoch: 206, Loss: 0.0373\n",
            "Epoch: 207, Loss: 0.0368\n",
            "Epoch: 208, Loss: 0.0379\n",
            "Epoch: 209, Loss: 0.0383\n",
            "Epoch: 210, Loss: 0.0376\n",
            "Epoch: 211, Loss: 0.0372\n",
            "Epoch: 212, Loss: 0.0367\n",
            "Epoch: 213, Loss: 0.0378\n",
            "Epoch: 214, Loss: 0.0363\n",
            "Epoch: 215, Loss: 0.0373\n",
            "Epoch: 216, Loss: 0.0382\n",
            "Epoch: 217, Loss: 0.0372\n",
            "Epoch: 218, Loss: 0.0383\n",
            "Epoch: 219, Loss: 0.0376\n",
            "Epoch: 220, Loss: 0.0368\n",
            "Epoch: 221, Loss: 0.0379\n",
            "Epoch: 222, Loss: 0.0365\n",
            "Epoch: 223, Loss: 0.0363\n",
            "Epoch: 224, Loss: 0.0372\n",
            "Epoch: 225, Loss: 0.0374\n",
            "Epoch: 226, Loss: 0.0370\n",
            "Epoch: 227, Loss: 0.0373\n",
            "Epoch: 228, Loss: 0.0380\n",
            "Epoch: 229, Loss: 0.0364\n",
            "Epoch: 230, Loss: 0.0378\n",
            "Epoch: 231, Loss: 0.0363\n",
            "Epoch: 232, Loss: 0.0362\n",
            "Epoch: 233, Loss: 0.0364\n",
            "Epoch: 234, Loss: 0.0380\n",
            "Epoch: 235, Loss: 0.0368\n",
            "Epoch: 236, Loss: 0.0369\n",
            "Epoch: 237, Loss: 0.0374\n",
            "Epoch: 238, Loss: 0.0371\n",
            "Epoch: 239, Loss: 0.0369\n",
            "Epoch: 240, Loss: 0.0373\n",
            "Epoch: 241, Loss: 0.0371\n",
            "Epoch: 242, Loss: 0.0380\n",
            "Epoch: 243, Loss: 0.0378\n",
            "Epoch: 244, Loss: 0.0360\n",
            "Epoch: 245, Loss: 0.0359\n",
            "Epoch: 246, Loss: 0.0363\n",
            "Epoch: 247, Loss: 0.0391\n",
            "Epoch: 248, Loss: 0.0371\n",
            "Epoch: 249, Loss: 0.0374\n",
            "Epoch: 250, Loss: 0.0372\n",
            "Epoch: 251, Loss: 0.0372\n",
            "Epoch: 252, Loss: 0.0369\n",
            "Epoch: 253, Loss: 0.0364\n",
            "Epoch: 254, Loss: 0.0361\n",
            "Epoch: 255, Loss: 0.0373\n",
            "Epoch: 256, Loss: 0.0354\n",
            "Epoch: 257, Loss: 0.0373\n",
            "Epoch: 258, Loss: 0.0364\n",
            "Epoch: 259, Loss: 0.0372\n",
            "Epoch: 260, Loss: 0.0372\n",
            "Epoch: 261, Loss: 0.0362\n",
            "Epoch: 262, Loss: 0.0372\n",
            "Epoch: 263, Loss: 0.0376\n",
            "Epoch: 264, Loss: 0.0366\n",
            "Epoch: 265, Loss: 0.0364\n",
            "Epoch: 266, Loss: 0.0338\n",
            "Epoch: 267, Loss: 0.0362\n",
            "Epoch: 268, Loss: 0.0356\n",
            "Epoch: 269, Loss: 0.0374\n",
            "Epoch: 270, Loss: 0.0367\n",
            "Epoch: 271, Loss: 0.0371\n",
            "Epoch: 272, Loss: 0.0366\n",
            "Epoch: 273, Loss: 0.0376\n",
            "Epoch: 274, Loss: 0.0373\n",
            "Epoch: 275, Loss: 0.0370\n",
            "Epoch: 276, Loss: 0.0366\n",
            "Epoch: 277, Loss: 0.0373\n",
            "Epoch: 278, Loss: 0.0364\n",
            "Epoch: 279, Loss: 0.0366\n",
            "Epoch: 280, Loss: 0.0363\n",
            "Epoch: 281, Loss: 0.0367\n",
            "Epoch: 282, Loss: 0.0376\n",
            "Epoch: 283, Loss: 0.0373\n",
            "Epoch: 284, Loss: 0.0369\n",
            "Epoch: 285, Loss: 0.0351\n",
            "Epoch: 286, Loss: 0.0364\n",
            "Epoch: 287, Loss: 0.0356\n",
            "Epoch: 288, Loss: 0.0349\n",
            "Epoch: 289, Loss: 0.0367\n",
            "Epoch: 290, Loss: 0.0370\n",
            "Epoch: 291, Loss: 0.0366\n",
            "Epoch: 292, Loss: 0.0353\n",
            "Epoch: 293, Loss: 0.0362\n",
            "Epoch: 294, Loss: 0.0356\n",
            "Epoch: 295, Loss: 0.0366\n",
            "Epoch: 296, Loss: 0.0358\n",
            "Epoch: 297, Loss: 0.0371\n",
            "Epoch: 298, Loss: 0.0366\n",
            "Epoch: 299, Loss: 0.0354\n",
            "Epoch: 300, Loss: 0.0368\n",
            "Epoch: 301, Loss: 0.0367\n",
            "Epoch: 302, Loss: 0.0366\n",
            "Epoch: 303, Loss: 0.0369\n",
            "Epoch: 304, Loss: 0.0368\n",
            "Epoch: 305, Loss: 0.0368\n",
            "Epoch: 306, Loss: 0.0375\n",
            "Epoch: 307, Loss: 0.0358\n",
            "Epoch: 308, Loss: 0.0361\n",
            "Epoch: 309, Loss: 0.0370\n",
            "Epoch: 310, Loss: 0.0364\n",
            "Epoch: 311, Loss: 0.0357\n",
            "Epoch: 312, Loss: 0.0357\n",
            "Epoch: 313, Loss: 0.0371\n",
            "Epoch: 314, Loss: 0.0358\n",
            "Epoch: 315, Loss: 0.0367\n",
            "Epoch: 316, Loss: 0.0363\n",
            "Epoch: 317, Loss: 0.0357\n",
            "Epoch: 318, Loss: 0.0367\n",
            "Epoch: 319, Loss: 0.0354\n",
            "Epoch: 320, Loss: 0.0358\n",
            "Epoch: 321, Loss: 0.0369\n",
            "Epoch: 322, Loss: 0.0364\n",
            "Epoch: 323, Loss: 0.0362\n",
            "Epoch: 324, Loss: 0.0355\n",
            "Epoch: 325, Loss: 0.0365\n",
            "Epoch: 326, Loss: 0.0357\n",
            "Epoch: 327, Loss: 0.0359\n",
            "Epoch: 328, Loss: 0.0356\n",
            "Epoch: 329, Loss: 0.0368\n",
            "Epoch: 330, Loss: 0.0356\n",
            "Epoch: 331, Loss: 0.0367\n",
            "Epoch: 332, Loss: 0.0349\n",
            "Epoch: 333, Loss: 0.0365\n",
            "Epoch: 334, Loss: 0.0367\n",
            "Epoch: 335, Loss: 0.0359\n",
            "Epoch: 336, Loss: 0.0353\n",
            "Epoch: 337, Loss: 0.0348\n",
            "Epoch: 338, Loss: 0.0371\n",
            "Epoch: 339, Loss: 0.0349\n",
            "Epoch: 340, Loss: 0.0361\n",
            "Epoch: 341, Loss: 0.0356\n",
            "Epoch: 342, Loss: 0.0353\n",
            "Epoch: 343, Loss: 0.0374\n",
            "Epoch: 344, Loss: 0.0364\n",
            "Epoch: 345, Loss: 0.0361\n",
            "Epoch: 346, Loss: 0.0362\n",
            "Epoch: 347, Loss: 0.0353\n",
            "Epoch: 348, Loss: 0.0366\n",
            "Epoch: 349, Loss: 0.0368\n",
            "Epoch: 350, Loss: 0.0362\n",
            "Epoch: 351, Loss: 0.0358\n",
            "Epoch: 352, Loss: 0.0360\n",
            "Epoch: 353, Loss: 0.0368\n",
            "Epoch: 354, Loss: 0.0348\n",
            "Epoch: 355, Loss: 0.0359\n",
            "Epoch: 356, Loss: 0.0353\n",
            "Epoch: 357, Loss: 0.0371\n",
            "Epoch: 358, Loss: 0.0367\n",
            "Epoch: 359, Loss: 0.0362\n",
            "Epoch: 360, Loss: 0.0354\n",
            "Epoch: 361, Loss: 0.0359\n",
            "Epoch: 362, Loss: 0.0362\n",
            "Epoch: 363, Loss: 0.0362\n",
            "Epoch: 364, Loss: 0.0363\n",
            "Epoch: 365, Loss: 0.0355\n",
            "Epoch: 366, Loss: 0.0371\n",
            "Epoch: 367, Loss: 0.0360\n",
            "Epoch: 368, Loss: 0.0363\n",
            "Epoch: 369, Loss: 0.0364\n",
            "Epoch: 370, Loss: 0.0353\n",
            "Epoch: 371, Loss: 0.0361\n",
            "Epoch: 372, Loss: 0.0360\n",
            "Epoch: 373, Loss: 0.0349\n",
            "Epoch: 374, Loss: 0.0353\n",
            "Epoch: 375, Loss: 0.0365\n",
            "Epoch: 376, Loss: 0.0359\n",
            "Epoch: 377, Loss: 0.0361\n",
            "Epoch: 378, Loss: 0.0355\n",
            "Epoch: 379, Loss: 0.0358\n",
            "Epoch: 380, Loss: 0.0359\n",
            "Epoch: 381, Loss: 0.0364\n",
            "Epoch: 382, Loss: 0.0360\n",
            "Epoch: 383, Loss: 0.0354\n",
            "Epoch: 384, Loss: 0.0366\n",
            "Epoch: 385, Loss: 0.0343\n",
            "Epoch: 386, Loss: 0.0362\n",
            "Epoch: 387, Loss: 0.0360\n",
            "Epoch: 388, Loss: 0.0352\n",
            "Epoch: 389, Loss: 0.0357\n",
            "Epoch: 390, Loss: 0.0363\n",
            "Epoch: 391, Loss: 0.0365\n",
            "Epoch: 392, Loss: 0.0343\n",
            "Epoch: 393, Loss: 0.0367\n",
            "Epoch: 394, Loss: 0.0358\n",
            "Epoch: 395, Loss: 0.0351\n",
            "Epoch: 396, Loss: 0.0364\n",
            "Epoch: 397, Loss: 0.0357\n",
            "Epoch: 398, Loss: 0.0355\n",
            "Epoch: 399, Loss: 0.0358\n",
            "Epoch: 400, Loss: 0.0356\n",
            "Epoch: 401, Loss: 0.0358\n",
            "Epoch: 402, Loss: 0.0354\n",
            "Epoch: 403, Loss: 0.0360\n",
            "Epoch: 404, Loss: 0.0378\n",
            "Epoch: 405, Loss: 0.0357\n",
            "Epoch: 406, Loss: 0.0361\n",
            "Epoch: 407, Loss: 0.0346\n",
            "Epoch: 408, Loss: 0.0362\n",
            "Epoch: 409, Loss: 0.0349\n",
            "Epoch: 410, Loss: 0.0370\n",
            "Epoch: 411, Loss: 0.0360\n",
            "Epoch: 412, Loss: 0.0350\n",
            "Epoch: 413, Loss: 0.0354\n",
            "Epoch: 414, Loss: 0.0356\n",
            "Epoch: 415, Loss: 0.0351\n",
            "Epoch: 416, Loss: 0.0352\n",
            "Epoch: 417, Loss: 0.0359\n",
            "Epoch: 418, Loss: 0.0359\n",
            "Epoch: 419, Loss: 0.0365\n",
            "Epoch: 420, Loss: 0.0356\n",
            "Epoch: 421, Loss: 0.0362\n",
            "Epoch: 422, Loss: 0.0362\n",
            "Epoch: 423, Loss: 0.0352\n",
            "Epoch: 424, Loss: 0.0364\n",
            "Epoch: 425, Loss: 0.0356\n",
            "Epoch: 426, Loss: 0.0355\n",
            "Epoch: 427, Loss: 0.0350\n",
            "Epoch: 428, Loss: 0.0364\n",
            "Epoch: 429, Loss: 0.0358\n",
            "Epoch: 430, Loss: 0.0357\n",
            "Epoch: 431, Loss: 0.0368\n",
            "Epoch: 432, Loss: 0.0353\n",
            "Epoch: 433, Loss: 0.0359\n",
            "Epoch: 434, Loss: 0.0360\n",
            "Epoch: 435, Loss: 0.0358\n",
            "Epoch: 436, Loss: 0.0352\n",
            "Epoch: 437, Loss: 0.0361\n",
            "Epoch: 438, Loss: 0.0355\n",
            "Epoch: 439, Loss: 0.0353\n",
            "Epoch: 440, Loss: 0.0355\n",
            "Epoch: 441, Loss: 0.0354\n",
            "Epoch: 442, Loss: 0.0356\n",
            "Epoch: 443, Loss: 0.0356\n",
            "Epoch: 444, Loss: 0.0365\n",
            "Epoch: 445, Loss: 0.0358\n",
            "Epoch: 446, Loss: 0.0354\n",
            "Epoch: 447, Loss: 0.0357\n",
            "Epoch: 448, Loss: 0.0364\n",
            "Epoch: 449, Loss: 0.0358\n",
            "Epoch: 450, Loss: 0.0360\n",
            "Epoch: 451, Loss: 0.0350\n",
            "Epoch: 452, Loss: 0.0359\n",
            "Epoch: 453, Loss: 0.0361\n",
            "Epoch: 454, Loss: 0.0349\n",
            "Epoch: 455, Loss: 0.0354\n",
            "Epoch: 456, Loss: 0.0352\n",
            "Epoch: 457, Loss: 0.0349\n",
            "Epoch: 458, Loss: 0.0354\n",
            "Epoch: 459, Loss: 0.0354\n",
            "Epoch: 460, Loss: 0.0355\n",
            "Epoch: 461, Loss: 0.0358\n",
            "Epoch: 462, Loss: 0.0347\n",
            "Epoch: 463, Loss: 0.0354\n",
            "Epoch: 464, Loss: 0.0365\n",
            "Epoch: 465, Loss: 0.0358\n",
            "Epoch: 466, Loss: 0.0355\n",
            "Epoch: 467, Loss: 0.0361\n",
            "Epoch: 468, Loss: 0.0345\n",
            "Epoch: 469, Loss: 0.0349\n",
            "Epoch: 470, Loss: 0.0357\n",
            "Epoch: 471, Loss: 0.0356\n",
            "Epoch: 472, Loss: 0.0358\n",
            "Epoch: 473, Loss: 0.0358\n",
            "Epoch: 474, Loss: 0.0360\n",
            "Epoch: 475, Loss: 0.0351\n",
            "Epoch: 476, Loss: 0.0343\n",
            "Epoch: 477, Loss: 0.0342\n",
            "Epoch: 478, Loss: 0.0348\n",
            "Epoch: 479, Loss: 0.0357\n",
            "Epoch: 480, Loss: 0.0360\n",
            "Epoch: 481, Loss: 0.0352\n",
            "Epoch: 482, Loss: 0.0343\n",
            "Epoch: 483, Loss: 0.0359\n",
            "Epoch: 484, Loss: 0.0352\n",
            "Epoch: 485, Loss: 0.0359\n",
            "Epoch: 486, Loss: 0.0356\n",
            "Epoch: 487, Loss: 0.0351\n",
            "Epoch: 488, Loss: 0.0358\n",
            "Epoch: 489, Loss: 0.0351\n",
            "Epoch: 490, Loss: 0.0355\n",
            "Epoch: 491, Loss: 0.0350\n",
            "Epoch: 492, Loss: 0.0351\n",
            "Epoch: 493, Loss: 0.0355\n",
            "Epoch: 494, Loss: 0.0344\n",
            "Epoch: 495, Loss: 0.0344\n",
            "Epoch: 496, Loss: 0.0349\n",
            "Epoch: 497, Loss: 0.0355\n",
            "Epoch: 498, Loss: 0.0343\n",
            "Epoch: 499, Loss: 0.0353\n",
            "Epoch: 500, Loss: 0.0346\n",
            "Epoch: 501, Loss: 0.0355\n",
            "Epoch: 502, Loss: 0.0357\n",
            "Epoch: 503, Loss: 0.0351\n",
            "Epoch: 504, Loss: 0.0352\n",
            "Epoch: 505, Loss: 0.0358\n",
            "Epoch: 506, Loss: 0.0343\n",
            "Epoch: 507, Loss: 0.0359\n",
            "Epoch: 508, Loss: 0.0369\n",
            "Epoch: 509, Loss: 0.0353\n",
            "Epoch: 510, Loss: 0.0348\n",
            "Epoch: 511, Loss: 0.0352\n",
            "Epoch: 512, Loss: 0.0359\n",
            "Epoch: 513, Loss: 0.0366\n",
            "Epoch: 514, Loss: 0.0358\n",
            "Epoch: 515, Loss: 0.0364\n",
            "Epoch: 516, Loss: 0.0349\n",
            "Epoch: 517, Loss: 0.0349\n",
            "Epoch: 518, Loss: 0.0361\n",
            "Epoch: 519, Loss: 0.0356\n",
            "Epoch: 520, Loss: 0.0344\n",
            "Epoch: 521, Loss: 0.0353\n",
            "Epoch: 522, Loss: 0.0345\n",
            "Epoch: 523, Loss: 0.0351\n",
            "Epoch: 524, Loss: 0.0351\n",
            "Epoch: 525, Loss: 0.0355\n",
            "Epoch: 526, Loss: 0.0353\n",
            "Epoch: 527, Loss: 0.0345\n",
            "Epoch: 528, Loss: 0.0348\n",
            "Epoch: 529, Loss: 0.0347\n",
            "Epoch: 530, Loss: 0.0347\n",
            "Epoch: 531, Loss: 0.0351\n",
            "Epoch: 532, Loss: 0.0350\n",
            "Epoch: 533, Loss: 0.0345\n",
            "Epoch: 534, Loss: 0.0355\n",
            "Epoch: 535, Loss: 0.0352\n",
            "Epoch: 536, Loss: 0.0342\n",
            "Epoch: 537, Loss: 0.0344\n",
            "Epoch: 538, Loss: 0.0350\n",
            "Epoch: 539, Loss: 0.0355\n",
            "Epoch: 540, Loss: 0.0351\n",
            "Epoch: 541, Loss: 0.0354\n",
            "Epoch: 542, Loss: 0.0344\n",
            "Epoch: 543, Loss: 0.0347\n",
            "Epoch: 544, Loss: 0.0363\n",
            "Epoch: 545, Loss: 0.0355\n",
            "Epoch: 546, Loss: 0.0359\n",
            "Epoch: 547, Loss: 0.0360\n",
            "Epoch: 548, Loss: 0.0353\n",
            "Epoch: 549, Loss: 0.0366\n",
            "Epoch: 550, Loss: 0.0352\n",
            "Epoch: 551, Loss: 0.0356\n",
            "Epoch: 552, Loss: 0.0348\n",
            "Epoch: 553, Loss: 0.0340\n",
            "Epoch: 554, Loss: 0.0339\n",
            "Epoch: 555, Loss: 0.0349\n",
            "Epoch: 556, Loss: 0.0352\n",
            "Epoch: 557, Loss: 0.0353\n",
            "Epoch: 558, Loss: 0.0347\n",
            "Epoch: 559, Loss: 0.0353\n",
            "Epoch: 560, Loss: 0.0347\n",
            "Epoch: 561, Loss: 0.0346\n",
            "Epoch: 562, Loss: 0.0353\n",
            "Epoch: 563, Loss: 0.0354\n",
            "Epoch: 564, Loss: 0.0349\n",
            "Epoch: 565, Loss: 0.0357\n",
            "Epoch: 566, Loss: 0.0351\n",
            "Epoch: 567, Loss: 0.0360\n",
            "Epoch: 568, Loss: 0.0350\n",
            "Epoch: 569, Loss: 0.0348\n",
            "Epoch: 570, Loss: 0.0356\n",
            "Epoch: 571, Loss: 0.0365\n",
            "Epoch: 572, Loss: 0.0346\n",
            "Epoch: 573, Loss: 0.0358\n",
            "Epoch: 574, Loss: 0.0352\n",
            "Epoch: 575, Loss: 0.0355\n",
            "Epoch: 576, Loss: 0.0349\n",
            "Epoch: 577, Loss: 0.0339\n",
            "Epoch: 578, Loss: 0.0349\n",
            "Epoch: 579, Loss: 0.0346\n",
            "Epoch: 580, Loss: 0.0367\n",
            "Epoch: 581, Loss: 0.0348\n",
            "Epoch: 582, Loss: 0.0344\n",
            "Epoch: 583, Loss: 0.0343\n",
            "Epoch: 584, Loss: 0.0358\n",
            "Epoch: 585, Loss: 0.0348\n",
            "Epoch: 586, Loss: 0.0357\n",
            "Epoch: 587, Loss: 0.0350\n",
            "Epoch: 588, Loss: 0.0351\n",
            "Epoch: 589, Loss: 0.0345\n",
            "Epoch: 590, Loss: 0.0369\n",
            "Epoch: 591, Loss: 0.0358\n",
            "Epoch: 592, Loss: 0.0345\n",
            "Epoch: 593, Loss: 0.0352\n",
            "Epoch: 594, Loss: 0.0350\n",
            "Epoch: 595, Loss: 0.0365\n",
            "Epoch: 596, Loss: 0.0348\n",
            "Epoch: 597, Loss: 0.0346\n",
            "Epoch: 598, Loss: 0.0339\n",
            "Epoch: 599, Loss: 0.0350\n",
            "Epoch: 600, Loss: 0.0353\n",
            "Epoch: 601, Loss: 0.0347\n",
            "Epoch: 602, Loss: 0.0354\n",
            "Epoch: 603, Loss: 0.0342\n",
            "Epoch: 604, Loss: 0.0361\n",
            "Epoch: 605, Loss: 0.0355\n",
            "Epoch: 606, Loss: 0.0352\n",
            "Epoch: 607, Loss: 0.0354\n",
            "Epoch: 608, Loss: 0.0356\n",
            "Epoch: 609, Loss: 0.0339\n",
            "Epoch: 610, Loss: 0.0349\n",
            "Epoch: 611, Loss: 0.0346\n",
            "Epoch: 612, Loss: 0.0355\n",
            "Epoch: 613, Loss: 0.0350\n",
            "Epoch: 614, Loss: 0.0346\n",
            "Epoch: 615, Loss: 0.0345\n",
            "Epoch: 616, Loss: 0.0349\n",
            "Epoch: 617, Loss: 0.0353\n",
            "Epoch: 618, Loss: 0.0353\n",
            "Epoch: 619, Loss: 0.0342\n",
            "Epoch: 620, Loss: 0.0349\n",
            "Epoch: 621, Loss: 0.0343\n",
            "Epoch: 622, Loss: 0.0347\n",
            "Epoch: 623, Loss: 0.0358\n",
            "Epoch: 624, Loss: 0.0345\n",
            "Epoch: 625, Loss: 0.0348\n",
            "Epoch: 626, Loss: 0.0338\n",
            "Epoch: 627, Loss: 0.0343\n",
            "Epoch: 628, Loss: 0.0354\n",
            "Epoch: 629, Loss: 0.0349\n",
            "Epoch: 630, Loss: 0.0349\n",
            "Epoch: 631, Loss: 0.0353\n",
            "Epoch: 632, Loss: 0.0344\n",
            "Epoch: 633, Loss: 0.0345\n",
            "Epoch: 634, Loss: 0.0336\n",
            "Epoch: 635, Loss: 0.0360\n",
            "Epoch: 636, Loss: 0.0360\n",
            "Epoch: 637, Loss: 0.0352\n",
            "Epoch: 638, Loss: 0.0352\n",
            "Epoch: 639, Loss: 0.0351\n",
            "Epoch: 640, Loss: 0.0350\n",
            "Epoch: 641, Loss: 0.0342\n",
            "Epoch: 642, Loss: 0.0356\n",
            "Epoch: 643, Loss: 0.0338\n",
            "Epoch: 644, Loss: 0.0352\n",
            "Epoch: 645, Loss: 0.0349\n",
            "Epoch: 646, Loss: 0.0353\n",
            "Epoch: 647, Loss: 0.0348\n",
            "Epoch: 648, Loss: 0.0359\n",
            "Epoch: 649, Loss: 0.0342\n",
            "Epoch: 650, Loss: 0.0358\n",
            "Epoch: 651, Loss: 0.0355\n",
            "Epoch: 652, Loss: 0.0353\n",
            "Epoch: 653, Loss: 0.0349\n",
            "Epoch: 654, Loss: 0.0355\n",
            "Epoch: 655, Loss: 0.0345\n",
            "Epoch: 656, Loss: 0.0346\n",
            "Epoch: 657, Loss: 0.0342\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     10\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss})\n",
            "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m head_index\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     21\u001b[0m     total_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m head_index\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m total_examples\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "complex_model.reset_parameters()\n",
        "complex_model.to(device)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(0, epochs+1):\n",
        "    loss = train(model=complex_model, optimizer=complex_optimizer)\n",
        "    losses.append(loss)\n",
        "    wandb.log({\"loss\": loss})\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "    if epoch % epochs%1000 == 0:\n",
        "        rank, mrr, hits = test(val_data, model=complex_model)\n",
        "        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}',\n",
        "              f'Val MRR: {mrr:.4f}, Val Hits@10: {hits:.4f}')\n",
        "        wandb.log({\"Val Mean Rank\" : rank, \"Val MRR\" : mrr, \"hits@10\": hits})\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val MRR</td><td>ââ</td></tr><tr><td>Val Mean Rank</td><td>ââ</td></tr><tr><td>hits@10</td><td>ââ</td></tr><tr><td>loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val MRR</td><td>0.16405</td></tr><tr><td>Val Mean Rank</td><td>230.30151</td></tr><tr><td>hits@10</td><td>0.36778</td></tr><tr><td>loss</td><td>0.06995</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-blaze-6</strong> at: <a href='https://wandb.ai/esl2024/ComplEx%20on%20Os_to_GO_iric/runs/x3btqtqw/workspace' target=\"_blank\">https://wandb.ai/esl2024/ComplEx%20on%20Os_to_GO_iric/runs/x3btqtqw/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240327_163330-x3btqtqw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WandB finished.\n",
            "Model saved at /home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/PARAMS_ComplEx_HC_6_times_5_on_full_Os_GO\n"
          ]
        }
      ],
      "source": [
        "wandb.finish()\n",
        "print(\"WandB finished.\")\n",
        "\n",
        "torch.save(complex_model.state_dict(), params_save_path)\n",
        "print(\"Model saved at\", params_save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
