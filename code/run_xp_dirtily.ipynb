{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/miniconda3/envs/pyg2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Baseline VS LinSim VS Noise : usual labels\n",
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbutzelliot\u001b[0m (\u001b[33mesl2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebutz/ESL2024/code/wandb/run-20240425_170824-oc9kat0m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/oc9kat0m' target=\"_blank\">usual labels</a></strong> to <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/oc9kat0m' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/oc9kat0m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is homogeneous : False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/ESL2024/code/utils/train_utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_links['source_node'] = df_links.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  genes={ x=[72335, 29] },\n",
      "  go={ x=[42979, 3] },\n",
      "  po={ x=[1662, 2] },\n",
      "  traito={ x=[1636, 1] },\n",
      "  prosite_profiles={ x=[627, 1] },\n",
      "  prosite_patterns={ x=[652, 1] },\n",
      "  superfamily={ x=[957, 1] },\n",
      "  panther={ x=[5971, 1] },\n",
      "  prints={ x=[420, 1] },\n",
      "  (genes, interacts_with, genes)={ edge_index=[2, 1120849] },\n",
      "  (genes, gene_ontology, go)={ edge_index=[2, 169248] },\n",
      "  (genes, trait_ontology, traito)={ edge_index=[2, 15080] },\n",
      "  (genes, plant_ontology, po)={ edge_index=[2, 6125] },\n",
      "  (genes, profile, prosite_profiles)={ edge_index=[2, 12239] },\n",
      "  (genes, pattern, prosite_patterns)={ edge_index=[2, 6677] },\n",
      "  (genes, family, superfamily)={ edge_index=[2, 20742] },\n",
      "  (genes, panther_id, panther)={ edge_index=[2, 25319] },\n",
      "  (genes, prints_id, prints)={ edge_index=[2, 4365] },\n",
      "  (go, is_a, go)={ edge_index=[2, 136436] },\n",
      "  (po, is_a, po)={ edge_index=[2, 3548] },\n",
      "  (traito, is_a, traito)={ edge_index=[2, 3770] },\n",
      "  (go, rev_gene_ontology, genes)={ edge_index=[2, 169248] },\n",
      "  (traito, rev_trait_ontology, genes)={ edge_index=[2, 15080] },\n",
      "  (po, rev_plant_ontology, genes)={ edge_index=[2, 6125] },\n",
      "  (prosite_profiles, rev_profile, genes)={ edge_index=[2, 12239] },\n",
      "  (prosite_patterns, rev_pattern, genes)={ edge_index=[2, 6677] },\n",
      "  (superfamily, rev_family, genes)={ edge_index=[2, 20742] },\n",
      "  (panther, rev_panther_id, genes)={ edge_index=[2, 25319] },\n",
      "  (prints, rev_prints_id, genes)={ edge_index=[2, 4365] }\n",
      ")\n",
      "data look valid :  True\n",
      "Function 'split_data' executed in 0.0168s\n",
      "Function 'build_dataloaders' executed in 0.1439s\n",
      "\n",
      "Loading ontology...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Avg. Loss: 0.1406405430\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 174.8709s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Avg. Loss: 0.1406245355\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 176.5707s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Avg. Loss: 0.1406234029\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 167.9614s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Avg. Loss: 0.1406229145\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 171.0272s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Avg. Loss: 0.1406225324\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 171.9374s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Avg. Loss: 0.1406226776\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 176.0397s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Avg. Loss: 0.1406224305\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 178.6417s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Avg. Loss: 0.1406226611\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 176.0011s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Avg. Loss: 0.1406223241\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 180.3439s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Avg. Loss: 0.1406224746\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 179.1546s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Avg. Loss: 0.1406221786\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 178.4411s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:32<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Avg. Loss: 0.1406219430\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 167.8366s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Avg. Loss: 0.1406223584\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.7360s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Avg. Loss: 0.1406218681\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 167.2688s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Avg. Loss: 0.1406219263\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.4590s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Avg. Loss: 0.1406219343\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 169.6022s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Avg. Loss: 0.1406219441\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.5248s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Avg. Loss: 0.1406217709\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.5563s\n",
      "Training done.\n",
      "Function 'train' executed in 3653.7941s\n",
      "Evaluation...\n",
      "Function 'evaluate' executed in 102.4554s\n",
      "Evaluation...\n",
      "Function 'evaluate' executed in 102.0941s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>hit_at_10</td><td>▁▃▄▄▆▆▅▇▇▇▇██▇▇█▇▇</td></tr><tr><td>loss</td><td>█▄▅▄▃▃▄▄▄▃▃▃▆▁▃▃▃▃▄▄▃▃▄▃▃▅▄▄▅▄▄▃▃▂▅▃▃▄▄▁</td></tr><tr><td>val_loss</td><td>▇█▃▁▁▁▂▄▆▃▆▆▄▅▆▅▅▆</td></tr><tr><td>validationMRR</td><td>▁▃▄▄▆▆▅▇▇▇▇██▇▇█▇▇</td></tr><tr><td>validationhit_at_1</td><td>▁▃▃▄▆▆▆▇▇▇▇██▇▇█▇█</td></tr><tr><td>validationhit_at_10</td><td>▁▃▄▄▅▄▅▇▅▅▅▇▇█▆▇▇▆</td></tr><tr><td>validationhit_at_3</td><td>▁▃▃▄▅▇▅▆▆▇▇██▆▆▇▆▆</td></tr><tr><td>validationhit_at_5</td><td>▁▃▄▄▄▅▄▆▆▆▆█▇▇▇█▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.14062</td></tr><tr><td>hit_at_10</td><td>0.43368</td></tr><tr><td>loss</td><td>0.14061</td></tr><tr><td>val_loss</td><td>0.16292</td></tr><tr><td>validationMRR</td><td>0.43368</td></tr><tr><td>validationhit_at_1</td><td>0.29815</td></tr><tr><td>validationhit_at_10</td><td>0.72842</td></tr><tr><td>validationhit_at_3</td><td>0.48776</td></tr><tr><td>validationhit_at_5</td><td>0.58773</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual labels</strong> at: <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/oc9kat0m' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/oc9kat0m</a><br/> View project at: <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240425_170824-oc9kat0m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ebutz/ESL2024/code/utils' )\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from nxontology.imports import from_file\n",
    "import wandb\n",
    "from play_with_complex import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from model_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
    "import play_with_complex as pwc\n",
    "\n",
    "# How to calculate loss ?\n",
    "# Choices are : 'usual', 'rand_linsim', linsim', 'gaussian_noise', 'usual'.\n",
    "labels_type = 'usual'\n",
    "\n",
    "# How many epochs between 2 time-consuming evaluations ?\n",
    "eval_period = 1\n",
    "\n",
    "# Wandb\n",
    "xp_name = 'GNN Baseline VS LinSim VS Noise'\n",
    "run_name = f'{labels_type} labels'\n",
    "print(xp_name,':',run_name)\n",
    "\n",
    "# Iric\n",
    "iric_path = '/home/ebutz/ESL2024/data/full_iric/iric.csv'\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/full_iric/altailed_mapped_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle'\n",
    "check_dicts = True\n",
    "\n",
    "ontology_path = '/home/ebutz/ESL2024/data/go-basic.json.gz'\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "run = wandb.init(project=xp_name, name = run_name)\n",
    "config = wandb.config\n",
    "config.labels_type = labels_type\n",
    "config.val_ratio=0.1\n",
    "config.homogeneous=False\n",
    "config.scorelist_size=1000\n",
    "config.split_ratio=0.8\n",
    "config.val_ratio=0.1\n",
    "config.test_ratio=0.1\n",
    "config.num_neighbors=[70,55,13,89,85]\n",
    "config.batch_size=1024\n",
    "config.train_neg_sampling_ratio=224\n",
    "config.epochs=18\n",
    "config.disjoint_train_ratio=0.6\n",
    "config.lr=0.0015308253347932983\n",
    "config.stopper_metric= 'hit_at_10'\n",
    "config.stopper_direction=\"maximize\"\n",
    "config.stopper_patience=5\n",
    "config.stopper_frequency=1\n",
    "config.stopper_relative_delta=0.05\n",
    "config.gamma=1.3\n",
    "config.alpha=0.42680473078813763\n",
    "config.gnn_layer='ResGatedGraphConv'\n",
    "config.dropout=0.1\n",
    "config.norm='DiffGroupNorm'\n",
    "config.aggregation = 'min'\n",
    "config.hidden_channels=115\n",
    "config.num_layers=3\n",
    "config.attention_heads=4\n",
    "config.homogeneous = False\n",
    "config.labels = {'head' : 'genes', 'relation' : 'gene_ontology', 'tail' : 'go'}\n",
    "\n",
    "print('Graph is homogeneous :',config.homogeneous)\n",
    "\n",
    "data = load_iric_data('/home/ebutz/ESL2024/data/full_iric/iric.csv', featureless=False)\n",
    "data  = T.ToUndirected(merge=True)(data) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "data = T.RemoveDuplicatedEdges()(data) # Remove duplicated edges\n",
    "print(data)\n",
    "print('data look valid : ',data.validate())\n",
    "\n",
    "train_data, val_data, test_data = split_data(data, config)\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data, val_data, test_data, config)\n",
    "\n",
    "# # ------------- Loading ontology ------------- #\n",
    "\n",
    "print(\"\\nLoading ontology...\")\n",
    "nxo = from_file(ontology_path)\n",
    "nxo.freeze()\n",
    "pwc.nxo = nxo\n",
    "\n",
    "df = pd.read_csv(iric_path, index_col=0, dtype=str)\n",
    "df['source_node'] = df.index\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "go_map = get_nodelist(df, 'go')\n",
    "go_to_idx = {node: i for i, node in enumerate(go_map)}\n",
    "idx_to_go = {i: node for i, node in enumerate(go_map)}\n",
    "\n",
    "# # ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = idx_to_go\n",
    "pwc.mapped_alt_tails = None\n",
    "pwc.device           = device\n",
    "\n",
    "# ------------- Model setup ------------- #\n",
    "\n",
    "gnn_layers = get_gnn_layers(config)\n",
    "norm_layers = get_norm_layers(config, len(data.node_types))\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "\n",
    "# ----------------- Loops ----------------- #\n",
    "@timer_func\n",
    "def evaluate(config, loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model.    \n",
    "    loader : DataLoader\n",
    "        The data loader to evaluate the model on.\n",
    "    model : Model\n",
    "        The model to evaluate.\n",
    "    criterion : callable\n",
    "        The loss function to use for evaluation.\n",
    "    compute_all_metrics : bool, optional\n",
    "        Whether to compute all metrics or partially. Default is False.\n",
    "    loader_type : str, optional\n",
    "        The type of the loader ('validation' or 'test'). Default is 'validation'.\n",
    "    stopper_metric : str or bool, optional\n",
    "        The metric that dictates when to stop training. If False, only the loss is computed. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of the evaluation metric and the loss, depending on the value of `stopper_metric`.\n",
    "        If compute_all_metrics is True, the function logs all metrics to W&B.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluation...\")\n",
    "    model.eval()\n",
    "    num_neg_samples = loader.neg_sampling.amount\n",
    "    with torch.no_grad():\n",
    "        if stopper_metric or compute_all_metrics:\n",
    "            ground_truths = torch.tensor([], device='cpu')\n",
    "            preds = torch.tensor([], device='cpu')\n",
    "            indexes = torch.tensor([], device='cpu')\n",
    "            total_loss, total_examples = 0, 0\n",
    "            index_end = loader.batch_size\n",
    "\n",
    "        for sampled_data in loader:\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            batch_size = len(sampled_data[config.labels['tail']].dst_pos_index)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples = torch.ones(batch_size, device=device)\n",
    "            neg_samples = torch.zeros(num_neg_samples*batch_size, device=device)\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            if stopper_metric or compute_all_metrics: # Store preds and truths for all batches, and compute indices to calc metrics\n",
    "                index_pos = torch.arange(end=index_end, start=index_end-batch_size) # index for predictions with pos. ground_truth\n",
    "                index_neg = torch.arange(end=index_end, start=index_end-batch_size).repeat_interleave(num_neg_samples)\n",
    "                index = torch.cat((index_pos, index_neg))\n",
    "                indexes = torch.cat((indexes, index.to('cpu')))\n",
    "                preds = torch.cat((preds, pred.to('cpu')))\n",
    "                ground_truths = torch.cat((ground_truths, ground_truth.to('cpu')))\n",
    "                index_end += batch_size\n",
    "\n",
    "            eval_loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            # if not stopper_metric: # Just logging loss\n",
    "            #     wandb.log({\"running_val_loss\": eval_loss})\n",
    "            #     break\n",
    "            # else:\n",
    "            total_loss += float(eval_loss) * pred.numel() \n",
    "            total_examples += pred.numel()\n",
    "            eval_loss = total_loss / total_examples\n",
    "\n",
    "            # if stopper_metric and not compute_all_metrics:\n",
    "            #     if index_end >= 2048: # Compute approx. intermediate metric on a few datapoints to speed up hyperopt process\n",
    "            #         break \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if stopper_metric and not compute_all_metrics: # Compute only the metric that dictates stopping\n",
    "            indexes = indexes.long()\n",
    "\n",
    "            # # Log heatmap between prediction and ground truth. Useful for visualization / debugging. overhead 0.2s per epoch for a single sample.\n",
    "            # heatmap = heatmaps(preds, ground_truths, indexes) \n",
    "            # wandb.log({f\"heatmap\": wandb.Image(heatmap)})\n",
    "            # heatmap.close() # Free up memory\n",
    "\n",
    "            match stopper_metric:\n",
    "                case \"val_loss\":\n",
    "                    eval_loss = total_loss / total_examples\n",
    "                    return eval_loss\n",
    "                \n",
    "                case \"mrr\":\n",
    "                    mrr = RetrievalMRR().to(device)\n",
    "                    return mrr(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_10\":\n",
    "                    hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "                    return hit_at_10(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_5\":\n",
    "                    hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "                    return hit_at_5(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_3\":\n",
    "                    hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "                    return hit_at_3(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_1\":\n",
    "                    hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "                    return hit_at_1(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case _:\n",
    "                    raise ValueError(f\"Unrecognized stopper metric: '{stopper_metric}'\")\n",
    "                \n",
    "        if compute_all_metrics: # Compute all metrics at the end of training\n",
    "            indexes = indexes.long()\n",
    "            mrr = RetrievalMRR().to(device)\n",
    "            hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "            hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "            hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "            hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "            \n",
    "            mrr = mrr(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_10 = hit_at_10(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_5 = hit_at_5(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_3 = hit_at_3(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_1 = hit_at_1(preds, ground_truths, indexes=indexes)\n",
    "\n",
    "            wandb.log({\n",
    "                f\"{loader_type}MRR\": mrr, f\"{loader_type}hit_at_10\": hit_at_10, f\"{loader_type}hit_at_5\": hit_at_5, f\"{loader_type}hit_at_3\": hit_at_3, f\"{loader_type}hit_at_1\": hit_at_1\n",
    "            })\n",
    "\n",
    "    return mrr, eval_loss\n",
    "\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "# print(model)\n",
    "\n",
    "criterion = sigmoid_focal_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# optimizer = Lion(model.parameters(), lr=config.lr, weight_decay=1e-2)\n",
    "early_stopper = EarlyStopper(frequency=config.stopper_frequency, patience=config.stopper_patience,\n",
    "                             direction=config.stopper_direction, relative_delta=config.stopper_relative_delta)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "@timer_func\n",
    "def train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type = 'usual', eval_period = 0):\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = total_examples = 0\n",
    "\n",
    "        for sampled_data in tqdm(train_loader, desc=\"Training\"):\n",
    "\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples  = torch.ones(len(sampled_data[config.labels['tail']].dst_pos_index), device=device)\n",
    "            neg_samples  = torch.zeros(len(sampled_data[config.labels['tail']].dst_neg_index.view(-1)), device=device) # As many zeroes as there are negative samples * batch_size\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            false_tails = sampled_data[config.labels['tail']].dst_neg_index.view(-1)\n",
    "            true_tails = sampled_data['go']['dst_pos_index']\n",
    "            true_tails = torch.repeat_interleave(true_tails,int(false_tails.size()[0]/true_tails.size()[0]))\n",
    "\n",
    "\n",
    "            if labels_type == 'linsim':\n",
    "                \n",
    "                lin_neg_samples = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "\n",
    "                pos_samples = pos_samples.to(device)\n",
    "                lin_neg_samples = lin_neg_samples.to(device)\n",
    "\n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'rand_linsim':\n",
    "\n",
    "                linsims = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "                lin_neg_samples = pwc.shuffle_tensor(linsims)\n",
    "                lin_neg_samples.to(device)\n",
    "                \n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'gaussian_noise':\n",
    "                # Add gaussian noise to pos_samples to simulate noisy labels. Added noise must be negative and not exceed 1.\n",
    "                ground_truth += torch.normal(mean=0, std=1, size=(len(ground_truth),), device=device)\n",
    "                # apply sigmoid\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'usual':\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            wandb.log({\"loss\": loss})\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "        train_loss = total_loss / total_examples\n",
    "        print(f\"Epoch: {epoch:03d}, Avg. Loss: {train_loss:.10f}\")\n",
    "\n",
    "\n",
    "        # scheduler.step()\n",
    "        if eval_period:\n",
    "            if epoch%eval_period == 0:\n",
    "                # Compute metrics and check for early stopping\n",
    "                score, val_loss = evaluate(config, val_loader, model, criterion, stopper_metric=config.stopper_metric, compute_all_metrics= True)\n",
    "                wandb.log({\"avg_loss\": train_loss, \"val_loss\": val_loss, f\"{config.stopper_metric}\": score})\n",
    "\n",
    "        # early_stopper(score)\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(\"Early stopping triggered at epoch\", epoch)\n",
    "        #     break\n",
    "\n",
    "    print(\"Training done.\")\n",
    "\n",
    "train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type=config.labels_type, eval_period = eval_period)\n",
    "evaluate(config,  val_loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric='hit_at_10')\n",
    "evaluate(config, test_loader, model, criterion, compute_all_metrics=False, loader_type='test'      , stopper_metric='hit_at_10')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Baseline VS LinSim VS Noise : gaussian_noise labels\n",
      "Device: 'cuda'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebutz/ESL2024/code/wandb/run-20240425_181329-iy5v06zg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/iy5v06zg' target=\"_blank\">gaussian_noise labels</a></strong> to <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/iy5v06zg' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/iy5v06zg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is homogeneous : False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/ESL2024/code/utils/train_utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_links['source_node'] = df_links.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  genes={ x=[72335, 29] },\n",
      "  go={ x=[42979, 3] },\n",
      "  po={ x=[1662, 2] },\n",
      "  traito={ x=[1636, 1] },\n",
      "  prosite_profiles={ x=[627, 1] },\n",
      "  prosite_patterns={ x=[652, 1] },\n",
      "  superfamily={ x=[957, 1] },\n",
      "  panther={ x=[5971, 1] },\n",
      "  prints={ x=[420, 1] },\n",
      "  (genes, interacts_with, genes)={ edge_index=[2, 1120849] },\n",
      "  (genes, gene_ontology, go)={ edge_index=[2, 169248] },\n",
      "  (genes, trait_ontology, traito)={ edge_index=[2, 15080] },\n",
      "  (genes, plant_ontology, po)={ edge_index=[2, 6125] },\n",
      "  (genes, profile, prosite_profiles)={ edge_index=[2, 12239] },\n",
      "  (genes, pattern, prosite_patterns)={ edge_index=[2, 6677] },\n",
      "  (genes, family, superfamily)={ edge_index=[2, 20742] },\n",
      "  (genes, panther_id, panther)={ edge_index=[2, 25319] },\n",
      "  (genes, prints_id, prints)={ edge_index=[2, 4365] },\n",
      "  (go, is_a, go)={ edge_index=[2, 136436] },\n",
      "  (po, is_a, po)={ edge_index=[2, 3548] },\n",
      "  (traito, is_a, traito)={ edge_index=[2, 3770] },\n",
      "  (go, rev_gene_ontology, genes)={ edge_index=[2, 169248] },\n",
      "  (traito, rev_trait_ontology, genes)={ edge_index=[2, 15080] },\n",
      "  (po, rev_plant_ontology, genes)={ edge_index=[2, 6125] },\n",
      "  (prosite_profiles, rev_profile, genes)={ edge_index=[2, 12239] },\n",
      "  (prosite_patterns, rev_pattern, genes)={ edge_index=[2, 6677] },\n",
      "  (superfamily, rev_family, genes)={ edge_index=[2, 20742] },\n",
      "  (panther, rev_panther_id, genes)={ edge_index=[2, 25319] },\n",
      "  (prints, rev_prints_id, genes)={ edge_index=[2, 4365] }\n",
      ")\n",
      "data look valid :  True\n",
      "Function 'split_data' executed in 0.0113s\n",
      "Function 'build_dataloaders' executed in 0.1352s\n",
      "\n",
      "Loading ontology...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Avg. Loss: 0.1405694494\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.8684s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Avg. Loss: 0.1405388689\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.3322s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Avg. Loss: 0.1405338241\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 164.3595s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Avg. Loss: 0.1405331540\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.6409s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Avg. Loss: 0.1405346842\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.3471s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Avg. Loss: 0.1405307500\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.9603s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Avg. Loss: 0.1405318886\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.6591s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Avg. Loss: 0.1405317909\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.3760s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Avg. Loss: 0.1405303624\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 164.3279s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Avg. Loss: 0.1405318287\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.6567s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Avg. Loss: 0.1405297026\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.0479s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Avg. Loss: 0.1405300804\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.1616s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Avg. Loss: 0.1405309733\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.0148s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Avg. Loss: 0.1405295778\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.3906s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Avg. Loss: 0.1405298218\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.2240s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:31<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Avg. Loss: 0.1405281482\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.7478s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:30<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Avg. Loss: 0.1405294022\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.9205s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:32<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Avg. Loss: 0.1405274302\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.1213s\n",
      "Training done.\n",
      "Function 'train' executed in 3540.1922s\n",
      "Evaluation...\n",
      "Function 'evaluate' executed in 102.9278s\n",
      "Evaluation...\n",
      "Function 'evaluate' executed in 102.1976s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁</td></tr><tr><td>hit_at_10</td><td>▁▁▄▅▆▆▅▅▆▅▇██▄▇▇▇▆</td></tr><tr><td>loss</td><td>█▅▄▆▆▅▄▄▅▆▄▄▄▇▄▅▅▄▄▄▄▃▃▄▃▄▁▄▅▄▄▃▄▃▄▅▅▃▄▆</td></tr><tr><td>val_loss</td><td>▄▁▆▄▆█▇▆▆█▇▆▆▇▆▇▇▇</td></tr><tr><td>validationMRR</td><td>▁▁▄▅▆▆▅▅▆▅▇██▄▇▇▇▆</td></tr><tr><td>validationhit_at_1</td><td>▁▂▄▅▆▆▆▅▆▅▇█▇▄▆▆▇▅</td></tr><tr><td>validationhit_at_10</td><td>▁▂▅▆▇▆▆▄▅▄▆█▇▇█▇██</td></tr><tr><td>validationhit_at_3</td><td>▂▁▃▅▇▆▄▅▆▅▇▇█▅██▇▆</td></tr><tr><td>validationhit_at_5</td><td>▄▁▄▆█▆▅▄▆▄█▇█▇███▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.14053</td></tr><tr><td>hit_at_10</td><td>0.41611</td></tr><tr><td>loss</td><td>0.14055</td></tr><tr><td>val_loss</td><td>0.14855</td></tr><tr><td>validationMRR</td><td>0.41611</td></tr><tr><td>validationhit_at_1</td><td>0.27484</td></tr><tr><td>validationhit_at_10</td><td>0.72413</td></tr><tr><td>validationhit_at_3</td><td>0.47356</td></tr><tr><td>validationhit_at_5</td><td>0.57744</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gaussian_noise labels</strong> at: <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/iy5v06zg' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/iy5v06zg</a><br/> View project at: <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240425_181329-iy5v06zg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ebutz/ESL2024/code/utils' )\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from nxontology.imports import from_file\n",
    "import wandb\n",
    "from play_with_complex import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from model_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
    "import play_with_complex as pwc\n",
    "\n",
    "# How to calculate loss ?\n",
    "# Choices are : 'usual', 'rand_linsim', linsim', 'gaussian_noise'\n",
    "labels_type = 'gaussian_noise'\n",
    "\n",
    "# How many epochs between 2 time-consuming evaluations ?\n",
    "eval_period = 1\n",
    "\n",
    "# Wandb\n",
    "xp_name = 'GNN Baseline VS LinSim VS Noise'\n",
    "run_name = f'{labels_type} labels'\n",
    "print(xp_name,':',run_name)\n",
    "\n",
    "# Iric\n",
    "iric_path = '/home/ebutz/ESL2024/data/full_iric/iric.csv'\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/full_iric/altailed_mapped_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle'\n",
    "check_dicts = True\n",
    "\n",
    "ontology_path = '/home/ebutz/ESL2024/data/go-basic.json.gz'\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "run = wandb.init(project=xp_name, name = run_name)\n",
    "config = wandb.config\n",
    "config.labels_type = labels_type\n",
    "config.val_ratio=0.1\n",
    "config.homogeneous=False\n",
    "config.scorelist_size=1000\n",
    "config.split_ratio=0.8\n",
    "config.val_ratio=0.1\n",
    "config.test_ratio=0.1\n",
    "config.num_neighbors=[70,55,13,89,85]\n",
    "config.batch_size=1024\n",
    "config.train_neg_sampling_ratio=224\n",
    "config.epochs=18\n",
    "config.disjoint_train_ratio=0.6\n",
    "config.lr=0.0015308253347932983\n",
    "config.stopper_metric= 'hit_at_10'\n",
    "config.stopper_direction=\"maximize\"\n",
    "config.stopper_patience=5\n",
    "config.stopper_frequency=1\n",
    "config.stopper_relative_delta=0.05\n",
    "config.gamma=1.3\n",
    "config.alpha=0.42680473078813763\n",
    "config.gnn_layer='ResGatedGraphConv'\n",
    "config.dropout=0.1\n",
    "config.norm='DiffGroupNorm'\n",
    "config.aggregation = 'min'\n",
    "config.hidden_channels=115\n",
    "config.num_layers=3\n",
    "config.attention_heads=4\n",
    "config.homogeneous = False\n",
    "config.labels = {'head' : 'genes', 'relation' : 'gene_ontology', 'tail' : 'go'}\n",
    "\n",
    "print('Graph is homogeneous :',config.homogeneous)\n",
    "\n",
    "data = load_iric_data('/home/ebutz/ESL2024/data/full_iric/iric.csv', featureless=False)\n",
    "data  = T.ToUndirected(merge=True)(data) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "data = T.RemoveDuplicatedEdges()(data) # Remove duplicated edges\n",
    "print(data)\n",
    "print('data look valid : ',data.validate())\n",
    "\n",
    "train_data, val_data, test_data = split_data(data, config)\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data, val_data, test_data, config)\n",
    "\n",
    "# # ------------- Loading ontology ------------- #\n",
    "\n",
    "print(\"\\nLoading ontology...\")\n",
    "nxo = from_file(ontology_path)\n",
    "nxo.freeze()\n",
    "pwc.nxo = nxo\n",
    "\n",
    "df = pd.read_csv(iric_path, index_col=0, dtype=str)\n",
    "df['source_node'] = df.index\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "go_map = get_nodelist(df, 'go')\n",
    "go_to_idx = {node: i for i, node in enumerate(go_map)}\n",
    "idx_to_go = {i: node for i, node in enumerate(go_map)}\n",
    "\n",
    "# # ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = idx_to_go\n",
    "pwc.mapped_alt_tails = None\n",
    "pwc.device           = device\n",
    "\n",
    "# ------------- Model setup ------------- #\n",
    "\n",
    "gnn_layers = get_gnn_layers(config)\n",
    "norm_layers = get_norm_layers(config, len(data.node_types))\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "\n",
    "# ----------------- Loops ----------------- #\n",
    "@timer_func\n",
    "def evaluate(config, loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model.    \n",
    "    loader : DataLoader\n",
    "        The data loader to evaluate the model on.\n",
    "    model : Model\n",
    "        The model to evaluate.\n",
    "    criterion : callable\n",
    "        The loss function to use for evaluation.\n",
    "    compute_all_metrics : bool, optional\n",
    "        Whether to compute all metrics or partially. Default is False.\n",
    "    loader_type : str, optional\n",
    "        The type of the loader ('validation' or 'test'). Default is 'validation'.\n",
    "    stopper_metric : str or bool, optional\n",
    "        The metric that dictates when to stop training. If False, only the loss is computed. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of the evaluation metric and the loss, depending on the value of `stopper_metric`.\n",
    "        If compute_all_metrics is True, the function logs all metrics to W&B.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluation...\")\n",
    "    model.eval()\n",
    "    num_neg_samples = loader.neg_sampling.amount\n",
    "    with torch.no_grad():\n",
    "        if stopper_metric or compute_all_metrics:\n",
    "            ground_truths = torch.tensor([], device='cpu')\n",
    "            preds = torch.tensor([], device='cpu')\n",
    "            indexes = torch.tensor([], device='cpu')\n",
    "            total_loss, total_examples = 0, 0\n",
    "            index_end = loader.batch_size\n",
    "\n",
    "        for sampled_data in loader:\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            batch_size = len(sampled_data[config.labels['tail']].dst_pos_index)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples = torch.ones(batch_size, device=device)\n",
    "            neg_samples = torch.zeros(num_neg_samples*batch_size, device=device)\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            if stopper_metric or compute_all_metrics: # Store preds and truths for all batches, and compute indices to calc metrics\n",
    "                index_pos = torch.arange(end=index_end, start=index_end-batch_size) # index for predictions with pos. ground_truth\n",
    "                index_neg = torch.arange(end=index_end, start=index_end-batch_size).repeat_interleave(num_neg_samples)\n",
    "                index = torch.cat((index_pos, index_neg))\n",
    "                indexes = torch.cat((indexes, index.to('cpu')))\n",
    "                preds = torch.cat((preds, pred.to('cpu')))\n",
    "                ground_truths = torch.cat((ground_truths, ground_truth.to('cpu')))\n",
    "                index_end += batch_size\n",
    "\n",
    "            eval_loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            # if not stopper_metric: # Just logging loss\n",
    "            #     wandb.log({\"running_val_loss\": eval_loss})\n",
    "            #     break\n",
    "            # else:\n",
    "            total_loss += float(eval_loss) * pred.numel() \n",
    "            total_examples += pred.numel()\n",
    "            eval_loss = total_loss / total_examples\n",
    "\n",
    "            # if stopper_metric and not compute_all_metrics:\n",
    "            #     if index_end >= 2048: # Compute approx. intermediate metric on a few datapoints to speed up hyperopt process\n",
    "            #         break \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if stopper_metric and not compute_all_metrics: # Compute only the metric that dictates stopping\n",
    "            indexes = indexes.long()\n",
    "\n",
    "            # # Log heatmap between prediction and ground truth. Useful for visualization / debugging. overhead 0.2s per epoch for a single sample.\n",
    "            # heatmap = heatmaps(preds, ground_truths, indexes) \n",
    "            # wandb.log({f\"heatmap\": wandb.Image(heatmap)})\n",
    "            # heatmap.close() # Free up memory\n",
    "\n",
    "            match stopper_metric:\n",
    "                case \"val_loss\":\n",
    "                    eval_loss = total_loss / total_examples\n",
    "                    return eval_loss\n",
    "                \n",
    "                case \"mrr\":\n",
    "                    mrr = RetrievalMRR().to(device)\n",
    "                    return mrr(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_10\":\n",
    "                    hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "                    return hit_at_10(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_5\":\n",
    "                    hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "                    return hit_at_5(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_3\":\n",
    "                    hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "                    return hit_at_3(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_1\":\n",
    "                    hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "                    return hit_at_1(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case _:\n",
    "                    raise ValueError(f\"Unrecognized stopper metric: '{stopper_metric}'\")\n",
    "                \n",
    "        if compute_all_metrics: # Compute all metrics at the end of training\n",
    "            indexes = indexes.long()\n",
    "            mrr = RetrievalMRR().to(device)\n",
    "            hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "            hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "            hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "            hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "            \n",
    "            mrr = mrr(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_10 = hit_at_10(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_5 = hit_at_5(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_3 = hit_at_3(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_1 = hit_at_1(preds, ground_truths, indexes=indexes)\n",
    "\n",
    "            wandb.log({\n",
    "                f\"{loader_type}MRR\": mrr, f\"{loader_type}hit_at_10\": hit_at_10, f\"{loader_type}hit_at_5\": hit_at_5, f\"{loader_type}hit_at_3\": hit_at_3, f\"{loader_type}hit_at_1\": hit_at_1\n",
    "            })\n",
    "\n",
    "    return mrr, eval_loss\n",
    "\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "# print(model)\n",
    "\n",
    "criterion = sigmoid_focal_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# optimizer = Lion(model.parameters(), lr=config.lr, weight_decay=1e-2)\n",
    "early_stopper = EarlyStopper(frequency=config.stopper_frequency, patience=config.stopper_patience,\n",
    "                             direction=config.stopper_direction, relative_delta=config.stopper_relative_delta)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "@timer_func\n",
    "def train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type = 'usual', eval_period = 0):\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = total_examples = 0\n",
    "\n",
    "        for sampled_data in tqdm(train_loader, desc=\"Training\"):\n",
    "\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples  = torch.ones(len(sampled_data[config.labels['tail']].dst_pos_index), device=device)\n",
    "            neg_samples  = torch.zeros(len(sampled_data[config.labels['tail']].dst_neg_index.view(-1)), device=device) # As many zeroes as there are negative samples * batch_size\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            false_tails = sampled_data[config.labels['tail']].dst_neg_index.view(-1)\n",
    "            true_tails = sampled_data['go']['dst_pos_index']\n",
    "            true_tails = torch.repeat_interleave(true_tails,int(false_tails.size()[0]/true_tails.size()[0]))\n",
    "\n",
    "\n",
    "            if labels_type == 'linsim':\n",
    "                \n",
    "                lin_neg_samples = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "\n",
    "                pos_samples = pos_samples.to(device)\n",
    "                lin_neg_samples = lin_neg_samples.to(device)\n",
    "\n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'rand_linsim':\n",
    "\n",
    "                linsims = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "                lin_neg_samples = pwc.shuffle_tensor(linsims)\n",
    "                lin_neg_samples.to(device)\n",
    "                \n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'gaussian_noise':\n",
    "                # Add gaussian noise to pos_samples to simulate noisy labels. Added noise must be negative and not exceed 1.\n",
    "                ground_truth += torch.normal(mean=0, std=1, size=(len(ground_truth),), device=device)\n",
    "                # apply sigmoid\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'usual':\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            wandb.log({\"loss\": loss})\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "        train_loss = total_loss / total_examples\n",
    "        print(f\"Epoch: {epoch:03d}, Avg. Loss: {train_loss:.10f}\")\n",
    "\n",
    "\n",
    "        # scheduler.step()\n",
    "        if eval_period:\n",
    "            if epoch%eval_period == 0:\n",
    "                # Compute metrics and check for early stopping\n",
    "                score, val_loss = evaluate(config, val_loader, model, criterion, stopper_metric=config.stopper_metric, compute_all_metrics= True)\n",
    "                wandb.log({\"avg_loss\": train_loss, \"val_loss\": val_loss, f\"{config.stopper_metric}\": score})\n",
    "\n",
    "        # early_stopper(score)\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(\"Early stopping triggered at epoch\", epoch)\n",
    "        #     break\n",
    "\n",
    "    print(\"Training done.\")\n",
    "\n",
    "train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type=config.labels_type, eval_period = eval_period)\n",
    "evaluate(config,  val_loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric='hit_at_10')\n",
    "evaluate(config, test_loader, model, criterion, compute_all_metrics=False, loader_type='test'      , stopper_metric='hit_at_10')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/miniconda3/envs/pyg2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Baseline VS LinSim VS Noise : linsim labels\n",
      "Device: 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbutzelliot\u001b[0m (\u001b[33mesl2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebutz/ESL2024/wandb/run-20240426_093102-urru43r8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/urru43r8' target=\"_blank\">linsim labels</a></strong> to <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/urru43r8' target=\"_blank\">https://wandb.ai/esl2024/GNN%20Baseline%20VS%20LinSim%20VS%20Noise/runs/urru43r8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is homogeneous : False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/ESL2024/code/utils/train_utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_links['source_node'] = df_links.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  genes={ x=[72335, 29] },\n",
      "  go={ x=[42979, 3] },\n",
      "  po={ x=[1662, 2] },\n",
      "  traito={ x=[1636, 1] },\n",
      "  prosite_profiles={ x=[627, 1] },\n",
      "  prosite_patterns={ x=[652, 1] },\n",
      "  superfamily={ x=[957, 1] },\n",
      "  panther={ x=[5971, 1] },\n",
      "  prints={ x=[420, 1] },\n",
      "  (genes, interacts_with, genes)={ edge_index=[2, 1120849] },\n",
      "  (genes, gene_ontology, go)={ edge_index=[2, 169248] },\n",
      "  (genes, trait_ontology, traito)={ edge_index=[2, 15080] },\n",
      "  (genes, plant_ontology, po)={ edge_index=[2, 6125] },\n",
      "  (genes, profile, prosite_profiles)={ edge_index=[2, 12239] },\n",
      "  (genes, pattern, prosite_patterns)={ edge_index=[2, 6677] },\n",
      "  (genes, family, superfamily)={ edge_index=[2, 20742] },\n",
      "  (genes, panther_id, panther)={ edge_index=[2, 25319] },\n",
      "  (genes, prints_id, prints)={ edge_index=[2, 4365] },\n",
      "  (go, is_a, go)={ edge_index=[2, 136436] },\n",
      "  (po, is_a, po)={ edge_index=[2, 3548] },\n",
      "  (traito, is_a, traito)={ edge_index=[2, 3770] },\n",
      "  (go, rev_gene_ontology, genes)={ edge_index=[2, 169248] },\n",
      "  (traito, rev_trait_ontology, genes)={ edge_index=[2, 15080] },\n",
      "  (po, rev_plant_ontology, genes)={ edge_index=[2, 6125] },\n",
      "  (prosite_profiles, rev_profile, genes)={ edge_index=[2, 12239] },\n",
      "  (prosite_patterns, rev_pattern, genes)={ edge_index=[2, 6677] },\n",
      "  (superfamily, rev_family, genes)={ edge_index=[2, 20742] },\n",
      "  (panther, rev_panther_id, genes)={ edge_index=[2, 25319] },\n",
      "  (prints, rev_prints_id, genes)={ edge_index=[2, 4365] }\n",
      ")\n",
      "data look valid :  True\n",
      "Function 'split_data' executed in 0.0135s\n",
      "Function 'build_dataloaders' executed in 0.1460s\n",
      "\n",
      "Loading ontology...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:54<00:00,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Avg. Loss: 0.0434585997\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 169.9036s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:56<00:00,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Avg. Loss: 0.0311114858\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.9469s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:49<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Avg. Loss: 0.0308846287\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 175.8239s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:52<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Avg. Loss: 0.0312296770\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 168.4892s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:50<00:00,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Avg. Loss: 0.0311502978\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.5699s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:46<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Avg. Loss: 0.0311125722\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.2675s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:56<00:00,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Avg. Loss: 0.0310835380\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 168.3076s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:45<00:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Avg. Loss: 0.0312168950\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 165.9901s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:49<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Avg. Loss: 0.0311443613\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.0789s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:49<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Avg. Loss: 0.0311954962\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'evaluate' executed in 166.8958s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:45<00:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Avg. Loss: 0.0310248189\n",
      "Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ebutz/ESL2024/code/utils' )\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from nxontology.imports import from_file\n",
    "import wandb\n",
    "from play_with_complex import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from model_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
    "import play_with_complex as pwc\n",
    "\n",
    "# How to calculate loss ?\n",
    "# Choices are : 'usual', 'rand_linsim', linsim', 'gaussian_noise'\n",
    "labels_type = 'linsim'\n",
    "\n",
    "# How many epochs between 2 time-consuming evaluations ?\n",
    "eval_period = 1\n",
    "\n",
    "# Wandb\n",
    "xp_name = 'GNN Baseline VS LinSim VS Noise'\n",
    "run_name = f'{labels_type} labels'\n",
    "print(xp_name,':',run_name)\n",
    "\n",
    "# Iric\n",
    "iric_path = '/home/ebutz/ESL2024/data/full_iric/iric.csv'\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/full_iric/altailed_mapped_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle'\n",
    "check_dicts = True\n",
    "\n",
    "ontology_path = '/home/ebutz/ESL2024/data/go-basic.json.gz'\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "run = wandb.init(project=xp_name, name = run_name)\n",
    "config = wandb.config\n",
    "config.labels_type = labels_type\n",
    "config.val_ratio=0.1\n",
    "config.homogeneous=False\n",
    "config.scorelist_size=1000\n",
    "config.split_ratio=0.8\n",
    "config.val_ratio=0.1\n",
    "config.test_ratio=0.1\n",
    "config.num_neighbors=[70,55,13,89,85]\n",
    "config.batch_size=1024\n",
    "config.train_neg_sampling_ratio=224\n",
    "config.epochs=18\n",
    "config.disjoint_train_ratio=0.6\n",
    "config.lr=0.0015308253347932983\n",
    "config.stopper_metric= 'hit_at_10'\n",
    "config.stopper_direction=\"maximize\"\n",
    "config.stopper_patience=5\n",
    "config.stopper_frequency=1\n",
    "config.stopper_relative_delta=0.05\n",
    "config.gamma=1.3\n",
    "config.alpha=0.42680473078813763\n",
    "config.gnn_layer='ResGatedGraphConv'\n",
    "config.dropout=0.1\n",
    "config.norm='DiffGroupNorm'\n",
    "config.aggregation = 'min'\n",
    "config.hidden_channels=115\n",
    "config.num_layers=3\n",
    "config.attention_heads=4\n",
    "config.homogeneous = False\n",
    "config.labels = {'head' : 'genes', 'relation' : 'gene_ontology', 'tail' : 'go'}\n",
    "\n",
    "print('Graph is homogeneous :',config.homogeneous)\n",
    "\n",
    "data = load_iric_data('/home/ebutz/ESL2024/data/full_iric/iric.csv', featureless=False)\n",
    "data  = T.ToUndirected(merge=True)(data) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "data = T.RemoveDuplicatedEdges()(data) # Remove duplicated edges\n",
    "print(data)\n",
    "print('data look valid : ',data.validate())\n",
    "\n",
    "train_data, val_data, test_data = split_data(data, config)\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data, val_data, test_data, config)\n",
    "\n",
    "# # ------------- Loading ontology ------------- #\n",
    "\n",
    "print(\"\\nLoading ontology...\")\n",
    "nxo = from_file(ontology_path)\n",
    "nxo.freeze()\n",
    "pwc.nxo = nxo\n",
    "\n",
    "df = pd.read_csv(iric_path, index_col=0, dtype=str)\n",
    "df['source_node'] = df.index\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "go_map = get_nodelist(df, 'go')\n",
    "go_to_idx = {node: i for i, node in enumerate(go_map)}\n",
    "idx_to_go = {i: node for i, node in enumerate(go_map)}\n",
    "\n",
    "# # ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = idx_to_go\n",
    "pwc.mapped_alt_tails = None\n",
    "pwc.device           = device\n",
    "\n",
    "# ------------- Model setup ------------- #\n",
    "\n",
    "gnn_layers = get_gnn_layers(config)\n",
    "norm_layers = get_norm_layers(config, len(data.node_types))\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "\n",
    "# ----------------- Loops ----------------- #\n",
    "@timer_func\n",
    "def evaluate(config, loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model.    \n",
    "    loader : DataLoader\n",
    "        The data loader to evaluate the model on.\n",
    "    model : Model\n",
    "        The model to evaluate.\n",
    "    criterion : callable\n",
    "        The loss function to use for evaluation.\n",
    "    compute_all_metrics : bool, optional\n",
    "        Whether to compute all metrics or partially. Default is False.\n",
    "    loader_type : str, optional\n",
    "        The type of the loader ('validation' or 'test'). Default is 'validation'.\n",
    "    stopper_metric : str or bool, optional\n",
    "        The metric that dictates when to stop training. If False, only the loss is computed. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of the evaluation metric and the loss, depending on the value of `stopper_metric`.\n",
    "        If compute_all_metrics is True, the function logs all metrics to W&B.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluation...\")\n",
    "    model.eval()\n",
    "    num_neg_samples = loader.neg_sampling.amount\n",
    "    with torch.no_grad():\n",
    "        if stopper_metric or compute_all_metrics:\n",
    "            ground_truths = torch.tensor([], device='cpu')\n",
    "            preds = torch.tensor([], device='cpu')\n",
    "            indexes = torch.tensor([], device='cpu')\n",
    "            total_loss, total_examples = 0, 0\n",
    "            index_end = loader.batch_size\n",
    "\n",
    "        for sampled_data in loader:\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            batch_size = len(sampled_data[config.labels['tail']].dst_pos_index)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples = torch.ones(batch_size, device=device)\n",
    "            neg_samples = torch.zeros(num_neg_samples*batch_size, device=device)\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            if stopper_metric or compute_all_metrics: # Store preds and truths for all batches, and compute indices to calc metrics\n",
    "                index_pos = torch.arange(end=index_end, start=index_end-batch_size) # index for predictions with pos. ground_truth\n",
    "                index_neg = torch.arange(end=index_end, start=index_end-batch_size).repeat_interleave(num_neg_samples)\n",
    "                index = torch.cat((index_pos, index_neg))\n",
    "                indexes = torch.cat((indexes, index.to('cpu')))\n",
    "                preds = torch.cat((preds, pred.to('cpu')))\n",
    "                ground_truths = torch.cat((ground_truths, ground_truth.to('cpu')))\n",
    "                index_end += batch_size\n",
    "\n",
    "            eval_loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            # if not stopper_metric: # Just logging loss\n",
    "            #     wandb.log({\"running_val_loss\": eval_loss})\n",
    "            #     break\n",
    "            # else:\n",
    "            total_loss += float(eval_loss) * pred.numel() \n",
    "            total_examples += pred.numel()\n",
    "            eval_loss = total_loss / total_examples\n",
    "\n",
    "            # if stopper_metric and not compute_all_metrics:\n",
    "            #     if index_end >= 2048: # Compute approx. intermediate metric on a few datapoints to speed up hyperopt process\n",
    "            #         break \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if stopper_metric and not compute_all_metrics: # Compute only the metric that dictates stopping\n",
    "            indexes = indexes.long()\n",
    "\n",
    "            # # Log heatmap between prediction and ground truth. Useful for visualization / debugging. overhead 0.2s per epoch for a single sample.\n",
    "            # heatmap = heatmaps(preds, ground_truths, indexes) \n",
    "            # wandb.log({f\"heatmap\": wandb.Image(heatmap)})\n",
    "            # heatmap.close() # Free up memory\n",
    "\n",
    "            match stopper_metric:\n",
    "                case \"val_loss\":\n",
    "                    eval_loss = total_loss / total_examples\n",
    "                    return eval_loss\n",
    "                \n",
    "                case \"mrr\":\n",
    "                    mrr = RetrievalMRR().to(device)\n",
    "                    return mrr(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_10\":\n",
    "                    hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "                    return hit_at_10(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_5\":\n",
    "                    hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "                    return hit_at_5(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_3\":\n",
    "                    hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "                    return hit_at_3(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_1\":\n",
    "                    hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "                    return hit_at_1(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case _:\n",
    "                    raise ValueError(f\"Unrecognized stopper metric: '{stopper_metric}'\")\n",
    "                \n",
    "        if compute_all_metrics: # Compute all metrics at the end of training\n",
    "            indexes = indexes.long()\n",
    "            mrr = RetrievalMRR().to(device)\n",
    "            hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "            hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "            hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "            hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "            \n",
    "            mrr = mrr(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_10 = hit_at_10(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_5 = hit_at_5(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_3 = hit_at_3(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_1 = hit_at_1(preds, ground_truths, indexes=indexes)\n",
    "\n",
    "            wandb.log({\n",
    "                f\"{loader_type}MRR\": mrr, f\"{loader_type}hit_at_10\": hit_at_10, f\"{loader_type}hit_at_5\": hit_at_5, f\"{loader_type}hit_at_3\": hit_at_3, f\"{loader_type}hit_at_1\": hit_at_1\n",
    "            })\n",
    "\n",
    "    return mrr, eval_loss\n",
    "\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "# print(model)\n",
    "\n",
    "criterion = sigmoid_focal_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# optimizer = Lion(model.parameters(), lr=config.lr, weight_decay=1e-2)\n",
    "early_stopper = EarlyStopper(frequency=config.stopper_frequency, patience=config.stopper_patience,\n",
    "                             direction=config.stopper_direction, relative_delta=config.stopper_relative_delta)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "@timer_func\n",
    "def train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type = 'usual', eval_period = 0):\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = total_examples = 0\n",
    "\n",
    "        for sampled_data in tqdm(train_loader, desc=\"Training\"):\n",
    "\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples  = torch.ones(len(sampled_data[config.labels['tail']].dst_pos_index), device=device)\n",
    "            neg_samples  = torch.zeros(len(sampled_data[config.labels['tail']].dst_neg_index.view(-1)), device=device) # As many zeroes as there are negative samples * batch_size\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            false_tails = sampled_data[config.labels['tail']].dst_neg_index.view(-1)\n",
    "            true_tails = sampled_data['go']['dst_pos_index']\n",
    "            true_tails = torch.repeat_interleave(true_tails,int(false_tails.size()[0]/true_tails.size()[0]))\n",
    "\n",
    "\n",
    "            if labels_type == 'linsim':\n",
    "                \n",
    "                lin_neg_samples = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "\n",
    "                pos_samples = pos_samples.to(device)\n",
    "                lin_neg_samples = lin_neg_samples.to(device)\n",
    "\n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'rand_linsim':\n",
    "\n",
    "                linsims = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "                lin_neg_samples = pwc.shuffle_tensor(linsims)\n",
    "                lin_neg_samples.to(device)\n",
    "                \n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'gaussian_noise':\n",
    "                # Add gaussian noise to pos_samples to simulate noisy labels. Added noise must be negative and not exceed 1.\n",
    "                ground_truth += torch.normal(mean=0, std=1, size=(len(ground_truth),), device=device)\n",
    "                # apply sigmoid\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'usual':\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            wandb.log({\"loss\": loss})\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "        train_loss = total_loss / total_examples\n",
    "        print(f\"Epoch: {epoch:03d}, Avg. Loss: {train_loss:.10f}\")\n",
    "\n",
    "\n",
    "        # scheduler.step()\n",
    "        if eval_period:\n",
    "            if epoch%eval_period == 0:\n",
    "                # Compute metrics and check for early stopping\n",
    "                score, val_loss = evaluate(config, val_loader, model, criterion, stopper_metric=config.stopper_metric, compute_all_metrics= True)\n",
    "                wandb.log({\"avg_loss\": train_loss, \"val_loss\": val_loss, f\"{config.stopper_metric}\": score})\n",
    "\n",
    "        # early_stopper(score)\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(\"Early stopping triggered at epoch\", epoch)\n",
    "        #     break\n",
    "\n",
    "    print(\"Training done.\")\n",
    "\n",
    "train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type=config.labels_type, eval_period = eval_period)\n",
    "evaluate(config,  val_loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric='hit_at_10')\n",
    "evaluate(config, test_loader, model, criterion, compute_all_metrics=False, loader_type='test'      , stopper_metric='hit_at_10')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/ebutz/ESL2024/code/utils' )\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from nxontology.imports import from_file\n",
    "import wandb\n",
    "from play_with_complex import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from model_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from torchmetrics.retrieval import RetrievalMRR, RetrievalHitRate\n",
    "import play_with_complex as pwc\n",
    "\n",
    "# How to calculate loss ?\n",
    "# Choices are : 'usual', 'rand_linsim', linsim', 'gaussian_noise'\n",
    "labels_type = 'gaussian_noise'\n",
    "\n",
    "# How many epochs between 2 time-consuming evaluations ?\n",
    "eval_period = 1\n",
    "\n",
    "# Wandb\n",
    "xp_name = 'GNN Baseline VS LinSim VS Noise'\n",
    "run_name = f'{labels_type} labels'\n",
    "print(xp_name,':',run_name)\n",
    "\n",
    "# Iric\n",
    "iric_path = '/home/ebutz/ESL2024/data/full_iric/iric.csv'\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/full_iric/altailed_mapped_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle'\n",
    "check_dicts = True\n",
    "\n",
    "ontology_path = '/home/ebutz/ESL2024/data/go-basic.json.gz'\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: '{device}'\")\n",
    "\n",
    "run = wandb.init(project=xp_name, name = run_name)\n",
    "config = wandb.config\n",
    "config.labels_type = labels_type\n",
    "config.val_ratio=0.1\n",
    "config.homogeneous=False\n",
    "config.scorelist_size=1000\n",
    "config.split_ratio=0.8\n",
    "config.val_ratio=0.1\n",
    "config.test_ratio=0.1\n",
    "config.num_neighbors=[70,55,13,89,85]\n",
    "config.batch_size=1024\n",
    "config.train_neg_sampling_ratio=224\n",
    "config.epochs=18\n",
    "config.disjoint_train_ratio=0.6\n",
    "config.lr=0.0015308253347932983\n",
    "config.stopper_metric= 'hit_at_10'\n",
    "config.stopper_direction=\"maximize\"\n",
    "config.stopper_patience=5\n",
    "config.stopper_frequency=1\n",
    "config.stopper_relative_delta=0.05\n",
    "config.gamma=1.3\n",
    "config.alpha=0.42680473078813763\n",
    "config.gnn_layer='ResGatedGraphConv'\n",
    "config.dropout=0.1\n",
    "config.norm='DiffGroupNorm'\n",
    "config.aggregation = 'min'\n",
    "config.hidden_channels=115\n",
    "config.num_layers=3\n",
    "config.attention_heads=4\n",
    "config.homogeneous = False\n",
    "config.labels = {'head' : 'genes', 'relation' : 'gene_ontology', 'tail' : 'go'}\n",
    "\n",
    "print('Graph is homogeneous :',config.homogeneous)\n",
    "\n",
    "data = load_iric_data('/home/ebutz/ESL2024/data/full_iric/iric.csv', featureless=False)\n",
    "data  = T.ToUndirected(merge=True)(data) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "data = T.RemoveDuplicatedEdges()(data) # Remove duplicated edges\n",
    "print(data)\n",
    "print('data look valid : ',data.validate())\n",
    "\n",
    "train_data, val_data, test_data = split_data(data, config)\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data, val_data, test_data, config)\n",
    "\n",
    "# # ------------- Loading ontology ------------- #\n",
    "\n",
    "print(\"\\nLoading ontology...\")\n",
    "nxo = from_file(ontology_path)\n",
    "nxo.freeze()\n",
    "pwc.nxo = nxo\n",
    "\n",
    "df = pd.read_csv(iric_path, index_col=0, dtype=str)\n",
    "df['source_node'] = df.index\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "go_map = get_nodelist(df, 'go')\n",
    "go_to_idx = {node: i for i, node in enumerate(go_map)}\n",
    "idx_to_go = {i: node for i, node in enumerate(go_map)}\n",
    "\n",
    "# # ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = idx_to_go\n",
    "pwc.mapped_alt_tails = None\n",
    "pwc.device           = device\n",
    "\n",
    "# ------------- Model setup ------------- #\n",
    "\n",
    "gnn_layers = get_gnn_layers(config)\n",
    "norm_layers = get_norm_layers(config, len(data.node_types))\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "\n",
    "# ----------------- Loops ----------------- #\n",
    "@timer_func\n",
    "def evaluate(config, loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model.    \n",
    "    loader : DataLoader\n",
    "        The data loader to evaluate the model on.\n",
    "    model : Model\n",
    "        The model to evaluate.\n",
    "    criterion : callable\n",
    "        The loss function to use for evaluation.\n",
    "    compute_all_metrics : bool, optional\n",
    "        Whether to compute all metrics or partially. Default is False.\n",
    "    loader_type : str, optional\n",
    "        The type of the loader ('validation' or 'test'). Default is 'validation'.\n",
    "    stopper_metric : str or bool, optional\n",
    "        The metric that dictates when to stop training. If False, only the loss is computed. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of the evaluation metric and the loss, depending on the value of `stopper_metric`.\n",
    "        If compute_all_metrics is True, the function logs all metrics to W&B.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluation...\")\n",
    "    model.eval()\n",
    "    num_neg_samples = loader.neg_sampling.amount\n",
    "    with torch.no_grad():\n",
    "        if stopper_metric or compute_all_metrics:\n",
    "            ground_truths = torch.tensor([], device='cpu')\n",
    "            preds = torch.tensor([], device='cpu')\n",
    "            indexes = torch.tensor([], device='cpu')\n",
    "            total_loss, total_examples = 0, 0\n",
    "            index_end = loader.batch_size\n",
    "\n",
    "        for sampled_data in loader:\n",
    "            sampled_data = sampled_data.to(device)\n",
    "\n",
    "            batch_size = len(sampled_data[config.labels['tail']].dst_pos_index)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples = torch.ones(batch_size, device=device)\n",
    "            neg_samples = torch.zeros(num_neg_samples*batch_size, device=device)\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            if stopper_metric or compute_all_metrics: # Store preds and truths for all batches, and compute indices to calc metrics\n",
    "                index_pos = torch.arange(end=index_end, start=index_end-batch_size) # index for predictions with pos. ground_truth\n",
    "                index_neg = torch.arange(end=index_end, start=index_end-batch_size).repeat_interleave(num_neg_samples)\n",
    "                index = torch.cat((index_pos, index_neg))\n",
    "                indexes = torch.cat((indexes, index.to('cpu')))\n",
    "                preds = torch.cat((preds, pred.to('cpu')))\n",
    "                ground_truths = torch.cat((ground_truths, ground_truth.to('cpu')))\n",
    "                index_end += batch_size\n",
    "\n",
    "            eval_loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            # if not stopper_metric: # Just logging loss\n",
    "            #     wandb.log({\"running_val_loss\": eval_loss})\n",
    "            #     break\n",
    "            # else:\n",
    "            total_loss += float(eval_loss) * pred.numel() \n",
    "            total_examples += pred.numel()\n",
    "            eval_loss = total_loss / total_examples\n",
    "\n",
    "            # if stopper_metric and not compute_all_metrics:\n",
    "            #     if index_end >= 2048: # Compute approx. intermediate metric on a few datapoints to speed up hyperopt process\n",
    "            #         break \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        if stopper_metric and not compute_all_metrics: # Compute only the metric that dictates stopping\n",
    "            indexes = indexes.long()\n",
    "\n",
    "            # # Log heatmap between prediction and ground truth. Useful for visualization / debugging. overhead 0.2s per epoch for a single sample.\n",
    "            # heatmap = heatmaps(preds, ground_truths, indexes) \n",
    "            # wandb.log({f\"heatmap\": wandb.Image(heatmap)})\n",
    "            # heatmap.close() # Free up memory\n",
    "\n",
    "            match stopper_metric:\n",
    "                case \"val_loss\":\n",
    "                    eval_loss = total_loss / total_examples\n",
    "                    return eval_loss\n",
    "                \n",
    "                case \"mrr\":\n",
    "                    mrr = RetrievalMRR().to(device)\n",
    "                    return mrr(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_10\":\n",
    "                    hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "                    return hit_at_10(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_5\":\n",
    "                    hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "                    return hit_at_5(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_3\":\n",
    "                    hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "                    return hit_at_3(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case \"hit_at_1\":\n",
    "                    hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "                    return hit_at_1(preds, ground_truths, indexes=indexes), eval_loss\n",
    "                \n",
    "                case _:\n",
    "                    raise ValueError(f\"Unrecognized stopper metric: '{stopper_metric}'\")\n",
    "                \n",
    "        if compute_all_metrics: # Compute all metrics at the end of training\n",
    "            indexes = indexes.long()\n",
    "            mrr = RetrievalMRR().to(device)\n",
    "            hit_at_10 = RetrievalHitRate(top_k=10).to(device)\n",
    "            hit_at_5 = RetrievalHitRate(top_k=5).to(device)\n",
    "            hit_at_3 = RetrievalHitRate(top_k=3).to(device)\n",
    "            hit_at_1 = RetrievalHitRate(top_k=1).to(device)\n",
    "            \n",
    "            mrr = mrr(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_10 = hit_at_10(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_5 = hit_at_5(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_3 = hit_at_3(preds, ground_truths, indexes=indexes)\n",
    "            hit_at_1 = hit_at_1(preds, ground_truths, indexes=indexes)\n",
    "\n",
    "            wandb.log({\n",
    "                f\"{loader_type}MRR\": mrr, f\"{loader_type}hit_at_10\": hit_at_10, f\"{loader_type}hit_at_5\": hit_at_5, f\"{loader_type}hit_at_3\": hit_at_3, f\"{loader_type}hit_at_1\": hit_at_1\n",
    "            })\n",
    "\n",
    "    return mrr, eval_loss\n",
    "\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)\n",
    "# print(model)\n",
    "\n",
    "criterion = sigmoid_focal_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# optimizer = Lion(model.parameters(), lr=config.lr, weight_decay=1e-2)\n",
    "early_stopper = EarlyStopper(frequency=config.stopper_frequency, patience=config.stopper_patience,\n",
    "                             direction=config.stopper_direction, relative_delta=config.stopper_relative_delta)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "@timer_func\n",
    "def train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type = 'usual', eval_period = 0):\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = total_examples = 0\n",
    "\n",
    "        for sampled_data in tqdm(train_loader, desc=\"Training\"):\n",
    "\n",
    "            sampled_data = sampled_data.to(device)\n",
    "            pred = model(sampled_data, config)\n",
    "            pos_samples  = torch.ones(len(sampled_data[config.labels['tail']].dst_pos_index), device=device)\n",
    "            neg_samples  = torch.zeros(len(sampled_data[config.labels['tail']].dst_neg_index.view(-1)), device=device) # As many zeroes as there are negative samples * batch_size\n",
    "            ground_truth = torch.cat((pos_samples, neg_samples))\n",
    "\n",
    "            false_tails = sampled_data[config.labels['tail']].dst_neg_index.view(-1)\n",
    "            true_tails = sampled_data['go']['dst_pos_index']\n",
    "            true_tails = torch.repeat_interleave(true_tails,int(false_tails.size()[0]/true_tails.size()[0]))\n",
    "\n",
    "\n",
    "            if labels_type == 'linsim':\n",
    "                \n",
    "                lin_neg_samples = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "\n",
    "                pos_samples = pos_samples.to(device)\n",
    "                lin_neg_samples = lin_neg_samples.to(device)\n",
    "\n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'rand_linsim':\n",
    "\n",
    "                linsims = pwc.lin_sims_for_batch(true_tails, false_tails)\n",
    "                lin_neg_samples = pwc.shuffle_tensor(linsims)\n",
    "                lin_neg_samples.to(device)\n",
    "                \n",
    "                lin_ground_truth = torch.cat((pos_samples, lin_neg_samples))\n",
    "                loss = criterion(pred, lin_ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'gaussian_noise':\n",
    "                # Add gaussian noise to pos_samples to simulate noisy labels. Added noise must be negative and not exceed 1.\n",
    "                ground_truth += torch.normal(mean=0, std=1, size=(len(ground_truth),), device=device)\n",
    "                # apply sigmoid\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            if labels_type == 'usual':\n",
    "                ground_truth = torch.sigmoid(ground_truth)\n",
    "                loss = criterion(pred, ground_truth, gamma=config.gamma, alpha=config.alpha)\n",
    "\n",
    "            wandb.log({\"loss\": loss})\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += float(loss) * pred.numel()\n",
    "            total_examples += pred.numel()\n",
    "        train_loss = total_loss / total_examples\n",
    "        print(f\"Epoch: {epoch:03d}, Avg. Loss: {train_loss:.10f}\")\n",
    "\n",
    "\n",
    "        # scheduler.step()\n",
    "        if eval_period:\n",
    "            if epoch%eval_period == 0:\n",
    "                # Compute metrics and check for early stopping\n",
    "                score, val_loss = evaluate(config, val_loader, model, criterion, stopper_metric=config.stopper_metric, compute_all_metrics= True)\n",
    "                wandb.log({\"avg_loss\": train_loss, \"val_loss\": val_loss, f\"{config.stopper_metric}\": score})\n",
    "\n",
    "        # early_stopper(score)\n",
    "        # if early_stopper.early_stop:\n",
    "        #     print(\"Early stopping triggered at epoch\", epoch)\n",
    "        #     break\n",
    "\n",
    "    print(\"Training done.\")\n",
    "\n",
    "train(config, train_loader, val_loader, model, criterion, optimizer, early_stopper, labels_type=config.labels_type, eval_period = eval_period)\n",
    "evaluate(config,  val_loader, model, criterion, compute_all_metrics=False, loader_type='validation', stopper_metric='hit_at_10')\n",
    "evaluate(config, test_loader, model, criterion, compute_all_metrics=False, loader_type='test'      , stopper_metric='hit_at_10')\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
