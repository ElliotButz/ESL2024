{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imports...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImports...\")\n",
    "import sys\n",
    "sys.path.append('/home/ebutz/ESL2024/code/utils' )\n",
    "from play_with_complex import *\n",
    "from data_utils import *\n",
    "from train_utils import *\n",
    "from model_utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nxontology.imports import from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbutzelliot\u001b[0m (\u001b[33mesl2024\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ebutz/ESL2024/code/wandb/run-20240423_160228-yz8msv58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esl2024/ESL2024-code/runs/yz8msv58' target=\"_blank\">fallen-cherry-23</a></strong> to <a href='https://wandb.ai/esl2024/ESL2024-code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esl2024/ESL2024-code' target=\"_blank\">https://wandb.ai/esl2024/ESL2024-code</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esl2024/ESL2024-code/runs/yz8msv58' target=\"_blank\">https://wandb.ai/esl2024/ESL2024-code/runs/yz8msv58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'wandb.sdk.wandb_config.Config'>\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "config = {'homogeneous': False, 'scorelist_size': 1000, 'split_ratio': 0.8,\n",
    "          'val_ratio': 0.1, 'test_ratio': 0.1, 'num_neighbors': [70, 55, 13, 89, 85],\n",
    "          'batch_size': 1024, 'train_neg_sampling_ratio': 224, 'epochs': 18, 'disjoint_train_ratio': 0.6,\n",
    "          'lr': 0.0015308253347932983, 'stopper_metric': 'mrr', 'stopper_direction': 'maximize',\n",
    "          'stopper_patience': 5, 'stopper_frequency': 1, 'stopper_relative_delta': 0.05, 'gamma': 1.3,\n",
    "          'alpha': 0.42680473078813763, 'gnn_layer': 'ResGatedGraphConv', 'dropout': 0.1,\n",
    "          'norm': 'DiffGroupNorm', 'aggregation': 'min', 'hidden_channels': 115, 'num_layers': 3, 'attention_heads': 4}\n",
    "\n",
    "run = wandb.init(config = config)\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "config.homogeneous = False\n",
    "\n",
    "config.labels = {'head' : 'genes', 'relation' : 'gene_ontology', 'tail' : 'go'}\n",
    "\n",
    "print(type(wandb.config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/full_iric/iric.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m altails_dict_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m check_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_iric_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miric_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatureless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data  \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToUndirected(merge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(data) \u001b[38;5;66;03m# Convert the graph to an undirected graph. Creates reverse edges for each edge.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mRemoveDuplicatedEdges()(data) \u001b[38;5;66;03m# Remove duplicated edges\u001b[39;00m\n",
      "File \u001b[0;32m~/ESL2024/code/utils/train_utils.py:86\u001b[0m, in \u001b[0;36mload_iric_data\u001b[0;34m(path, featureless)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_iric_data\u001b[39m(path, featureless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Load iric data from its file, preprocess it, and return it in a format suitable for graph-based machine learning models under the PyG framework.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m        edge index arrays for different types of edges, and the number of nodes for each node type.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_node\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m     88\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/full_iric/iric.csv'"
     ]
    }
   ],
   "source": [
    "# Iric\n",
    "iric_path = '/home/ebutz/ESL2024/data/full_iric/iric.csv'\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/full_iric/altailed_mapped_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/full_iric/altail_iric_DICT.pickle'\n",
    "check_dicts = True\n",
    "\n",
    "data = load_iric_data(iric_path, featureless=False)\n",
    "data  = T.ToUndirected(merge=True)(data) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "data = T.RemoveDuplicatedEdges()(data) # Remove duplicated edges\n",
    "print(data)\n",
    "assert data.validate()\n",
    "\n",
    "train_data, val_data, test_data = split_data(data, config)\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data, val_data, test_data, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cuda check...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_iric_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ------------- Loading datas ------------- #\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m iric \u001b[38;5;241m=\u001b[39m \u001b[43mload_iric_data\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/iric.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, featureless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m display(iric)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"\\nLoading iric...\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# mapped_iric = pd.read_pickle(mapped_iric_path)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(mapped_iric.head())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ------------- Loading ontology ------------- #\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_iric_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Model to train :\n",
    "hidden_channels = 176\n",
    "batch_size      = 4096\n",
    "epochs          = 200\n",
    "eval_period     = 2\n",
    "lin_factor      = 1\n",
    "\n",
    "use_wandb  = True\n",
    "\n",
    "params_save_name = f\"PARAMS_ComplEx_6_times_{hidden_channels}_HC_{epochs}_epochs_{batch_size}_BS_on_full_iric\"\n",
    "model_parameters_path = \"/home/ebutz/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
    "\n",
    "# Ontology\n",
    "ontology_path = \"/home/ebutz/ESL2024/data/go-basic.json.gz\"\n",
    "\n",
    "# ------------- Cuda ------------- #\n",
    "\n",
    "print(\"\\nCuda check...\")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# ------------- Loading datas ------------- #\n",
    "\n",
    "iric = load_iric_data('data/iric.csv', featureless=False)\n",
    "display(iric)\n",
    "\n",
    "print(\"\\nLoading iric...\")\n",
    "mapped_iric = pd.read_pickle(mapped_iric_path)\n",
    "print(mapped_iric.head())\n",
    "print('mapped_alt_tails type :', type(mapped_iric.iloc[0]['mapped_alt_tails']))\n",
    "\n",
    "GO_to_map = mapped_iric.set_index('object')['mapped_object'].to_dict()\n",
    "map_to_GO = {value: key for key, value in GO_to_map.items()}\n",
    "\n",
    "if check_dicts:\n",
    "    looks_ok: bool = True\n",
    "    for i in tqdm(range(len(list(mapped_iric['object']))), desc = \"Checking GO to MAP dict\"):\n",
    "        if GO_to_map[mapped_iric['object'][i]]!=mapped_iric['mapped_object'][i] :\n",
    "            looks_ok = False\n",
    "    print('GO - Mapping dicts looks ok :', looks_ok)\n",
    "\n",
    "with open(altails_dict_path, 'rb') as handle:\n",
    "    mapped_alt_tails = pickle.load(handle)\n",
    "print(\"Alternative tails dict (first key-value pair):\", list(mapped_alt_tails.items())[0])\n",
    "\n",
    "# ------------- Loading ontology ------------- #\n",
    "\n",
    "print(\"\\nLoading ontology...\")\n",
    "nxo = from_file(ontology_path)\n",
    "nxo.freeze()\n",
    "pwc.nxo = nxo\n",
    "\n",
    "# ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = map_to_GO\n",
    "pwc.mapped_alt_tails = mapped_alt_tails\n",
    "pwc.device           = device\n",
    "\n",
    "# ------------- Making datasets ------------- #\n",
    "\n",
    "print(\"\\nMaking datasets...\")\n",
    "# # Edges index :\n",
    "# heads = list(mapped_iric['mapped_subject'])\n",
    "# tails = list(mapped_iric['mapped_object'])\n",
    "# edge_index = torch.tensor([heads,tails], dtype=torch.long)\n",
    "# # edges attributes :\n",
    "# edge_attributes = torch.tensor(mapped_iric['mapped_predicate'])\n",
    "\n",
    "\n",
    "# iric_pyg = Data(\n",
    "#                 num_nodes = len(set(mapped_iric['object']).union(set(mapped_iric['subject']))),\n",
    "#                 edge_index = edge_index,\n",
    "#                 edge_attr = edge_attributes\n",
    "#                 )\n",
    "\n",
    "hetero_iric = iric_pyg.to_heterogeneous()\n",
    "hetero_iric  = T.ToUndirected(merge=True)(hetero_iric) # Convert the graph to an undirected graph. Creates reverse edges for each edge.\n",
    "hetero_iric = T.RemoveDuplicatedEdges()(hetero_iric)\n",
    "print(hetero_iric)\n",
    "\n",
    "print(\"\\nDataset looks valid :\",hetero_iric.validate(raise_on_error=True))\n",
    "\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EdgeStorage' object has no attribute 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     34\u001b[0m         split \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mRandomLinkSplit(\n\u001b[1;32m     35\u001b[0m             num_val\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mval_ratio,\n\u001b[1;32m     36\u001b[0m             num_test\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_ratio,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m             add_negative_train_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# If True, neg_sampling_ratio random negative samples per positive sample to the training set\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m split(data)\n\u001b[0;32m---> 46\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhetero_iric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(data, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     split \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mRandomLinkSplit(\n\u001b[1;32m     35\u001b[0m         num_val\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mval_ratio,\n\u001b[1;32m     36\u001b[0m         num_test\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtest_ratio,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         add_negative_train_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# If True, neg_sampling_ratio random negative samples per positive sample to the training set\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/torch_geometric/transforms/base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/torch_geometric/transforms/random_link_split.py:183\u001b[0m, in \u001b[0;36mRandomLinkSplit.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    178\u001b[0m is_undirected \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m store\u001b[38;5;241m.\u001b[39mis_bipartite()\n\u001b[1;32m    179\u001b[0m is_undirected \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m (rev_edge_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    180\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(data, HeteroData)\n\u001b[1;32m    181\u001b[0m                       \u001b[38;5;129;01mand\u001b[39;00m store\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m==\u001b[39m data[rev_edge_type]\u001b[38;5;241m.\u001b[39m_key))\n\u001b[0;32m--> 183\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_undirected:\n\u001b[1;32m    185\u001b[0m     mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EdgeStorage' object has no attribute 'edge_index'"
     ]
    }
   ],
   "source": [
    "def split_data(data, config):\n",
    "    \"\"\"\n",
    "    Split the data into training, validation, and test sets using a random link split.\n",
    "\n",
    "    The split is performed on the edges of the graph, and the function ensures that the reverse edge types are split accordingly to avoid leakage.\n",
    "    See the documentation for torch_geometric.loader.RandomLinkSplit for more information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torch_geometric.data.HeteroData\n",
    "        The data to be split. This should be a HeteroData object containing the preprocessed data.\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the split. This should include the ratios for the validation and test sets, \n",
    "        the disjoint train ratio, and the neg_sampling_ratio.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_data, val_data, test_data : tuple of torch_geometric.data.HeteroData\n",
    "        A tuple containing the training, validation, and test data. Each of these is a HeteroData object.\n",
    "    \"\"\"\n",
    "    if config.labels:\n",
    "        labels = (config.labels['head'], config.labels['relation'], config.labels['tail'])\n",
    "        split = T.RandomLinkSplit(\n",
    "            num_val= config.val_ratio,\n",
    "            num_test= config.test_ratio,\n",
    "            is_undirected=True,\n",
    "            disjoint_train_ratio=config.disjoint_train_ratio, # Amount of supervision edges in train set\n",
    "            neg_sampling_ratio=0,\n",
    "            add_negative_train_samples=False, # If True, neg_sampling_ratio random negative samples per positive sample to the training set\n",
    "            edge_types=[labels], # Only use these edge types for splitting\n",
    "            rev_edge_types=[(config.labels['tail'], \"rev_\" + config.labels['relation'], config.labels['head'])], # Ensure that the reverse edge types are split accordingly to avoid leakage\n",
    "        )\n",
    "    else:\n",
    "        split = T.RandomLinkSplit(\n",
    "            num_val= config.val_ratio,\n",
    "            num_test= config.test_ratio,\n",
    "            is_undirected=True,\n",
    "            disjoint_train_ratio=config.disjoint_train_ratio, # Amount of supervision edges in train set\n",
    "            neg_sampling_ratio=0,\n",
    "            add_negative_train_samples=False, # If True, neg_sampling_ratio random negative samples per positive sample to the training set\n",
    "        )\n",
    "\n",
    "\n",
    "    return split(data)\n",
    "\n",
    "train_set, val_set, test_set = split_data(hetero_iric, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 81\u001b[0m\n\u001b[1;32m     63\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m LinkNeighborLoader(\n\u001b[1;32m     64\u001b[0m         data\u001b[38;5;241m=\u001b[39mtest_data,\n\u001b[1;32m     65\u001b[0m         num_neighbors\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_neighbors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader, test_loader\n\u001b[1;32m     78\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m build_dataloaders(train_data \u001b[38;5;241m=\u001b[39m train_set,\n\u001b[1;32m     79\u001b[0m                                                           val_data   \u001b[38;5;241m=\u001b[39m val_set,\n\u001b[1;32m     80\u001b[0m                                                           test_data  \u001b[38;5;241m=\u001b[39m test_set,\n\u001b[0;32m---> 81\u001b[0m                                                           config     \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "def build_dataloaders(train_data, val_data, test_data, config):\n",
    "    \"\"\"\n",
    "    Build data loaders for training, validation, and testing.\n",
    "\n",
    "    This function creates data loaders for the training, validation, and test sets. \n",
    "    The data loaders are used to load the data in batches during training and evaluation.\n",
    "    Note that the loaders create their own negative samples.\n",
    "    The validation and test loaders are created with a fixed number of negative samples, according to OGB's evaluation protocol.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch_geometric.data.HeteroData\n",
    "        The training data.\n",
    "    val_data : torch_geometric.data.HeteroData\n",
    "        The validation data.\n",
    "    test_data : torch_geometric.data.HeteroData\n",
    "        The test data.\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the data loaders. This should include the number of neighbors to sample, \n",
    "        the negative sampling ratio, and the batch size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader, val_loader, test_loader : tuple of torch_geometric.loader.LinkNeighborLoader\n",
    "        A tuple containing the data loaders for the training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    if config.labels:\n",
    "        edge_label_index = ((config.labels['head'], config.labels['relation'], config.labels['tail']), train_data[config.labels['head'], config.labels['relation'], config.labels['tail']].edge_label_index)\n",
    "    else:\n",
    "        edge_label_index = None\n",
    "        \n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=train_data,\n",
    "        num_neighbors=config.num_neighbors, # Sample for each edge 20 neighbors in the first hop and at most 10 in the second.\n",
    "        neg_sampling=NegativeSampling(mode=\"triplet\", amount=config.train_neg_sampling_ratio), # Samples n negative edges per positive edge in the subgraph\n",
    "        # Seed supervision edges from which to sample the subgraph that will be used for message passing.\n",
    "        edge_label_index=edge_label_index,\n",
    "        edge_label=None,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True, # Shuffle data each epoch\n",
    "        pin_memory=True,\n",
    "        num_workers=5\n",
    "    )\n",
    "\n",
    "    eval_batch_size = config.batch_size\n",
    "    if config.train_neg_sampling_ratio < 1000: # Possible OOM issues as evaluation batch size are going to be larger than during training\n",
    "        # Dynamically adapt batch size. This will cause evaluation to be slower, but it will prevent OOM errors.\n",
    "        eval_batch_size = int(config.batch_size * (config.train_neg_sampling_ratio / 1000))\n",
    "\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=config.num_neighbors,\n",
    "        neg_sampling=NegativeSampling(mode=\"triplet\", amount=config.scorelist_size), # Samples 1000 negative edges per positive edge in the subgraph\n",
    "        # Seed supervision edges from which to sample the subgraph that will be used for message passing.\n",
    "        edge_label_index=edge_label_index,\n",
    "        edge_label=None, # edge_label needs to be None for \"triplet\" negative sampling mode\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=5\n",
    "    )\n",
    "\n",
    "    test_loader = LinkNeighborLoader(\n",
    "        data=test_data,\n",
    "        num_neighbors=config.num_neighbors,\n",
    "        neg_sampling=NegativeSampling(mode=\"triplet\", amount=config.scorelist_size), # Samples 500 negative edges per positive edge in the subgraph\n",
    "        # Seed supervision edges from which to sample the subgraph that will be used for message passing.\n",
    "        edge_label_index=edge_label_index,\n",
    "        edge_label=None, # edge_label needs to be None for \"triplet\" negative sampling mode\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=5\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = build_dataloaders(train_data = train_set,\n",
    "                                                          val_data   = val_set,\n",
    "                                                          test_data  = test_set,\n",
    "                                                          config     = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import *\n",
    "from torch_geometric.nn.conv import *\n",
    "from torch_geometric.nn.norm import *\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Heterogeneous Graph Neural Network (HeteroGNN) class that applies multiple layers of a specified GNN layer to node features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_layers : int\n",
    "        The number of GNN layers to apply.\n",
    "    gnn_layer : torch.nn.Module\n",
    "        The GNN layer to apply. This layer should be capable of handling bipartite graphs with differently sized node features.\n",
    "    norm : callable\n",
    "        A normalization function to apply to the node features after each GNN layer. If None, no normalization is applied.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    convs : torch.nn.ModuleList\n",
    "        A list of GNN layers to sequentially apply.\n",
    "    norm : callable\n",
    "        The normalization function to apply to the node features.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x_dict, edge_index_dict):\n",
    "        Apply the GNN layers to the node features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, gnn_layer, norm=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # When working on bipartite graph with differently sized node features, layers need both lazy init and bipartite handling properties (see PyG GNN cheatsheet)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(deepcopy(gnn_layer))\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            if self.norm:\n",
    "                x_dict = self.norm(x_dict)\n",
    "            x_dict= x_dict.relu()\n",
    "        return x_dict\n",
    "    \n",
    "class Linear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A class that applies a linear transformation to the node pair embeddings obtained from a GNN encoder for link prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_channels : int\n",
    "        The size of the hidden layer (i.e the embedding size of nodes).\n",
    "    dropout : float\n",
    "        The dropout rate for the dropout layer.\n",
    "    norm : callable\n",
    "        A normalization function to apply to the node embeddings after the first linear transformation. If None, no normalization is applied.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    lin1 : torch.nn.Linear\n",
    "        The first linear transformation layer.\n",
    "    lin2 : torch.nn.Linear\n",
    "        The second linear transformation layer.\n",
    "    dropout : torch.nn.Dropout\n",
    "        The dropout layer.\n",
    "    norm : callable\n",
    "        The normalization function to apply to the node embeddings.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(z_dict, sampled_data):\n",
    "        Apply the linear transformations to the node embeddings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, norm=None):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * config.hidden_channels, config.hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(config.hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.norm = norm\n",
    "    def forward(self, z_dict, sampled_data, config):\n",
    "        src_idx, dst_idx = get_sampled_edge_indices(sampled_data, config)\n",
    "\n",
    "        x = torch.cat([z_dict[config.labels['tail']][src_idx], z_dict[config.labels['tail']][dst_idx]], dim=-1) # Concatenate node embeddings\n",
    "        x = self.lin1(x)\n",
    "        x = torch.relu(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "def get_gnn_layers(config):\n",
    "    \"\"\"\n",
    "    Returns possible GNN layer architectures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model. This should include the number of layers, the hidden dimension, \n",
    "        the number of attention heads (if required), the dropout rate, and the normalization layer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    layers : dict\n",
    "        A dict containing the layer architectures.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(config, 'attention_heads'):\n",
    "        config.attention_heads = 1\n",
    "\n",
    "    gnn_layers = {\n",
    "        \"GATv2Conv\" : GATv2Conv(in_channels= (-1,-1), out_channels=config.hidden_channels, heads=config.attention_heads, dropout=config.dropout, concat=False, add_self_loops=False),\n",
    "        \"TransformerConv\" : TransformerConv(in_channels= (-1,-1), out_channels=config.hidden_channels, heads=config.attention_heads, concat=False, dropout=config.dropout),\n",
    "        \"MFConv\" : MFConv(in_channels= (-1,-1), out_channels=config.hidden_channels, max_degree=10),\n",
    "        \"SAGEConv\" : SAGEConv(in_channels= (-1,-1), out_channels=config.hidden_channels),\n",
    "        \"GraphConv\" : GraphConv(in_channels= (-1,-1), out_channels=config.hidden_channels),\n",
    "        \"ResGatedGraphConv\" : ResGatedGraphConv(in_channels= (-1,-1), out_channels=config.hidden_channels),\n",
    "        \"LEConv\" : LEConv(in_channels= (-1,-1), out_channels=config.hidden_channels, aggr=config.aggregation),\n",
    "        \"GeneralConv\" : GENConv(in_channels= (-1,-1), out_channels=config.hidden_channels, aggr=config.aggregation, learn_t=True, learn_p=True)\n",
    "    }\n",
    "\n",
    "    return gnn_layers\n",
    "\n",
    "def get_norm_layers(config, nb_nodetypes):\n",
    "    \"\"\"\n",
    "    Returns possible normalization layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        An object containing the configuration parameters for the model. This should include the hidden dimension, \n",
    "        alongside the number of node types in the graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    norm_layers : dict\n",
    "        A dict containing the normalization layers.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Normalization layers pool\n",
    "    norm_layers = {\n",
    "        \"BatchNorm\" : BatchNorm(config.hidden_channels),\n",
    "        \"LayerNorm\" : LayerNorm(config.hidden_channels),\n",
    "        \"InstanceNorm\" : InstanceNorm(config.hidden_channels),\n",
    "        \"GraphNorm\" : GraphNorm(config.hidden_channels),\n",
    "        \"GraphSizeNorm\" : GraphSizeNorm(),\n",
    "        \"MeanSubtractionNorm\" : MeanSubtractionNorm(),\n",
    "        \"DiffGroupNorm\" : DiffGroupNorm(config.hidden_channels, groups=nb_nodetypes),\n",
    "        \"None\" : None\n",
    "    }\n",
    "\n",
    "    return norm_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Model class that combines an encoder and a decoder for processing graph data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : object\n",
    "        A configuration object that contains the parameters for the model. It should have the following attributes:\n",
    "        - num_layers: The number of layers in the encoder.\n",
    "        - gnn_layer: The index of the GNN layer to use in the encoder.\n",
    "        - norm: The index of the normalization function to use in the encoder and decoder.\n",
    "        - aggregation: The aggregation method to use in the encoder. It should be one of \"sum\", \"mean\", \"min\", \"max\", or \"mul\".\n",
    "        - hidden_channels: The size of the hidden layer in the decoder.\n",
    "        - dropout: The dropout rate for the decoder.\n",
    "    norm_layers : dict of callable\n",
    "        A list of normalization functions. The function to use is selected by the 'norm' attribute of the config object.\n",
    "    gnn_layers : dict of torch.nn.Module\n",
    "        A list of GNN layers. The layer to use is selected by the 'gnn_layer' attribute of the config object.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    encoder : HeteroGNN\n",
    "        The encoder part of the model.\n",
    "    decoder : Linear\n",
    "        The decoder part of the model.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(sampled_data):\n",
    "        Apply the encoder and decoder to the sampled data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data, norm_layers, gnn_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = HeteroGNN(num_layers=config.num_layers, gnn_layer=gnn_layers[config.gnn_layer], norm=norm_layers[config.norm])\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr=config.aggregation)\n",
    "        self.decoder = Linear(config, norm=norm_layers[config.norm])\n",
    "\n",
    "    def forward(self, sampled_data, config):\n",
    "        z_dict = self.encoder(sampled_data.x_dict, sampled_data.edge_index_dict)\n",
    "        return self.decoder(z_dict, sampled_data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hetero_iric\n",
    "\n",
    "gnn_layers = get_gnn_layers(config)\n",
    "norm_layers = get_norm_layers(config, len(data.node_types))\n",
    "\n",
    "model = Model(config, data, norm_layers, gnn_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
