{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQDup52LepGf"
      },
      "source": [
        "# Imports, install and mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdMkskBXNLUF",
        "outputId": "ff02518a-fc51-41b4-bc43-d319dd151544"
      },
      "outputs": [],
      "source": [
        "# ! pip install cuda\n",
        "# ! pip install torch_geometric\n",
        "# ! pip install dgl\n",
        "# ! pip install nxontology\n",
        "# ! pip install tensordict\n",
        "# ! pip install numpy==1.22.1\n",
        "# ! pip install pandas\n",
        "# ! pip install tensorflow\n",
        "# ! pip install scipy\n",
        "# ! pip install pydantic\n",
        "# ! pip install matplotlib\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import ComplEx\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "from torch_geometric.loader import DataLoader\n",
        "from nxontology.imports import from_file\n",
        "import pickle\n",
        "import wandb\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Settings \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Datas\n",
        "mapped_iric_path = '/home/elliot/Documents/ESL2024/data/little_mapped_Os_to_GO_iric.tsv'\n",
        "datasets_save_path = '/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/little_dataset_'\n",
        "\n",
        "val_path = datasets_save_path + 'VAL' +  '.pickle'\n",
        "test_path = datasets_save_path + 'TEST' +  '.pickle'\n",
        "train_path = datasets_save_path + 'TRAIN' +  '.pickle'\n",
        "\n",
        "# Model to re-train :\n",
        "hidden_channels = 5\n",
        "batch_size = 4096\n",
        "epochs = 500\n",
        "lin_factor = 0.5\n",
        "\n",
        "params_save_name = f\"PARAMS_ComplEx_6_times_{hidden_channels}_HC_{epochs}_epochs_{batch_size}_BS_on_full_Os_GO\"\n",
        "model_parameters_path = \"/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
        "\n",
        "# Ontology\n",
        "url = \"/home/elliot/Documents/ESL2024/data/go-basic.json.gz\"\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu' # Tip : Use cpu for debugging\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>predicate</th>\n",
              "      <th>object</th>\n",
              "      <th>mapped_subject</th>\n",
              "      <th>mapped_predicate</th>\n",
              "      <th>mapped_object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0031267</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0006886</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005622</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005623</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>1720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0090630</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>OsNippo01g223000</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005784</td>\n",
              "      <td>1980</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005634</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>2657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005737</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0003676</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>2811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0090305</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10001 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                subject      predicate      object  mapped_subject  \\\n",
              "0      OsNippo01g010050  gene ontology  GO:0031267            2270   \n",
              "1      OsNippo01g010050  gene ontology  GO:0006886            2270   \n",
              "2      OsNippo01g010050  gene ontology  GO:0005622            2270   \n",
              "3      OsNippo01g010050  gene ontology  GO:0005623            2270   \n",
              "4      OsNippo01g010050  gene ontology  GO:0090630            2270   \n",
              "...                 ...            ...         ...             ...   \n",
              "9996   OsNippo01g223000  gene ontology  GO:0005784            1980   \n",
              "9997   OsNippo01g223050  gene ontology  GO:0005634            1922   \n",
              "9998   OsNippo01g223050  gene ontology  GO:0005737            1922   \n",
              "9999   OsNippo01g223050  gene ontology  GO:0003676            1922   \n",
              "10000  OsNippo01g223050  gene ontology  GO:0090305            1922   \n",
              "\n",
              "       mapped_predicate  mapped_object  \n",
              "0                     0           2505  \n",
              "1                     0           2022  \n",
              "2                     0           2237  \n",
              "3                     0           1720  \n",
              "4                     0             76  \n",
              "...                 ...            ...  \n",
              "9996                  0            141  \n",
              "9997                  0           2657  \n",
              "9998                  0            160  \n",
              "9999                  0           2811  \n",
              "10000                 0            362  \n",
              "\n",
              "[10001 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10001\n",
            "Dict looks ok : True\n",
            "{2505: 'GO:0031267', 2022: 'GO:0006886', 2237: 'GO:0005622', 1720: 'GO:0005623', 76: 'GO:0090630', 515: 'GO:0043087', 1419: 'GO:0005096', 1161: 'GO:0020037', 2316: 'GO:0016705', 2918: 'GO:0055114', 1094: 'GO:0004497', 3128: 'GO:0005506', 2524: 'GO:0009055', 2490: 'GO:0016722', 1806: 'GO:0005507', 2287: 'GO:0016491', 2415: 'GO:0005886', 1736: 'GO:0009506', 132: 'GO:0046658', 3092: 'GO:0016020', 2811: 'GO:0003676', 863: 'GO:0006412', 1710: 'GO:0019843', 2678: 'GO:0003723', 340: 'GO:0042542', 832: 'GO:0000028', 2190: 'GO:0022627', 2458: 'GO:0009651', 212: 'GO:0003735', 1821: 'GO:0009414', 997: 'GO:0009737', 2511: 'GO:0050832', 1178: 'GO:0003729', 1730: 'GO:0005840', 1120: 'GO:0015935', 181: 'GO:0005783', 2768: 'GO:0016829', 653: 'GO:0006888', 2364: 'GO:0051788', 3144: 'GO:0006629', 1913: 'GO:0006635', 1542: 'GO:0043161', 13: 'GO:0009751', 1022: 'GO:0030149', 2554: 'GO:0030170', 1261: 'GO:0009407', 2418: 'GO:0005789', 2876: 'GO:0003824', 2126: 'GO:0016831', 903: 'GO:0080129', 932: 'GO:0006915', 2734: 'GO:0016830', 1520: 'GO:0019752', 496: 'GO:0006665', 368: 'GO:0008117', 160: 'GO:0005737', 2657: 'GO:0005634', 2938: 'GO:0061015', 314: 'GO:0000166', 2772: 'GO:0008270', 343: 'GO:0005515', 2745: 'GO:0046872', 1751: 'GO:0031072', 1835: 'GO:0010322', 2047: 'GO:0061077', 1564: 'GO:1902395', 2192: 'GO:0006457', 1140: 'GO:0051082', 3138: 'GO:0009507', 196: 'GO:0004629', 3230: 'GO:0048229', 286: 'GO:0016788', 3165: 'GO:0003993', 2498: 'GO:0009395', 2549: 'GO:0009409', 1158: 'GO:0048364', 1047: 'GO:0043476', 2668: 'GO:0009543', 3102: 'GO:0007623', 1744: 'GO:0009658', 2455: 'GO:0015979', 1394: 'GO:0006355', 1493: 'GO:0001228', 347: 'GO:0045944', 2253: 'GO:0043565', 2838: 'GO:0046982', 3265: 'GO:0000976', 1263: 'GO:0000786', 2728: 'GO:0003677', 550: 'GO:0010027', 2823: 'GO:0005524', 1588: 'GO:0009570', 1127: 'GO:0004765', 1960: 'GO:0009941', 235: 'GO:0009535', 2520: 'GO:0071482', 2168: 'GO:0009767', 1235: 'GO:0009579', 418: 'GO:0009536', 247: 'GO:0098807', 798: 'GO:0009416', 100: 'GO:0015031', 1826: 'GO:0005771', 1157: 'GO:0005768', 1416: 'GO:0007034', 538: 'GO:0032511', 3120: 'GO:0006900', 2402: 'GO:0042128', 2337: 'GO:0022857', 127: 'GO:0005215', 707: 'GO:1904680', 490: 'GO:0035673', 1: 'GO:0006857', 2273: 'GO:0015333', 1748: 'GO:0055085', 1397: 'GO:0016853', 3173: 'GO:0006694', 785: 'GO:0016126', 871: 'GO:0047750', 1875: 'GO:0000247', 1478: 'GO:0016125', 3222: 'GO:0004769', 1805: 'GO:0008202', 605: 'GO:0110165', 2607: 'GO:0034472', 1673: 'GO:0016180', 1599: 'GO:0032039', 2496: 'GO:0018108', 2055: 'GO:0016772', 2309: 'GO:0004713', 2301: 'GO:0004672', 3342: 'GO:0005488', 2991: 'GO:0006468', 1033: 'GO:0004675', 1191: 'GO:0061630', 2553: 'GO:0016567', 32: 'GO:0044238', 1610: 'GO:0044260', 254: 'GO:0002229', 307: 'GO:0009753', 2649: 'GO:0042803', 510: 'GO:0045893', 2414: 'GO:0019252', 611: 'GO:2000014', 724: 'GO:0045892', 1051: 'GO:0031625', 1810: 'GO:0031461', 255: 'GO:1990756', 1716: 'GO:0032436', 26: 'GO:0006281', 2295: 'GO:0032040', 2064: 'GO:0006364', 2005: 'GO:0005730', 1168: 'GO:0016458', 2793: 'GO:0000462', 1606: 'GO:0016747', 1110: 'GO:0050734', 2268: 'GO:0003777', 357: 'GO:0005874', 1512: 'GO:0007018', 2784: 'GO:0008017', 62: 'GO:0046983', 1352: 'GO:0006366', 1927: 'GO:0000978', 1797: 'GO:0006357', 2609: 'GO:0000981', 1192: 'GO:0016226', 1417: 'GO:0051536', 668: 'GO:0051537', 846: 'GO:0005739', 2695: 'GO:0005198', 833: 'GO:0097428', 740: 'GO:0016301', 3083: 'GO:0016310', 1807: 'GO:0009987', 1515: 'GO:0006890', 1010: 'GO:0051179', 1090: 'GO:0030173', 2248: 'GO:0043231', 223: 'GO:0006621', 1041: 'GO:0000302', 2630: 'GO:0002238', 2521: 'GO:0004674', 549: 'GO:0051321', 2419: 'GO:0016572', 1537: 'GO:0106310', 345: 'GO:0000077', 630: 'GO:0006974', 1055: 'GO:0000724', 1500: 'GO:0008630', 3206: 'GO:0016740', 893: 'GO:1904262', 3098: 'GO:0008380', 2682: 'GO:0010212', 465: 'GO:2001020', 2526: 'GO:0006302', 1616: 'GO:0000723', 2114: 'GO:0031467', 2479: 'GO:0019005', 188: 'GO:0031463', 1843: 'GO:0031462', 864: 'GO:0080008', 243: 'GO:0097602', 1474: 'GO:0019788', 2339: 'GO:0031146', 1656: 'GO:0043224', 1220: 'GO:0009733', 1525: 'GO:0006511', 459: 'GO:0045116', 1499: 'GO:0008299', 2519: 'GO:0016116', 608: 'GO:0046686', 786: 'GO:0030604', 638: 'GO:0030145', 1595: 'GO:0009411', 2679: 'GO:0070402', 847: 'GO:0019288', 972: 'GO:0051484', 1929: 'GO:1990429', 2636: 'GO:0016560', 2466: 'GO:0005778', 2780: 'GO:0005777', 2810: 'GO:0005102', 3043: 'GO:0005794', 525: 'GO:0016757', 374: 'GO:0006486', 352: 'GO:0010343', 2437: 'GO:0042651', 398: 'GO:0000304', 2542: 'GO:0070012', 3289: 'GO:0004252', 1238: 'GO:0016787', 1714: 'GO:0006508', 3030: 'GO:0046688', 1543: 'GO:0008233', 3036: 'GO:0008236', 984: 'GO:0004175', 1408: 'GO:0070008', 2906: 'GO:0005829', 1253: 'GO:2000112', 2422: 'GO:0000977', 1860: 'GO:0090575', 56: 'GO:0003700', 2326: 'GO:0008134', 2448: 'GO:0006203', 1757: 'GO:0042254', 1737: 'GO:0010109', 71: 'GO:0008081', 471: 'GO:0008832', 472: 'GO:0003714', 2397: 'GO:0000785', 804: 'GO:0000122', 878: 'GO:0016575', 2775: 'GO:0000118', 2109: 'GO:0035435', 213: 'GO:0016036', 162: 'GO:0005802', 1338: 'GO:0006817', 1684: 'GO:0034224', 1946: 'GO:0034756', 2220: 'GO:0000822', 887: 'GO:0010106', 1148: 'GO:0015114', 2618: 'GO:0008152', 2350: 'GO:0016746', 220: 'GO:0003985', 233: 'GO:0003988', 1897: 'GO:0007166', 92: 'GO:0006099', 844: 'GO:0015977', 539: 'GO:0008964', 1273: 'GO:0030599', 2972: 'GO:0043086', 184: 'GO:0004857', 283: 'GO:0003755', 2166: 'GO:0000413', 1731: 'GO:1990124', 81: 'GO:0034063', 990: 'GO:0017148', 487: 'GO:0033962', 1961: 'GO:0000932', 2843: 'GO:0031087', 3155: 'GO:0010606', 2932: 'GO:0009791', 147: 'GO:0048767', 1396: 'GO:0048765', 125: 'GO:0010030', 3038: 'GO:0009787', 520: 'GO:0048623', 1691: 'GO:0048316', 294: 'GO:0010187', 587: 'GO:0006810', 801: 'GO:0015267', 2107: 'GO:0098542', 3272: 'GO:0043531', 540: 'GO:0006952', 1225: 'GO:0016929', 2751: 'GO:0008234', 208: 'GO:0016926', 2759: 'GO:0030247', 774: 'GO:0007031', 292: 'GO:0010229', 1991: 'GO:0016884', 1226: 'GO:0042742', 2436: 'GO:0009611', 3080: 'GO:0010150', 575: 'GO:1901363', 3123: 'GO:0097159', 2850: 'GO:0003746', 144: 'GO:0012501', 976: 'GO:0006414', 2563: 'GO:0006309', 2824: 'GO:0005525', 2036: 'GO:0031348', 746: 'GO:0043069', 1128: 'GO:0003924', 2379: 'GO:0009534', 1682: 'GO:0006096', 1464: 'GO:0004332', 379: 'GO:0006979', 48: 'GO:0010287', 1637: 'GO:0030388', 277: 'GO:0034458', 218: 'GO:0003724', 2976: 'GO:0004386', 1071: 'GO:0003725', 1993: 'GO:0001558', 944: 'GO:0008654', 922: 'GO:0017157', 2345: 'GO:0019827', 626: 'GO:0106245', 3279: 'GO:0006892', 1767: 'GO:0003882', 703: 'GO:0006644', 628: 'GO:0006646', 532: 'GO:0006659', 1626: 'GO:0048831', 1715: 'GO:0080006', 778: 'GO:0000139', 3005: 'GO:0098791', 743: 'GO:0009664', 1856: 'GO:0052636', 560: 'GO:0016763', 0: 'GO:0032543', 840: 'GO:0002161', 2943: 'GO:0006429', 2160: 'GO:0016874', 925: 'GO:0009793', 18: 'GO:0048731', 269: 'GO:0004823', 2676: 'GO:0004812', 3062: 'GO:0106074', 1665: 'GO:0006418', 2297: 'GO:0004813', 2377: 'GO:0043039', 390: 'GO:0002196', 872: 'GO:0006419', 263: 'GO:0006450', 2487: 'GO:0043229', 3273: 'GO:0008168', 795: 'GO:0016279', 904: 'GO:0018026', 3277: 'GO:0032259', 656: 'GO:0018022', 2639: 'GO:0018027', 870: 'GO:0008757', 2925: 'GO:0004842', 405: 'GO:0016887', 1766: 'GO:0140359', 3044: 'GO:0042626', 2401: 'GO:0017111', 2088: 'GO:0008375', 919: 'GO:0015020', 1433: 'GO:0007275', 2147: 'GO:0010951', 3237: 'GO:0030414', 1669: 'GO:0005576', 2284: 'GO:0010466', 1333: 'GO:0004867', 205: 'GO:0045087', 1037: 'GO:0080027', 1605: 'GO:0009642', 120: 'GO:0002213', 641: 'GO:0010288', 1586: 'GO:0030001', 1164: 'GO:0010043', 3: 'GO:0030864', 2573: 'GO:0051015', 185: 'GO:0030836', 2327: 'GO:0042643', 3281: 'GO:0030042', 337: 'GO:0061631', 3089: 'GO:0016881', 1517: 'GO:0006301', 2000: 'GO:0019028', 2900: 'GO:0080188', 929: 'GO:0031047', 2063: 'GO:0042594', 604: 'GO:0055062', 1091: 'GO:0004322', 443: 'GO:0005618', 1945: 'GO:0044237', 228: 'GO:0016616', 3096: 'GO:0050662', 1851: 'GO:0000032', 1374: 'GO:0005975', 2596: 'GO:0009298', 845: 'GO:0004476', 973: 'GO:0000014', 1974: 'GO:0008309', 2587: 'GO:0004518', 3201: 'GO:0043765', 1600: 'GO:0004521', 128: 'GO:1990238', 1281: 'GO:0004519', 995: 'GO:0090502', 2907: 'GO:0006308', 362: 'GO:0090305', 2691: 'GO:0009738', 2443: 'GO:0010366', 3130: 'GO:0009688', 2953: 'GO:0016123', 103: 'GO:0009723', 629: 'GO:0031969', 3135: 'GO:0005774', 2481: 'GO:0008289', 1167: 'GO:0006869', 1489: 'GO:0008285', 1196: 'GO:0030154', 1723: 'GO:0010097', 2854: 'GO:0048437', 900: 'GO:0010094', 3172: 'GO:0048444', 799: 'GO:0010468', 579: 'GO:0010039', 1504: 'GO:0008324', 3180: 'GO:0006812', 2207: 'GO:0009739', 3004: 'GO:0005773', 2459: 'GO:0098655', 2620: 'GO:0006828', 494: 'GO:0006811', 1345: 'GO:0005384', 2623: 'GO:0010042', 161: 'GO:0010112', 1409: 'GO:0004553', 94: 'GO:0030246', 673: 'GO:0000272', 149: 'GO:0090599', 2427: 'GO:0016798', 2557: 'GO:0006351', 1747: 'GO:0009873', 1447: 'GO:0045047', 3015: 'GO:0005787', 1889: 'GO:0031090', 1307: 'GO:0006465', 2869: 'GO:0003712', 174: 'GO:0016592', 3327: 'GO:0009736', 2059: 'GO:0010090', 137: 'GO:0010026', 2483: 'GO:0009740', 987: 'GO:1990904', 1279: 'GO:0006397', 1414: 'GO:0004045', 938: 'GO:0005351', 1468: 'GO:0015293', 2398: 'GO:1902600', 3323: 'GO:0034219', 3243: 'GO:0008643', 730: 'GO:1901700', 1549: 'GO:0008610', 1778: 'GO:0009628', 642: 'GO:1990465', 544: 'GO:0006950', 1510: 'GO:0009924', 2384: 'GO:0000248', 2254: 'GO:0006633', 2924: 'GO:0010033', 2851: 'GO:1900426', 3202: 'GO:0010224', 2029: 'GO:0071219', 2086: 'GO:0005516', 2930: 'GO:0080142', 1885: 'GO:0031176', 1934: 'GO:0043169', 267: 'GO:0005509', 2522: 'GO:0019722', 2856: 'GO:0043621', 2399: 'GO:0051259', 736: 'GO:0009408', 192: 'GO:0031410', 2306: 'GO:0010286', 1442: 'GO:0009644', 1567: 'GO:0003006', 238: 'GO:0016811', 823: 'GO:0006541', 1093: 'GO:0016854', 1193: 'GO:0016855', 3032: 'GO:0009617', 1002: 'GO:0008195', 2275: 'GO:0042577', 1888: 'GO:0016311', 423: 'GO:0004190', 2643: 'GO:0030163', 203: 'GO:0015934', 1407: 'GO:0042273', 278: 'GO:0022625', 1583: 'GO:0002181', 1435: 'GO:0048510', 46: 'GO:0030912', 3081: 'GO:0009910', 1746: 'GO:0048366', 1658: 'GO:0048573', 1077: 'GO:0048527', 533: 'GO:0009741', 3217: 'GO:0007033', 1603: 'GO:0016197', 1814: 'GO:0032502', 1817: 'GO:0043622', 3072: 'GO:0055028', 661: 'GO:0004222', 2260: 'GO:0008237', 1811: 'GO:0002949', 528: 'GO:0000722', 376: 'GO:0000408', 324: 'GO:0009247', 2605: 'GO:0009058', 2442: 'GO:0046506', 3119: 'GO:0009812', 1931: 'GO:0006794', 3050: 'GO:0009813', 2748: 'GO:0046510', 1649: 'GO:0003682', 1421: 'GO:0015748', 163: 'GO:0098869', 3176: 'GO:0004601', 1609: 'GO:0071705', 3040: 'GO:0005743', 2132: 'GO:0015711', 591: 'GO:0070131', 2153: 'GO:0005759', 595: 'GO:0008494', 1092: 'GO:0097177', 1790: 'GO:0045488', 3203: 'GO:0019898', 2937: 'GO:0045721', 1643: 'GO:0007039', 2043: 'GO:0006623', 1316: 'GO:0034657', 391: 'GO:0031981', 2848: 'GO:0071712', 1939: 'GO:0030433', 1575: 'GO:0031593', 3287: 'GO:0034098', 2621: 'GO:0000176', 1476: 'GO:0000460', 2983: 'GO:0010417', 1297: 'GO:0042285', 2826: 'GO:0015018', 1234: 'GO:0009834', 1840: 'GO:0071555', 475: 'GO:0032784', 1356: 'GO:0009742', 2558: 'GO:0006094', 1020: 'GO:0046166', 2476: 'GO:0004807', 2584: 'GO:0006098', 1034: 'GO:0019563', 2195: 'GO:0000027', 1221: 'GO:0051707', 2961: 'GO:0005694', 452: 'GO:0006334', 3060: 'GO:0009620', 2243: 'GO:0030527', 2100: 'GO:0009922', 573: 'GO:0019166', 209: 'GO:0046520', 948: 'GO:0016627', 551: 'GO:0042761', 2356: 'GO:0006816', 974: 'GO:0009923', 2999: 'GO:0007030', 3074: 'GO:0010025', 1385: 'GO:0008318', 1180: 'GO:0005953', 2928: 'GO:0018342', 1997: 'GO:0018344', 3320: 'GO:0102158', 206: 'GO:0102344', 1129: 'GO:0006631', 2528: 'GO:0030176', 917: 'GO:0030148', 1629: 'GO:0102345', 2177: 'GO:0018812', 1482: 'GO:0030497', 2223: 'GO:0102343', 1902: 'GO:0032977', 2017: 'GO:0090150', 2101: 'GO:0072598', 1427: 'GO:0051205', 942: 'GO:0006536', 1745: 'GO:0006751', 2863: 'GO:0003840', 3111: 'GO:0000048', 1803: 'GO:0036374', 1683: 'GO:0006749', 2346: 'GO:0019344', 1536: 'GO:0006520', 1174: 'GO:0022626', 503: 'GO:0010332', 350: 'GO:0004815', 1019: 'GO:0006422', 1023: 'GO:0010038', 2827: 'GO:0046689', 2736: 'GO:0010446', 2658: 'GO:0048544', 1498: 'GO:0009699', 2235: 'GO:0048046', 1727: 'GO:0004714', 157: 'GO:0046777', 355: 'GO:0045292', 1777: 'GO:0016607', 476: 'GO:0000398', 3264: 'GO:0000381', 2053: 'GO:0001405', 826: 'GO:0030150', 1470: 'GO:0001671', 2180: 'GO:0008139', 1556: 'GO:0061608', 3179: 'GO:0048471', 371: 'GO:0005654', 1799: 'GO:0008565', 2579: 'GO:0005643', 937: 'GO:0006607', 1453: 'GO:0006606', 1972: 'GO:0000373', 2263: 'GO:0051603', 1255: 'GO:0004180', 2546: 'GO:0004185', 1271: 'GO:0004814', 1142: 'GO:0048608', 3317: 'GO:0006420', 784: 'GO:0042393', 875: 'GO:0006325', 1479: 'GO:0001227', 2251: 'GO:0010310', 2018: 'GO:0003995', 3185: 'GO:0050660', 1920: 'GO:0004737', 3191: 'GO:0000287', 2635: 'GO:0030976', 1554: 'GO:0000949', 1107: 'GO:0030598', 1976: 'GO:0035821', 3248: 'GO:0000154', 2622: 'GO:0009405', 3002: 'GO:0006970', 3304: 'GO:0090729', 1373: 'GO:0045007', 920: 'GO:0051775', 493: 'GO:0009845', 483: 'GO:0071944', 1879: 'GO:0042549', 657: 'GO:0004722', 3122: 'GO:0004721', 2574: 'GO:0017018', 819: 'GO:0016791', 2831: 'GO:0071818', 1463: 'GO:0045048', 621: 'GO:0009635', 1293: 'GO:0045454', 2659: 'GO:0034976', 1971: 'GO:0000278', 3231: 'GO:0005615', 2218: 'GO:0005764', 1540: 'GO:0007062', 1194: 'GO:0006260', 1871: 'GO:0006261', 631: 'GO:0042555', 723: 'GO:0005338', 3154: 'GO:0015931', 670: 'GO:0015790', 1153: 'GO:0005464', 2805: 'GO:1901264', 2974: 'GO:0015297', 2420: 'GO:0010520', 620: 'GO:0051026', 882: 'GO:0003779', 688: 'GO:0015035', 1820: 'GO:0022900', 2447: 'GO:0006662', 553: 'GO:1904294', 2731: 'GO:0006497', 951: 'GO:0080025', 384: 'GO:0034045', 2673: 'GO:0006914', 2860: 'GO:0000045', 3170: 'GO:0032266', 454: 'GO:0044804', 1688: 'GO:0000422', 1831: 'GO:0034497', 2046: 'GO:0006004', 389: 'GO:0010199', 2255: 'GO:1900369', 582: 'GO:0043447', 6: 'GO:0060147', 1782: 'GO:0006723', 2948: 'GO:0008483', 415: 'GO:0050362', 3308: 'GO:0016846', 2786: 'GO:0009851', 2547: 'GO:0010325', 1097: 'GO:0047274', 45: 'GO:0009413', 1353: 'GO:0016409', 3113: 'GO:0018345', 2804: 'GO:0016758', 1957: 'GO:0097502', 2095: 'GO:0000030', 1068: 'GO:0015714', 101: 'GO:0015121', 258: 'GO:0071917', 970: 'GO:0035436', 749: 'GO:0089722', 3161: 'GO:0009670', 531: 'GO:0015120', 288: 'GO:0000184', 2908: 'GO:0046470', 1465: 'GO:0016042', 800: 'GO:0006654', 1624: 'GO:0050826', 816: 'GO:0009631', 2941: 'GO:0004630', 2536: 'GO:0070290', 2130: 'GO:0042744', 1850: 'GO:0009505', 2439: 'GO:0140825', 2219: 'GO:0009555', 1738: 'GO:0004650', 3268: 'GO:0009908', 2508: 'GO:0006200', 329: 'GO:0000325', 665: 'GO:0010099', 3184: 'GO:0010114', 857: 'GO:0010218', 2687: 'GO:0010017', 2009: 'GO:0031984', 284: 'GO:0004128', 3293: 'GO:0031418', 2172: 'GO:0019511', 2765: 'GO:0051213', 637: 'GO:0018401', 2302: 'GO:0004656', 1928: 'GO:0003730', 2836: 'GO:0061158', 1882: 'GO:0048825', 2766: 'GO:0009734', 2010: 'GO:0097573', 3071: 'GO:0015038', 89: 'GO:0035251', 1625: 'GO:0008194', 1630: 'GO:0080043', 2257: 'GO:0080044', 1432: 'GO:0009451', 1415: 'GO:0040008', 3169: 'GO:0004221', 2884: 'GO:0061815', 1035: 'GO:0071370', 2280: 'GO:0004197', 1845: 'GO:0004843', 2832: 'GO:0008242', 375: 'GO:0009965', 2597: 'GO:0016579', 2412: 'GO:0042127', 2032: 'GO:1990450', 1679: 'GO:0010143', 894: 'GO:0042335', 1562: 'GO:0006570', 2533: 'GO:0006568', 297: 'GO:0010252', 2532: 'GO:0016769', 2351: 'GO:0006555', 1558: 'GO:0010326', 787: 'GO:0009641', 1717: 'GO:0009698', 1505: 'GO:1901997', 1137: 'GO:0009072', 698: 'GO:0006558', 2429: 'GO:0006569', 1056: 'GO:0010044', 1524: 'GO:0019888', 1244: 'GO:0000159', 3078: 'GO:0072542', 2278: 'GO:0031952', 2112: 'GO:0006470', 2686: 'GO:0007165', 2890: 'GO:0050790', 2363: 'GO:0048367', 2939: 'GO:0016410', 1169: 'GO:0031201', 2655: 'GO:0012505', 499: 'GO:0005484', 400: 'GO:0016192', 97: 'GO:0000149', 1130: 'GO:0048278', 1363: 'GO:0006906', 1480: 'GO:0009528', 279: 'GO:0009501', 219: 'GO:0009706', 545: 'GO:0009660', 1455: 'GO:0009306', 1306: 'GO:0031972', 1769: 'GO:0032588', 1245: 'GO:0032482', 1280: 'GO:0007264', 1861: 'GO:0006184', 2633: 'GO:0006913', 2040: 'GO:0032300', 260: 'GO:0030983', 2020: 'GO:0008094', 3319: 'GO:0043570', 2560: 'GO:0045910', 2858: 'GO:0007095', 963: 'GO:0032301', 2901: 'GO:0140664', 3227: 'GO:0000710', 710: 'GO:0035064', 773: 'GO:0006290', 72: 'GO:0036297', 2872: 'GO:0043111', 1697: 'GO:0000400', 2213: 'GO:0003684', 1545: 'GO:0032876', 820: 'GO:0032143', 2474: 'GO:0006298', 1072: 'GO:0032357', 2952: 'GO:0032405', 3045: 'GO:0051096', 2383: 'GO:0031213', 2799: 'GO:0140662', 1941: 'GO:0006833', 2874: 'GO:0015250', 2779: 'GO:0007389', 1003: 'GO:0042752', 669: 'GO:0030187', 3198: 'GO:0048511', 2913: 'GO:2000028', 1390: 'GO:0009266', 1506: 'GO:0047484', 806: 'GO:0010119', 606: 'GO:0048586', 1477: 'GO:0050896', 1937: 'GO:0004103', 1523: 'GO:0006656', 51: 'GO:0004305', 2323: 'GO:0016773', 2660: 'GO:0006657', 1319: 'GO:0034485', 1559: 'GO:0004439', 124: 'GO:0004445', 757: 'GO:0046856', 2814: 'GO:0004437', 2149: 'GO:0008622', 422: 'GO:0006396', 3117: 'GO:0031124', 2947: 'GO:0051731', 1327: 'GO:0000448', 2456: 'GO:0010431', 154: 'GO:0005635', 53: 'GO:0007568', 3137: 'GO:0046907', 1358: 'GO:0006625', 1659: 'GO:0110102', 2671: 'GO:1901259', 738: 'GO:0006399', 2186: 'GO:0070180', 457: 'GO:1902775', 2378: 'GO:0035101', 59: 'GO:0031491', 2517: 'GO:0004819', 3000: 'GO:0006425', 1288: 'GO:0048481', 1471: 'GO:0010029', 3299: 'GO:0070483', 1732: 'GO:0017172', 1308: 'GO:0016702', 2942: 'GO:0047800', 1792: 'GO:0009637', 2283: 'GO:0009881', 2803: 'GO:0050794', 2001: 'GO:0018298', 2629: 'GO:0004871', 977: 'GO:0018105', 1631: 'GO:0035556', 2446: 'GO:0019139', 411: 'GO:0071949', 1977: 'GO:0008762', 1587: 'GO:0009735', 1503: 'GO:0009725', 416: 'GO:0009690', 3142: 'GO:0016614', 383: 'GO:0016706', 2957: 'GO:0008948', 2722: 'GO:0006108', 569: 'GO:0004471', 1399: 'GO:0006090', 2911: 'GO:0004470', 403: 'GO:0051287', 224: 'GO:0004473', 2249: 'GO:0005536', 2995: 'GO:0046835', 2550: 'GO:0008865', 1425: 'GO:0019158', 1718: 'GO:0004396', 1392: 'GO:0001678', 939: 'GO:0051156', 3301: 'GO:0006006', 1501: 'GO:0004340', 2710: 'GO:0019318', 639: 'GO:0002182', 408: 'GO:0030036', 1947: 'GO:0015629', 1357: 'GO:0007010', 138: 'GO:0005856', 3048: 'GO:0005758', 1147: 'GO:0005750', 1518: 'GO:0006627', 3239: 'GO:0005741', 737: 'GO:0006002', 2277: 'GO:0061615', 2181: 'GO:0005945', 241: 'GO:0003872', 2891: 'GO:0051233', 3336: 'GO:0005819', 1105: 'GO:0035174', 3070: 'GO:0007052', 2672: 'GO:0035404', 2290: 'GO:0043987', 1050: 'GO:0005876', 446: 'GO:0000779', 1768: 'GO:0035175', 2033: 'GO:0032133', 1690: 'GO:0031616', 2829: 'GO:0032465', 1492: 'GO:0009867', 2307: 'GO:0016573', 2408: 'GO:0006338', 3275: 'GO:0003713', 1895: 'GO:0035066', 2051: 'GO:0009693', 2102: 'GO:0016847', 523: 'GO:0001510', 2665: 'GO:0008173', 3134: 'GO:0030488', 619: 'GO:0030696', 1513: 'GO:0051726', 1612: 'GO:0000003', 183: 'GO:0017053', 2269: 'GO:0009574', 239: 'GO:0009524', 1657: 'GO:0007105', 1217: 'GO:2000031', 2763: 'GO:0009682', 2844: 'GO:2000022', 1664: 'GO:0009862', 2030: 'GO:0010200', 1337: 'GO:0002237', 1670: 'GO:0010942', 672: 'GO:0001666', 3086: 'GO:0016563', 2411: 'GO:0009627', 1940: 'GO:0009625', 3023: 'GO:0009863', 1694: 'GO:0008219', 1406: 'GO:0106167', 1674: 'GO:0003743', 289: 'GO:0002188', 521: 'GO:0001731', 272: 'GO:0006413', 432: 'GO:0031540', 597: 'GO:0016132', 3026: 'GO:0010268', 207: 'GO:0016709', 1198: 'GO:0009961', 1149: 'GO:0030422', 1822: 'GO:0001172', 191: 'GO:0031380', 2971: 'GO:0003968', 2945: 'GO:0016779', 1765: 'GO:0006897', 1557: 'GO:0018107', 530: 'GO:0016298', 614: 'GO:0004638', 3063: 'GO:0006189', 2802: 'GO:0043727', 720: 'GO:0006164', 486: 'GO:0000153', 2357: 'GO:0019787', 2661: 'GO:0019776', 556: 'GO:1905392', 1098: 'GO:0048263', 1262: 'GO:0010075', 426: 'GO:0010014', 467: 'GO:0010087', 704: 'GO:0080060', 828: 'GO:0009855', 1361: 'GO:0043227', 2444: 'GO:0015036', 256: 'GO:0051028', 2187: 'GO:0044613', 2082: 'GO:0044615', 2120: 'GO:0006999', 2395: 'GO:0017056', 999: 'GO:0003697', 1780: 'GO:0005543', 1847: 'GO:0031965', 1266: 'GO:0006878', 2883: 'GO:0010476', 2535: 'GO:0048653', 194: 'GO:0009640', 2340: 'GO:0010045', 829: 'GO:0009959', 3100: 'GO:0009639', 1708: 'GO:0006297', 534: 'GO:0006271', 3097: 'GO:0003887', 666: 'GO:0043625', 1918: 'GO:1904161', 427: 'GO:0010588', 3003: 'GO:1900865', 2859: 'GO:0010305', 1183: 'GO:0006353', 285: 'GO:0043232', 1703: 'GO:0004300', 2206: 'GO:0004165', 511: 'GO:0004478', 2894: 'GO:0006556', 2148: 'GO:2000601', 1705: 'GO:0071933', 1534: 'GO:0010148', 498: 'GO:0010118', 1048: 'GO:0034237', 35: 'GO:0031227', 612: 'GO:0006506', 585: 'GO:0000026', 3288: 'GO:0052926', 1641: 'GO:0006487', 2121: 'GO:0052918', 775: 'GO:0019904', 805: 'GO:0008104', 3283: 'GO:0030014', 2060: 'GO:0052634', 881: 'GO:0009685', 873: 'GO:0045487', 2868: 'GO:0015189', 603: 'GO:0051938', 2880: 'GO:0015813', 2034: 'GO:1903401', 3309: 'GO:0006865', 2506: 'GO:0003333', 1771: 'GO:0005313', 750: 'GO:0015171', 2074: 'GO:0009992', 1145: 'GO:0006672', 2238: 'GO:0009860', 1990: 'GO:0090406', 3159: 'GO:0006668', 2534: 'GO:1902456', 2124: 'GO:0071633', 464: 'GO:0005249', 1613: 'GO:0034220', 1919: 'GO:0005244', 1267: 'GO:0071805', 759: 'GO:0006813', 1013: 'GO:0005267', 2743: 'GO:0050891', 2592: 'GO:0005216', 1276: 'GO:0034765', 7: 'GO:0008963', 2226: 'GO:0006814', 2424: 'GO:0006874', 1383: 'GO:0070588', 3087: 'GO:0051592', 1958: 'GO:0035725', 1544: 'GO:0005432', 1454: 'GO:0015369', 692: 'GO:0055074', 1201: 'GO:0071472', 1173: 'GO:0000987', 949: 'GO:0005652', 251: 'GO:2000032', 825: 'GO:0016413', 2551: 'GO:1990538', 1964: 'GO:1990937', 1571: 'GO:0015211', 3084: 'GO:0005345', 633: 'GO:0015860', 3082: 'GO:1904823', 945: 'GO:0036529', 2491: 'GO:1903189', 2561: 'GO:0036524', 3337: 'GO:0106046', 1923: 'GO:0008429', 1086: 'GO:0009909', 1074: 'GO:0015910', 1628: 'GO:0042760', 865: 'GO:0005324', 1844: 'GO:0051568', 791: 'GO:0034968', 2495: 'GO:0009294', 1569: 'GO:0018024', 598: 'GO:0060629', 2258: 'GO:0061458', 1490: 'GO:0045141', 996: 'GO:0008810', 2653: 'GO:0030245', 1989: 'GO:0070181', 105: 'GO:0051119', 1359: 'GO:0102053', 3025: 'GO:0080123', 1329: 'GO:0102057', 1042: 'GO:0102058', 495: 'GO:0009864', 3145: 'GO:0009694', 346: 'GO:0004930', 3310: 'GO:0007189', 1589: 'GO:0019222', 2432: 'GO:0006820', 1494: 'GO:0009705', 794: 'GO:0015743', 588: 'GO:0015095', 1410: 'GO:0015085', 1452: 'GO:0010270', 2771: 'GO:0033314', 1251: 'GO:0030174', 1014: 'GO:0008553', 2564: 'GO:0033178', 2895: 'GO:0046961', 8: 'GO:0015996', 1303: 'GO:0009926', 2543: 'GO:0010020', 2760: 'GO:1990542', 836: 'GO:0050661', 396: 'GO:0051607', 3055: 'GO:0009901', 2355: 'GO:0004499', 537: 'GO:0103075', 709: 'GO:1903826', 1756: 'GO:0006561', 271: 'GO:0061459', 3110: 'GO:0006844', 2979: 'GO:0006972', 2325: 'GO:0089709', 1312: 'GO:1903352', 1252: 'GO:0005290', 2981: 'GO:0000064', 1750: 'GO:0043022', 617: 'GO:0015940', 2334: 'GO:0003864', 3064: 'GO:0009269', 1819: 'GO:0008308', 758: 'GO:0015698', 2697: 'GO:0006873', 1864: 'GO:0010373', 1450: 'GO:0034256', 2787: 'GO:0042170', 2123: 'GO:0010304', 2964: 'GO:0047995', 93: 'GO:0048037', 88: 'GO:0016618', 242: 'GO:0009854', 646: 'GO:0030267', 1332: 'GO:0001732', 818: 'GO:0033290', 1036: 'GO:0005852', 2820: 'GO:0002183', 1852: 'GO:0016282', 130: 'GO:0035091', 635: 'GO:0043130', 1462: 'GO:0043328', 2610: 'GO:0040029', 610: 'GO:0048497', 930: 'GO:0052164', 2861: 'GO:0032956', 2962: 'GO:0030838', 52: 'GO:0005938', 44: 'GO:0031532', 2989: 'GO:0016477', 2200: 'GO:0042995', 3153: 'GO:0007163', 780: 'GO:0019901', 153: 'GO:0030865', 11: 'GO:0007266', 468: 'GO:0000902', 2464: 'GO:0008360', 2433: 'GO:0051302', 2935: 'GO:0007015', 2271: 'GO:0019899', 159: 'GO:0031347', 1596: 'GO:0047617', 1862: 'GO:0000151', 121: 'GO:0009116', 838: 'GO:0006307', 3314: 'GO:0016070', 1764: 'GO:0042781', 3158: 'GO:0008033', 2509: 'GO:0072684', 2244: 'GO:0035652', 1384: 'GO:0030276', 1925: 'GO:0030136', 305: 'GO:0000105', 2951: 'GO:0004399', 3261: 'GO:0008652', 2698: 'GO:0007049', 447: 'GO:0007088', 2380: 'GO:0016538', 1791: 'GO:0000307', 2368: 'GO:0000079', 2624: 'GO:0045787', 1232: 'GO:0051301', 1836: 'GO:0008284', 880: 'GO:0030244', 1854: 'GO:0051716', 1115: 'GO:1990468', 3006: 'GO:0043981', 2666: 'GO:0043982', 716: 'GO:0044154', 2473: 'GO:0043983', 3302: 'GO:0070776', 1292: 'GO:1990467', 2885: 'GO:0000123', 2308: 'GO:0035267', 2232: 'GO:0043972', 1982: 'GO:0043968', 1340: 'GO:0044772', 3298: 'GO:0006265', 837: 'GO:0000712', 2503: 'GO:0003918', 1739: 'GO:0003916', 15: 'GO:0000819', 2881: 'GO:0005793', 1260: 'GO:0048280', 928: 'GO:0012507', 1009: 'GO:0004430', 3103: 'GO:0046854', 1133: 'GO:0000419', 748: 'GO:0070921', 1062: 'GO:0006306', 522: 'GO:0005049', 1973: 'GO:0006611', 788: 'GO:0051017', 2281: 'GO:0009902', 2296: 'GO:0005884', 1590: 'GO:0060359', 2882: 'GO:0010167', 156: 'GO:0009610', 3075: 'GO:0016161', 1370: 'GO:0102229', 98: 'GO:0051539', 2956: 'GO:0016868', 1190: 'GO:0008696', 104: 'GO:0046656', 1952: 'GO:0004084', 217: 'GO:0035672', 2709: 'GO:0051087', 506: 'GO:0051085', 434: 'GO:0046943', 1781: 'GO:0015605', 2358: 'GO:0015718', 694: 'GO:0007178', 1866: 'GO:0009755', 2367: 'GO:0001653', 2341: 'GO:0009790', 2478: 'GO:0009788', 3014: 'GO:0019216', 296: 'GO:0019478', 1268: 'GO:0000049', 373: 'GO:0051499', 2977: 'GO:0051500', 306: 'GO:0004402', 2128: 'GO:0031490', 1391: 'GO:0005667', 3131: 'GO:0003680', 1830: 'GO:0010228', 2640: 'GO:0004450', 1855: 'GO:0006739', 1640: 'GO:0006102', 1342: 'GO:0031298', 2204: 'GO:0071704', 412: 'GO:0000727', 2917: 'GO:0032508', 1166: 'GO:0000811', 14: 'GO:0004659', 2075: 'GO:0009653', 600: 'GO:0033609', 2527: 'GO:0045735', 1334: 'GO:2000280', 1857: 'GO:0010497', 1903: 'GO:0046564', 1294: 'GO:0035308', 1150: 'GO:0004864', 3292: 'GO:0008253', 969: 'GO:0034196', 49: 'GO:1990052', 3042: 'GO:0009707', 3067: 'GO:0070300', 1423: 'GO:0005762', 1539: 'GO:0072344', 36: 'GO:0009933', 1224: 'GO:0005689', 1057: 'GO:0046910', 378: 'GO:0016032', 718: 'GO:0006310', 78: 'GO:0004806', 687: 'GO:0008970', 1555: 'GO:0070863', 107: 'GO:0030127', 231: 'GO:0016050', 2233: 'GO:0003400', 783: 'GO:0070971', 883: 'GO:0061024', 971: 'GO:0062208', 2586: 'GO:0010072', 2335: 'GO:0140311', 3332: 'GO:0045490', 1531: 'GO:0042545', 1487: 'GO:0045330', 1955: 'GO:0005868', 2749: 'GO:0045505', 624: 'GO:0030286', 33: 'GO:0051959', 116: 'GO:0003774', 733: 'GO:0015630', 571: 'GO:0007017', 2196: 'GO:2000582', 1300: 'GO:0005875', 369: 'GO:0008097', 2589: 'GO:0017070', 2794: 'GO:0005688', 623: 'GO:0120115', 564: 'GO:0005681', 1182: 'GO:0000956', 261: 'GO:0097526', 2585: 'GO:0000387', 3340: 'GO:0045841', 1560: 'GO:0072686', 2625: 'GO:0009629', 1486: 'GO:0009911', 70: 'GO:0000914', 265: 'GO:0000911', 2504: 'GO:0006747', 822: 'GO:0003919', 327: 'GO:0003690', 1264: 'GO:0032422', 2744: 'GO:0006986', 356: 'GO:0048658', 2717: 'GO:0000244', 3126: 'GO:2000630', 1969: 'GO:0071013', 2274: 'GO:0046540', 566: 'GO:2000636', 1677: 'GO:0005977', 527: 'GO:0006011', 1099: 'GO:0070569', 2960: 'GO:0003983', 67: 'GO:0009638', 320: 'GO:0120009', 2304: 'GO:0015914', 2634: 'GO:0008526', 2644: 'GO:0045550', 221: 'GO:0015995', 516: 'GO:0016628', 2555: 'GO:0051188', 401: 'GO:0102067', 923: 'GO:0005347', 1880: 'GO:0015867', 1573: 'GO:0016071', 1983: 'GO:0006403', 796: 'GO:0000339', 594: 'GO:0048255', 3341: 'GO:0010494', 435: 'GO:0070370', 2889: 'GO:0016209', 535: 'GO:0051920', 1400: 'GO:0034599', 168: 'GO:0008379', 200: 'GO:0015174', 2202: 'GO:0080009', 1231: 'GO:0006139', 1346: 'GO:0043414', 74: 'GO:0036396', 1401: 'GO:0090304', 302: 'GO:0070122', 102: 'GO:0000502', 2921: 'GO:0008541', 3181: 'GO:0070536', 1024: 'GO:0070628', 589: 'GO:0061578', 541: 'GO:0030162', 1290: 'GO:0005639', 714: 'GO:0007129', 719: 'GO:0043495', 685: 'GO:0006998', 809: 'GO:0030234', 2595: 'GO:0009846', 37: 'GO:0010274', 1740: 'GO:0006552', 3252: 'GO:0004419', 1001: 'GO:0046951', 2045: 'GO:0016833', 2970: 'GO:0005092', 1979: 'GO:0005968', 1043: 'GO:2000541', 2571: 'GO:0004663', 1082: 'GO:0010152', 2958: 'GO:0009609', 2115: 'GO:0008283', 148: 'GO:0000812', 1160: 'GO:0043486', 3148: 'GO:0050897', 3334: 'GO:0004869', 2565: 'GO:0006424', 3013: 'GO:0017102', 1926: 'GO:0004818', 1611: 'GO:0004176', 3125: 'GO:0051117', 3065: 'GO:0009368', 2071: 'GO:0006515', 2286: 'GO:0019781', 2601: 'GO:0032446', 282: 'GO:0036211', 2090: 'GO:0008641', 3297: 'GO:0009877', 1959: 'GO:0098552', 565: 'GO:0043248', 2514: 'GO:0006406', 1887: 'GO:0000209', 2252: 'GO:0030071', 2750: 'GO:0031145', 84: 'GO:0035194', 2493: 'GO:0048856', 298: 'GO:0004449', 940: 'GO:0008320', 445: 'GO:0044070', 2348: 'GO:0005742', 2590: 'GO:0007219', 1776: 'GO:0042500', 2606: 'GO:0016485', 1045: 'GO:0005798', 1786: 'GO:0004635', 1962: 'GO:0004636', 2967: 'GO:0004743', 361: 'GO:0030955', 2723: 'GO:0032869', 647: 'GO:0006491', 2674: 'GO:0010506', 2449: 'GO:0043067', 1749: 'GO:0017177', 2616: 'GO:0140326', 895: 'GO:0006754', 3305: 'GO:0140603', 1460: 'GO:0015662', 1084: 'GO:0045332', 2582: 'GO:0005388', 3325: 'GO:0051169', 1621: 'GO:0032544', 481: 'GO:0042802', 2923: 'GO:0019706', 117: 'GO:0018230', 2797: 'GO:0006612', 2288: 'GO:0036068', 2841: 'GO:0006636', 1598: 'GO:0010207', 2330: 'GO:1901401', 268: 'GO:0009668', 561: 'GO:0048529', 568: 'GO:0031408', 2138: 'GO:0008180', 1295: 'GO:0000338', 536: 'GO:0005315', 3233: 'GO:0009624', 22: 'GO:0000175', 2353: 'GO:0000467', 3136: 'GO:0004527', 2373: 'GO:0008310', 1274: 'GO:1902626', 1733: 'GO:0009314', 3028: 'GO:0007229', 2530: 'GO:0043023', 2021: 'GO:0000054', 2662: 'GO:0030687', 2795: 'GO:0000470', 328: 'GO:0042256', 2347: 'GO:0030117', 1283: 'GO:0006891', 1222: 'GO:0030126', 2352: 'GO:0030663', 640: 'GO:0005200', 3139: 'GO:0051258', 23: 'GO:0000226', 2725: 'GO:0031966', 2276: 'GO:0051047', 1550: 'GO:0010954', 1785: 'GO:0034620', 1064: 'GO:0044322', 732: 'GO:1904211', 3274: 'GO:0031293', 3295: 'GO:0034644', 232: 'GO:0043687', 69: 'GO:0010555', 1078: 'GO:0031977', 2873: 'GO:0016018', 2084: 'GO:0042026', 1553: 'GO:0009832', 517: 'GO:0009898', 3105: 'GO:0000815', 3290: 'GO:0010184', 869: 'GO:0032922', 2042: 'GO:0009958', 2562: 'GO:0090229', 1404: 'GO:0004729', 2994: 'GO:0006783', 2312: 'GO:0006782', 2580: 'GO:0006779', 142: 'GO:0004568', 34: 'GO:0006032', 1824: 'GO:0016998', 3225: 'GO:0080146', 1908: 'GO:0019448', 171: 'GO:0070814', 2366: 'GO:0010540', 2135: 'GO:0009630', 1483: 'GO:0008559', 2482: 'GO:0010315', 2245: 'GO:0009809', 2690: 'GO:0050801', 167: 'GO:0006730', 2300: 'GO:0015948', 524: 'GO:0080110', 1636: 'GO:0048654', 843: 'GO:0008287', 601: 'GO:0061711', 1700: 'GO:0070265', 3282: 'GO:0052386', 3188: 'GO:0045493', 113: 'GO:0009044', 1899: 'GO:0046556', 1837: 'GO:0031222', 1651: 'GO:0090447', 3285: 'GO:0008408', 229: 'GO:0044262', 1277: 'GO:0004775', 1848: 'GO:0003878', 2737: 'GO:0046912', 1354: 'GO:0006085', 2431: 'GO:0090058', 2598: 'GO:0080153', 1287: 'GO:0031359', 2741: 'GO:1990573', 3093: 'GO:0055064', 725: 'GO:1902476', 1202: 'GO:0055075', 2338: 'GO:0015379', 2806: 'GO:0006884', 1126: 'GO:0015377', 3041: 'GO:2000649', 2178: 'GO:0055078', 1867: 'GO:0031402', 3331: 'GO:0015079', 1368: 'GO:0010233', 2705: 'GO:0006821', 993: 'GO:0052689', 1742: 'GO:0004725', 2627: 'GO:0035335', 1849: 'GO:0008138', 419: 'GO:0051923', 921: 'GO:0008146', 1443: 'GO:0031405', 2632: 'GO:0016407', 3213: 'GO:0043754', 2638: 'GO:0046949', 2182: 'GO:0042732', 1760: 'GO:0048040', 2144: 'GO:0070403', 1219: 'GO:0033320', 3294: 'GO:0010416', 1894: 'GO:0008115', 2370: 'GO:0006744', 295: 'GO:0017004', 2931: 'GO:0052793', 1485: 'GO:0009646', 1793: 'GO:0015462', 1622: 'GO:0016464', 1438: 'GO:0006605', 2118: 'GO:0017038', 2035: 'GO:0071806', 2197: 'GO:0090110', 760: 'GO:0016236', 1648: 'GO:0010584', 335: 'GO:0004427', 2013: 'GO:0006796', 609: 'GO:0071011', 2980: 'GO:0046394', 164: 'GO:0031386', 2602: 'GO:0019941', 198: 'GO:0016668', 2299: 'GO:0045252', 3269: 'GO:0004148', 2822: 'GO:1990714', 2336: 'GO:0048354', 1911: 'GO:0010405', 2699: 'GO:0009059', 2936: 'GO:0018258', 1177: 'GO:0008378', 2003: 'GO:0080147', 1411: 'GO:0034645', 982: 'GO:1901137', 2576: 'GO:0061157', 2700: 'GO:1990247', 643: 'GO:0033926', 3150: 'GO:0005987', 596: 'GO:0004575', 2092: 'GO:0004564', 1511: 'GO:0010333', 1060: 'GO:0009678', 41: 'GO:0030289', 850: 'GO:0009960', 2914: 'GO:0005669', 470: 'GO:0140492', 2566: 'GO:0071108', 2966: 'GO:0009856', 1726: 'GO:0003756', 3099: 'GO:0010048', 1582: 'GO:0016207', 1089: 'GO:0106290', 2484: 'GO:0033549', 855: 'GO:0008692', 2758: 'GO:0044281', 2452: 'GO:0003857', 1787: 'GO:0080050', 1646: 'GO:0048450', 766: 'GO:0051103', 3280: 'GO:0006471', 1289: 'GO:0003950', 165: 'GO:0006273', 3276: 'GO:1990404', 2529: 'GO:0070212', 2767: 'GO:0003910', 2028: 'GO:0006807', 1642: 'GO:0006904', 580: 'GO:0060321', 9: 'GO:0090522', 2769: 'GO:0006887', 915: 'GO:0000145', 365: 'GO:0006893', 2819: 'GO:0070062', 3199: 'GO:0004364', 1813: 'GO:0042221', 1186: 'GO:0009636', 3215: 'GO:0043295', 301: 'GO:0006896', 1547: 'GO:0030123', 934: 'GO:0050613', 482: 'GO:0072583', 978: 'GO:0031982', 1519: 'GO:0072318', 1218: 'GO:0006898', 2453: 'GO:0046488', 3207: 'GO:0080030', 2654: 'GO:0080031', 3077: 'GO:0080032', 3286: 'GO:0009696', 946: 'GO:0009229', 731: 'GO:0004788', 1398: 'GO:0030975', 3115: 'GO:0006772', 1943: 'GO:0009527', 2242: 'GO:0019740', 1441: 'GO:0048307', 201: 'GO:0032934', 505: 'GO:0004556', 781: 'GO:0004352', 1627: 'GO:0006538', 2413: 'GO:0016174', 2342: 'GO:0009566', 2262: 'GO:0043020', 684: 'GO:0050665', 912: 'GO:0050664', 236: 'GO:0010266', 708: 'GO:0015937', 2707: 'GO:0004140', 2228: 'GO:1901576', 3219: 'GO:0004144', 907: 'GO:0044249', 695: 'GO:0008374', 3112: 'GO:0036172', 2724: 'GO:0004417', 2173: 'GO:0009228', 2604: 'GO:0048830', 1254: 'GO:0046825', 1966: 'GO:0006405', 1159: 'GO:0051168', 2198: 'GO:0042565', 761: 'GO:0010015', 3167: 'GO:0070816', 1798: 'GO:0008353', 151: 'GO:0032968', 721: 'GO:0004693', 2904: 'GO:0140658', 3168: 'GO:0016589', 3149: 'GO:0016584', 2179: 'GO:0016818', 884: 'GO:0016585', 1863: 'GO:0010256', 2127: 'GO:0046685', 1752: 'GO:0009615', 3079: 'GO:0010181', 2613: 'GO:0016629', 2430: 'GO:0050589', 2054: 'GO:0010023', 1686: 'GO:0009718', 2011: 'GO:0004816', 1229: 'GO:0006421', 3061: 'GO:0052716', 3200: 'GO:0008471', 1259: 'GO:0046274', 500: 'GO:0045604', 602: 'GO:2000024', 2694: 'GO:0045995', 756: 'GO:0010077', 2311: 'GO:0009423', 2539: 'GO:0009073', 280: 'GO:0004764', 2926: 'GO:0019632', 1209: 'GO:0003855', 1838: 'GO:0009607', 2324: 'GO:0044403', 3033: 'GO:0005546', 16: 'GO:0034398', 1170: 'GO:0000973', 2265: 'GO:0044614', 1741: 'GO:0010117', 436: 'GO:0015112', 3090: 'GO:0050982', 1532: 'GO:0071260', 1484: 'GO:0008381', 1828: 'GO:0005261', 1893: 'GO:0042391', 1101: 'GO:0006289', 849: 'GO:0043047', 2332: 'GO:0006268', 1568: 'GO:0005662', 2174: 'GO:0007004', 3016: 'GO:0002939', 1029: 'GO:0008175', 2990: 'GO:0006400', 1366: 'GO:0009019', 3007: 'GO:0052906', 300: 'GO:0080156', 1593: 'GO:1902065', 824: 'GO:0034051', 1914: 'GO:0010193', 1118: 'GO:0017017', 1228: 'GO:0008330', 1429: 'GO:0010225', 2968: 'GO:0043407', 2328: 'GO:0043409', 1758: 'GO:0005544', 848: 'GO:0071249', 1335: 'GO:0006437', 1095: 'GO:0004831', 1617: 'GO:0043953', 1195: 'GO:0065002', 1204: 'GO:0009567', 2692: 'GO:0009977', 2410: 'GO:0015450', 29: 'GO:0043235', 1794: 'GO:0033281', 1317: 'GO:0010242', 593: 'GO:0009523', 752: 'GO:0009654', 2454: 'GO:0016790', 586: 'GO:0000036', 2065: 'GO:0016297', 1313: 'GO:0048026', 2594: 'GO:0015086', 2600: 'GO:0070574', 2796: 'GO:0046873', 1079: 'GO:0034755', 3129: 'GO:0055072', 1446: 'GO:0071421', 189: 'GO:0006826', 1709: 'GO:0006863', 2403: 'GO:0042908', 370: 'GO:0042910', 1712: 'GO:0006855', 3101: 'GO:1990961', 1924: 'GO:0043269', 700: 'GO:0051300', 2259: 'GO:0048443', 1514: 'GO:0035195', 19: 'GO:0006379', 3009: 'GO:0016442', 2008: 'GO:0006368', 2969: 'GO:0032783', 2438: 'GO:0003711', 2246: 'GO:0008023', 2113: 'GO:0034243', 526: 'GO:0009840', 1135: 'GO:0080082', 380: 'GO:0102483', 1321: 'GO:0080083', 739: 'GO:0008422', 1645: 'GO:0004565', 1422: 'GO:0080079', 2393: 'GO:0033907', 1007: 'GO:0047668', 988: 'GO:0080081', 2985: 'GO:0031098', 936: 'GO:0007112', 172: 'GO:0000165', 1910: 'GO:0031435', 1891: 'GO:0032147', 3031: 'GO:0051019', 1579: 'GO:0004708', 2754: 'GO:0000187', 983: 'GO:0010311', 1136: 'GO:0033014', 1724: 'GO:0051123', 2973: 'GO:0046695', 1788: 'GO:0061629', 1315: 'GO:0051090', 2650: 'GO:0006367', 293: 'GO:0006352', 1461: 'GO:0000124', 960: 'GO:0016251', 3300: 'GO:0005761', 2140: 'GO:0009853', 3019: 'GO:0043879', 1865: 'GO:0010008', 851: 'GO:0080171', 1208: 'GO:0006903', 789: 'GO:1990019', 2039: 'GO:0007032', 879: 'GO:0050821', 2614: 'GO:0031647', 1375: 'GO:0006862', 170: 'GO:0035352', 634: 'GO:0051724', 866: 'GO:0005085', 2: 'GO:0030008', 1707: 'GO:0005801', 264: 'GO:0000463', 2497: 'GO:0005871', 1256: 'GO:0009558', 3338: 'GO:0032367', 1884: 'GO:0055037', 1440: 'GO:0032432', 3163: 'GO:0051639', 431: 'GO:0010069', 319: 'GO:0033612', 607: 'GO:0009865', 1039: 'GO:0019277', 2409: 'GO:0050511', 547: 'GO:0046527', 85: 'GO:0030259', 118: 'GO:0022408', 173: 'GO:0052692', 860: 'GO:0004557', 2151: 'GO:0044255', 2122: 'GO:0005763', 2425: 'GO:0009744', 3223: 'GO:0009743', 313: 'GO:0044183', 1144: 'GO:0000380', 187: 'GO:0043484', 681: 'GO:0009887', 3278: 'GO:0048440', 2485: 'GO:0009888', 2993: 'GO:0010492', 2773: 'GO:0010267', 1026: 'GO:0009616', 1314: 'GO:0048467', 508: 'GO:0042138', 1239: 'GO:0016765', 2037: 'GO:0045333', 2116: 'GO:0048034', 514: 'GO:0008495', 927: 'GO:0004311', 2239: 'GO:0006384', 1444: 'GO:0000127', 3209: 'GO:0070898', 469: 'GO:0047769', 3190: 'GO:0009094', 2677: 'GO:0009807', 1585: 'GO:0003899', 2006: 'GO:0005666', 108: 'GO:0006360', 859: 'GO:0006383', 1833: 'GO:0005665', 570: 'GO:0005736', 334: 'GO:1902555', 1815: 'GO:1902494', 1572: 'GO:0034470', 2996: 'GO:0010073', 360: 'GO:0004435', 3328: 'GO:0048507', 1215: 'GO:0008352', 1901: 'GO:0051013', 2061: 'GO:0007019', 1721: 'GO:0071004', 2815: 'GO:0005685', 751: 'GO:0098662', 2732: 'GO:0000796', 1633: 'GO:0007076', 2392: 'GO:0044547', 489: 'GO:0072587', 321: 'GO:0000148', 627: 'GO:0003843', 2800: 'GO:0006075', 1309: 'GO:0009556', 1496: 'GO:0005227', 2950: 'GO:0017119', 2603: 'GO:0036265', 449: 'GO:0008176', 1704: 'GO:0106004', 2477: 'GO:0043527', 1388: 'GO:0019852', 3156: 'GO:0006270', 807: 'GO:0017116', 2303: 'GO:1902975', 2461: 'GO:0000347', 554: 'GO:0003678', 1108: 'GO:1900864', 3311: 'GO:0019199', 699: 'GO:0030570', 2329: 'GO:0045905', 935: 'GO:0043985', 342: 'GO:0070079', 2224: 'GO:0033749', 2513: 'GO:0033746', 1328: 'GO:0070078', 891: 'GO:0080134', 3208: 'GO:0015706', 1377: 'GO:0052546', 115: 'GO:0009826', 2531: 'GO:0060773', 2916: 'GO:0007154', 955: 'GO:0061136', 662: 'GO:0016075', 197: 'GO:0090501', 2853: 'GO:0004525', 519: 'GO:0007035', 1930: 'GO:0043291', 1584: 'GO:0080005', 246: 'GO:0036361', 2317: 'GO:0047661', 1734: 'GO:0043686', 394: 'GO:0031365', 1339: 'GO:0042586', 1143: 'GO:0018206', 726: 'GO:0071218', 1886: 'GO:0030544', 876: 'GO:0015691', 1578: 'GO:0055073', 1006: 'GO:0006537', 399: 'GO:0016639', 1412: 'GO:0004354', 918: 'GO:0005844', 1299: 'GO:0004828', 3312: 'GO:0006434', 1912: 'GO:0006408', 2886: 'GO:0042743', 417: 'GO:0000323', 2440: 'GO:0006624', 735: 'GO:0010179', 3001: 'GO:0009850', 2089: 'GO:0005788', 2208: 'GO:0031902', 1998: 'GO:0042147', 1326: 'GO:0016102', 1607: 'GO:0005528', 3024: 'GO:0031204', 3140: 'GO:0031205', 2472: 'GO:0006616', 141: 'GO:0005784'}\n",
            "{'GO:0031267': 2505, 'GO:0006886': 2022, 'GO:0005622': 2237, 'GO:0005623': 1720, 'GO:0090630': 76, 'GO:0043087': 515, 'GO:0005096': 1419, 'GO:0020037': 1161, 'GO:0016705': 2316, 'GO:0055114': 2918, 'GO:0004497': 1094, 'GO:0005506': 3128, 'GO:0009055': 2524, 'GO:0016722': 2490, 'GO:0005507': 1806, 'GO:0016491': 2287, 'GO:0005886': 2415, 'GO:0009506': 1736, 'GO:0046658': 132, 'GO:0016020': 3092, 'GO:0003676': 2811, 'GO:0006412': 863, 'GO:0019843': 1710, 'GO:0003723': 2678, 'GO:0042542': 340, 'GO:0000028': 832, 'GO:0022627': 2190, 'GO:0009651': 2458, 'GO:0003735': 212, 'GO:0009414': 1821, 'GO:0009737': 997, 'GO:0050832': 2511, 'GO:0003729': 1178, 'GO:0005840': 1730, 'GO:0015935': 1120, 'GO:0005783': 181, 'GO:0016829': 2768, 'GO:0006888': 653, 'GO:0051788': 2364, 'GO:0006629': 3144, 'GO:0006635': 1913, 'GO:0043161': 1542, 'GO:0009751': 13, 'GO:0030149': 1022, 'GO:0030170': 2554, 'GO:0009407': 1261, 'GO:0005789': 2418, 'GO:0003824': 2876, 'GO:0016831': 2126, 'GO:0080129': 903, 'GO:0006915': 932, 'GO:0016830': 2734, 'GO:0019752': 1520, 'GO:0006665': 496, 'GO:0008117': 368, 'GO:0005737': 160, 'GO:0005634': 2657, 'GO:0061015': 2938, 'GO:0000166': 314, 'GO:0008270': 2772, 'GO:0005515': 343, 'GO:0046872': 2745, 'GO:0031072': 1751, 'GO:0010322': 1835, 'GO:0061077': 2047, 'GO:1902395': 1564, 'GO:0006457': 2192, 'GO:0051082': 1140, 'GO:0009507': 3138, 'GO:0004629': 196, 'GO:0048229': 3230, 'GO:0016788': 286, 'GO:0003993': 3165, 'GO:0009395': 2498, 'GO:0009409': 2549, 'GO:0048364': 1158, 'GO:0043476': 1047, 'GO:0009543': 2668, 'GO:0007623': 3102, 'GO:0009658': 1744, 'GO:0015979': 2455, 'GO:0006355': 1394, 'GO:0001228': 1493, 'GO:0045944': 347, 'GO:0043565': 2253, 'GO:0046982': 2838, 'GO:0000976': 3265, 'GO:0000786': 1263, 'GO:0003677': 2728, 'GO:0010027': 550, 'GO:0005524': 2823, 'GO:0009570': 1588, 'GO:0004765': 1127, 'GO:0009941': 1960, 'GO:0009535': 235, 'GO:0071482': 2520, 'GO:0009767': 2168, 'GO:0009579': 1235, 'GO:0009536': 418, 'GO:0098807': 247, 'GO:0009416': 798, 'GO:0015031': 100, 'GO:0005771': 1826, 'GO:0005768': 1157, 'GO:0007034': 1416, 'GO:0032511': 538, 'GO:0006900': 3120, 'GO:0042128': 2402, 'GO:0022857': 2337, 'GO:0005215': 127, 'GO:1904680': 707, 'GO:0035673': 490, 'GO:0006857': 1, 'GO:0015333': 2273, 'GO:0055085': 1748, 'GO:0016853': 1397, 'GO:0006694': 3173, 'GO:0016126': 785, 'GO:0047750': 871, 'GO:0000247': 1875, 'GO:0016125': 1478, 'GO:0004769': 3222, 'GO:0008202': 1805, 'GO:0110165': 605, 'GO:0034472': 2607, 'GO:0016180': 1673, 'GO:0032039': 1599, 'GO:0018108': 2496, 'GO:0016772': 2055, 'GO:0004713': 2309, 'GO:0004672': 2301, 'GO:0005488': 3342, 'GO:0006468': 2991, 'GO:0004675': 1033, 'GO:0061630': 1191, 'GO:0016567': 2553, 'GO:0044238': 32, 'GO:0044260': 1610, 'GO:0002229': 254, 'GO:0009753': 307, 'GO:0042803': 2649, 'GO:0045893': 510, 'GO:0019252': 2414, 'GO:2000014': 611, 'GO:0045892': 724, 'GO:0031625': 1051, 'GO:0031461': 1810, 'GO:1990756': 255, 'GO:0032436': 1716, 'GO:0006281': 26, 'GO:0032040': 2295, 'GO:0006364': 2064, 'GO:0005730': 2005, 'GO:0016458': 1168, 'GO:0000462': 2793, 'GO:0016747': 1606, 'GO:0050734': 1110, 'GO:0003777': 2268, 'GO:0005874': 357, 'GO:0007018': 1512, 'GO:0008017': 2784, 'GO:0046983': 62, 'GO:0006366': 1352, 'GO:0000978': 1927, 'GO:0006357': 1797, 'GO:0000981': 2609, 'GO:0016226': 1192, 'GO:0051536': 1417, 'GO:0051537': 668, 'GO:0005739': 846, 'GO:0005198': 2695, 'GO:0097428': 833, 'GO:0016301': 740, 'GO:0016310': 3083, 'GO:0009987': 1807, 'GO:0006890': 1515, 'GO:0051179': 1010, 'GO:0030173': 1090, 'GO:0043231': 2248, 'GO:0006621': 223, 'GO:0000302': 1041, 'GO:0002238': 2630, 'GO:0004674': 2521, 'GO:0051321': 549, 'GO:0016572': 2419, 'GO:0106310': 1537, 'GO:0000077': 345, 'GO:0006974': 630, 'GO:0000724': 1055, 'GO:0008630': 1500, 'GO:0016740': 3206, 'GO:1904262': 893, 'GO:0008380': 3098, 'GO:0010212': 2682, 'GO:2001020': 465, 'GO:0006302': 2526, 'GO:0000723': 1616, 'GO:0031467': 2114, 'GO:0019005': 2479, 'GO:0031463': 188, 'GO:0031462': 1843, 'GO:0080008': 864, 'GO:0097602': 243, 'GO:0019788': 1474, 'GO:0031146': 2339, 'GO:0043224': 1656, 'GO:0009733': 1220, 'GO:0006511': 1525, 'GO:0045116': 459, 'GO:0008299': 1499, 'GO:0016116': 2519, 'GO:0046686': 608, 'GO:0030604': 786, 'GO:0030145': 638, 'GO:0009411': 1595, 'GO:0070402': 2679, 'GO:0019288': 847, 'GO:0051484': 972, 'GO:1990429': 1929, 'GO:0016560': 2636, 'GO:0005778': 2466, 'GO:0005777': 2780, 'GO:0005102': 2810, 'GO:0005794': 3043, 'GO:0016757': 525, 'GO:0006486': 374, 'GO:0010343': 352, 'GO:0042651': 2437, 'GO:0000304': 398, 'GO:0070012': 2542, 'GO:0004252': 3289, 'GO:0016787': 1238, 'GO:0006508': 1714, 'GO:0046688': 3030, 'GO:0008233': 1543, 'GO:0008236': 3036, 'GO:0004175': 984, 'GO:0070008': 1408, 'GO:0005829': 2906, 'GO:2000112': 1253, 'GO:0000977': 2422, 'GO:0090575': 1860, 'GO:0003700': 56, 'GO:0008134': 2326, 'GO:0006203': 2448, 'GO:0042254': 1757, 'GO:0010109': 1737, 'GO:0008081': 71, 'GO:0008832': 471, 'GO:0003714': 472, 'GO:0000785': 2397, 'GO:0000122': 804, 'GO:0016575': 878, 'GO:0000118': 2775, 'GO:0035435': 2109, 'GO:0016036': 213, 'GO:0005802': 162, 'GO:0006817': 1338, 'GO:0034224': 1684, 'GO:0034756': 1946, 'GO:0000822': 2220, 'GO:0010106': 887, 'GO:0015114': 1148, 'GO:0008152': 2618, 'GO:0016746': 2350, 'GO:0003985': 220, 'GO:0003988': 233, 'GO:0007166': 1897, 'GO:0006099': 92, 'GO:0015977': 844, 'GO:0008964': 539, 'GO:0030599': 1273, 'GO:0043086': 2972, 'GO:0004857': 184, 'GO:0003755': 283, 'GO:0000413': 2166, 'GO:1990124': 1731, 'GO:0034063': 81, 'GO:0017148': 990, 'GO:0033962': 487, 'GO:0000932': 1961, 'GO:0031087': 2843, 'GO:0010606': 3155, 'GO:0009791': 2932, 'GO:0048767': 147, 'GO:0048765': 1396, 'GO:0010030': 125, 'GO:0009787': 3038, 'GO:0048623': 520, 'GO:0048316': 1691, 'GO:0010187': 294, 'GO:0006810': 587, 'GO:0015267': 801, 'GO:0098542': 2107, 'GO:0043531': 3272, 'GO:0006952': 540, 'GO:0016929': 1225, 'GO:0008234': 2751, 'GO:0016926': 208, 'GO:0030247': 2759, 'GO:0007031': 774, 'GO:0010229': 292, 'GO:0016884': 1991, 'GO:0042742': 1226, 'GO:0009611': 2436, 'GO:0010150': 3080, 'GO:1901363': 575, 'GO:0097159': 3123, 'GO:0003746': 2850, 'GO:0012501': 144, 'GO:0006414': 976, 'GO:0006309': 2563, 'GO:0005525': 2824, 'GO:0031348': 2036, 'GO:0043069': 746, 'GO:0003924': 1128, 'GO:0009534': 2379, 'GO:0006096': 1682, 'GO:0004332': 1464, 'GO:0006979': 379, 'GO:0010287': 48, 'GO:0030388': 1637, 'GO:0034458': 277, 'GO:0003724': 218, 'GO:0004386': 2976, 'GO:0003725': 1071, 'GO:0001558': 1993, 'GO:0008654': 944, 'GO:0017157': 922, 'GO:0019827': 2345, 'GO:0106245': 626, 'GO:0006892': 3279, 'GO:0003882': 1767, 'GO:0006644': 703, 'GO:0006646': 628, 'GO:0006659': 532, 'GO:0048831': 1626, 'GO:0080006': 1715, 'GO:0000139': 778, 'GO:0098791': 3005, 'GO:0009664': 743, 'GO:0052636': 1856, 'GO:0016763': 560, 'GO:0032543': 0, 'GO:0002161': 840, 'GO:0006429': 2943, 'GO:0016874': 2160, 'GO:0009793': 925, 'GO:0048731': 18, 'GO:0004823': 269, 'GO:0004812': 2676, 'GO:0106074': 3062, 'GO:0006418': 1665, 'GO:0004813': 2297, 'GO:0043039': 2377, 'GO:0002196': 390, 'GO:0006419': 872, 'GO:0006450': 263, 'GO:0043229': 2487, 'GO:0008168': 3273, 'GO:0016279': 795, 'GO:0018026': 904, 'GO:0032259': 3277, 'GO:0018022': 656, 'GO:0018027': 2639, 'GO:0008757': 870, 'GO:0004842': 2925, 'GO:0016887': 405, 'GO:0140359': 1766, 'GO:0042626': 3044, 'GO:0017111': 2401, 'GO:0008375': 2088, 'GO:0015020': 919, 'GO:0007275': 1433, 'GO:0010951': 2147, 'GO:0030414': 3237, 'GO:0005576': 1669, 'GO:0010466': 2284, 'GO:0004867': 1333, 'GO:0045087': 205, 'GO:0080027': 1037, 'GO:0009642': 1605, 'GO:0002213': 120, 'GO:0010288': 641, 'GO:0030001': 1586, 'GO:0010043': 1164, 'GO:0030864': 3, 'GO:0051015': 2573, 'GO:0030836': 185, 'GO:0042643': 2327, 'GO:0030042': 3281, 'GO:0061631': 337, 'GO:0016881': 3089, 'GO:0006301': 1517, 'GO:0019028': 2000, 'GO:0080188': 2900, 'GO:0031047': 929, 'GO:0042594': 2063, 'GO:0055062': 604, 'GO:0004322': 1091, 'GO:0005618': 443, 'GO:0044237': 1945, 'GO:0016616': 228, 'GO:0050662': 3096, 'GO:0000032': 1851, 'GO:0005975': 1374, 'GO:0009298': 2596, 'GO:0004476': 845, 'GO:0000014': 973, 'GO:0008309': 1974, 'GO:0004518': 2587, 'GO:0043765': 3201, 'GO:0004521': 1600, 'GO:1990238': 128, 'GO:0004519': 1281, 'GO:0090502': 995, 'GO:0006308': 2907, 'GO:0090305': 362, 'GO:0009738': 2691, 'GO:0010366': 2443, 'GO:0009688': 3130, 'GO:0016123': 2953, 'GO:0009723': 103, 'GO:0031969': 629, 'GO:0005774': 3135, 'GO:0008289': 2481, 'GO:0006869': 1167, 'GO:0008285': 1489, 'GO:0030154': 1196, 'GO:0010097': 1723, 'GO:0048437': 2854, 'GO:0010094': 900, 'GO:0048444': 3172, 'GO:0010468': 799, 'GO:0010039': 579, 'GO:0008324': 1504, 'GO:0006812': 3180, 'GO:0009739': 2207, 'GO:0005773': 3004, 'GO:0098655': 2459, 'GO:0006828': 2620, 'GO:0006811': 494, 'GO:0005384': 1345, 'GO:0010042': 2623, 'GO:0010112': 161, 'GO:0004553': 1409, 'GO:0030246': 94, 'GO:0000272': 673, 'GO:0090599': 149, 'GO:0016798': 2427, 'GO:0006351': 2557, 'GO:0009873': 1747, 'GO:0045047': 1447, 'GO:0005787': 3015, 'GO:0031090': 1889, 'GO:0006465': 1307, 'GO:0003712': 2869, 'GO:0016592': 174, 'GO:0009736': 3327, 'GO:0010090': 2059, 'GO:0010026': 137, 'GO:0009740': 2483, 'GO:1990904': 987, 'GO:0006397': 1279, 'GO:0004045': 1414, 'GO:0005351': 938, 'GO:0015293': 1468, 'GO:1902600': 2398, 'GO:0034219': 3323, 'GO:0008643': 3243, 'GO:1901700': 730, 'GO:0008610': 1549, 'GO:0009628': 1778, 'GO:1990465': 642, 'GO:0006950': 544, 'GO:0009924': 1510, 'GO:0000248': 2384, 'GO:0006633': 2254, 'GO:0010033': 2924, 'GO:1900426': 2851, 'GO:0010224': 3202, 'GO:0071219': 2029, 'GO:0005516': 2086, 'GO:0080142': 2930, 'GO:0031176': 1885, 'GO:0043169': 1934, 'GO:0005509': 267, 'GO:0019722': 2522, 'GO:0043621': 2856, 'GO:0051259': 2399, 'GO:0009408': 736, 'GO:0031410': 192, 'GO:0010286': 2306, 'GO:0009644': 1442, 'GO:0003006': 1567, 'GO:0016811': 238, 'GO:0006541': 823, 'GO:0016854': 1093, 'GO:0016855': 1193, 'GO:0009617': 3032, 'GO:0008195': 1002, 'GO:0042577': 2275, 'GO:0016311': 1888, 'GO:0004190': 423, 'GO:0030163': 2643, 'GO:0015934': 203, 'GO:0042273': 1407, 'GO:0022625': 278, 'GO:0002181': 1583, 'GO:0048510': 1435, 'GO:0030912': 46, 'GO:0009910': 3081, 'GO:0048366': 1746, 'GO:0048573': 1658, 'GO:0048527': 1077, 'GO:0009741': 533, 'GO:0007033': 3217, 'GO:0016197': 1603, 'GO:0032502': 1814, 'GO:0043622': 1817, 'GO:0055028': 3072, 'GO:0004222': 661, 'GO:0008237': 2260, 'GO:0002949': 1811, 'GO:0000722': 528, 'GO:0000408': 376, 'GO:0009247': 324, 'GO:0009058': 2605, 'GO:0046506': 2442, 'GO:0009812': 3119, 'GO:0006794': 1931, 'GO:0009813': 3050, 'GO:0046510': 2748, 'GO:0003682': 1649, 'GO:0015748': 1421, 'GO:0098869': 163, 'GO:0004601': 3176, 'GO:0071705': 1609, 'GO:0005743': 3040, 'GO:0015711': 2132, 'GO:0070131': 591, 'GO:0005759': 2153, 'GO:0008494': 595, 'GO:0097177': 1092, 'GO:0045488': 1790, 'GO:0019898': 3203, 'GO:0045721': 2937, 'GO:0007039': 1643, 'GO:0006623': 2043, 'GO:0034657': 1316, 'GO:0031981': 391, 'GO:0071712': 2848, 'GO:0030433': 1939, 'GO:0031593': 1575, 'GO:0034098': 3287, 'GO:0000176': 2621, 'GO:0000460': 1476, 'GO:0010417': 2983, 'GO:0042285': 1297, 'GO:0015018': 2826, 'GO:0009834': 1234, 'GO:0071555': 1840, 'GO:0032784': 475, 'GO:0009742': 1356, 'GO:0006094': 2558, 'GO:0046166': 1020, 'GO:0004807': 2476, 'GO:0006098': 2584, 'GO:0019563': 1034, 'GO:0000027': 2195, 'GO:0051707': 1221, 'GO:0005694': 2961, 'GO:0006334': 452, 'GO:0009620': 3060, 'GO:0030527': 2243, 'GO:0009922': 2100, 'GO:0019166': 573, 'GO:0046520': 209, 'GO:0016627': 948, 'GO:0042761': 551, 'GO:0006816': 2356, 'GO:0009923': 974, 'GO:0007030': 2999, 'GO:0010025': 3074, 'GO:0008318': 1385, 'GO:0005953': 1180, 'GO:0018342': 2928, 'GO:0018344': 1997, 'GO:0102158': 3320, 'GO:0102344': 206, 'GO:0006631': 1129, 'GO:0030176': 2528, 'GO:0030148': 917, 'GO:0102345': 1629, 'GO:0018812': 2177, 'GO:0030497': 1482, 'GO:0102343': 2223, 'GO:0032977': 1902, 'GO:0090150': 2017, 'GO:0072598': 2101, 'GO:0051205': 1427, 'GO:0006536': 942, 'GO:0006751': 1745, 'GO:0003840': 2863, 'GO:0000048': 3111, 'GO:0036374': 1803, 'GO:0006749': 1683, 'GO:0019344': 2346, 'GO:0006520': 1536, 'GO:0022626': 1174, 'GO:0010332': 503, 'GO:0004815': 350, 'GO:0006422': 1019, 'GO:0010038': 1023, 'GO:0046689': 2827, 'GO:0010446': 2736, 'GO:0048544': 2658, 'GO:0009699': 1498, 'GO:0048046': 2235, 'GO:0004714': 1727, 'GO:0046777': 157, 'GO:0045292': 355, 'GO:0016607': 1777, 'GO:0000398': 476, 'GO:0000381': 3264, 'GO:0001405': 2053, 'GO:0030150': 826, 'GO:0001671': 1470, 'GO:0008139': 2180, 'GO:0061608': 1556, 'GO:0048471': 3179, 'GO:0005654': 371, 'GO:0008565': 1799, 'GO:0005643': 2579, 'GO:0006607': 937, 'GO:0006606': 1453, 'GO:0000373': 1972, 'GO:0051603': 2263, 'GO:0004180': 1255, 'GO:0004185': 2546, 'GO:0004814': 1271, 'GO:0048608': 1142, 'GO:0006420': 3317, 'GO:0042393': 784, 'GO:0006325': 875, 'GO:0001227': 1479, 'GO:0010310': 2251, 'GO:0003995': 2018, 'GO:0050660': 3185, 'GO:0004737': 1920, 'GO:0000287': 3191, 'GO:0030976': 2635, 'GO:0000949': 1554, 'GO:0030598': 1107, 'GO:0035821': 1976, 'GO:0000154': 3248, 'GO:0009405': 2622, 'GO:0006970': 3002, 'GO:0090729': 3304, 'GO:0045007': 1373, 'GO:0051775': 920, 'GO:0009845': 493, 'GO:0071944': 483, 'GO:0042549': 1879, 'GO:0004722': 657, 'GO:0004721': 3122, 'GO:0017018': 2574, 'GO:0016791': 819, 'GO:0071818': 2831, 'GO:0045048': 1463, 'GO:0009635': 621, 'GO:0045454': 1293, 'GO:0034976': 2659, 'GO:0000278': 1971, 'GO:0005615': 3231, 'GO:0005764': 2218, 'GO:0007062': 1540, 'GO:0006260': 1194, 'GO:0006261': 1871, 'GO:0042555': 631, 'GO:0005338': 723, 'GO:0015931': 3154, 'GO:0015790': 670, 'GO:0005464': 1153, 'GO:1901264': 2805, 'GO:0015297': 2974, 'GO:0010520': 2420, 'GO:0051026': 620, 'GO:0003779': 882, 'GO:0015035': 688, 'GO:0022900': 1820, 'GO:0006662': 2447, 'GO:1904294': 553, 'GO:0006497': 2731, 'GO:0080025': 951, 'GO:0034045': 384, 'GO:0006914': 2673, 'GO:0000045': 2860, 'GO:0032266': 3170, 'GO:0044804': 454, 'GO:0000422': 1688, 'GO:0034497': 1831, 'GO:0006004': 2046, 'GO:0010199': 389, 'GO:1900369': 2255, 'GO:0043447': 582, 'GO:0060147': 6, 'GO:0006723': 1782, 'GO:0008483': 2948, 'GO:0050362': 415, 'GO:0016846': 3308, 'GO:0009851': 2786, 'GO:0010325': 2547, 'GO:0047274': 1097, 'GO:0009413': 45, 'GO:0016409': 1353, 'GO:0018345': 3113, 'GO:0016758': 2804, 'GO:0097502': 1957, 'GO:0000030': 2095, 'GO:0015714': 1068, 'GO:0015121': 101, 'GO:0071917': 258, 'GO:0035436': 970, 'GO:0089722': 749, 'GO:0009670': 3161, 'GO:0015120': 531, 'GO:0000184': 288, 'GO:0046470': 2908, 'GO:0016042': 1465, 'GO:0006654': 800, 'GO:0050826': 1624, 'GO:0009631': 816, 'GO:0004630': 2941, 'GO:0070290': 2536, 'GO:0042744': 2130, 'GO:0009505': 1850, 'GO:0140825': 2439, 'GO:0009555': 2219, 'GO:0004650': 1738, 'GO:0009908': 3268, 'GO:0006200': 2508, 'GO:0000325': 329, 'GO:0010099': 665, 'GO:0010114': 3184, 'GO:0010218': 857, 'GO:0010017': 2687, 'GO:0031984': 2009, 'GO:0004128': 284, 'GO:0031418': 3293, 'GO:0019511': 2172, 'GO:0051213': 2765, 'GO:0018401': 637, 'GO:0004656': 2302, 'GO:0003730': 1928, 'GO:0061158': 2836, 'GO:0048825': 1882, 'GO:0009734': 2766, 'GO:0097573': 2010, 'GO:0015038': 3071, 'GO:0035251': 89, 'GO:0008194': 1625, 'GO:0080043': 1630, 'GO:0080044': 2257, 'GO:0009451': 1432, 'GO:0040008': 1415, 'GO:0004221': 3169, 'GO:0061815': 2884, 'GO:0071370': 1035, 'GO:0004197': 2280, 'GO:0004843': 1845, 'GO:0008242': 2832, 'GO:0009965': 375, 'GO:0016579': 2597, 'GO:0042127': 2412, 'GO:1990450': 2032, 'GO:0010143': 1679, 'GO:0042335': 894, 'GO:0006570': 1562, 'GO:0006568': 2533, 'GO:0010252': 297, 'GO:0016769': 2532, 'GO:0006555': 2351, 'GO:0010326': 1558, 'GO:0009641': 787, 'GO:0009698': 1717, 'GO:1901997': 1505, 'GO:0009072': 1137, 'GO:0006558': 698, 'GO:0006569': 2429, 'GO:0010044': 1056, 'GO:0019888': 1524, 'GO:0000159': 1244, 'GO:0072542': 3078, 'GO:0031952': 2278, 'GO:0006470': 2112, 'GO:0007165': 2686, 'GO:0050790': 2890, 'GO:0048367': 2363, 'GO:0016410': 2939, 'GO:0031201': 1169, 'GO:0012505': 2655, 'GO:0005484': 499, 'GO:0016192': 400, 'GO:0000149': 97, 'GO:0048278': 1130, 'GO:0006906': 1363, 'GO:0009528': 1480, 'GO:0009501': 279, 'GO:0009706': 219, 'GO:0009660': 545, 'GO:0009306': 1455, 'GO:0031972': 1306, 'GO:0032588': 1769, 'GO:0032482': 1245, 'GO:0007264': 1280, 'GO:0006184': 1861, 'GO:0006913': 2633, 'GO:0032300': 2040, 'GO:0030983': 260, 'GO:0008094': 2020, 'GO:0043570': 3319, 'GO:0045910': 2560, 'GO:0007095': 2858, 'GO:0032301': 963, 'GO:0140664': 2901, 'GO:0000710': 3227, 'GO:0035064': 710, 'GO:0006290': 773, 'GO:0036297': 72, 'GO:0043111': 2872, 'GO:0000400': 1697, 'GO:0003684': 2213, 'GO:0032876': 1545, 'GO:0032143': 820, 'GO:0006298': 2474, 'GO:0032357': 1072, 'GO:0032405': 2952, 'GO:0051096': 3045, 'GO:0031213': 2383, 'GO:0140662': 2799, 'GO:0006833': 1941, 'GO:0015250': 2874, 'GO:0007389': 2779, 'GO:0042752': 1003, 'GO:0030187': 669, 'GO:0048511': 3198, 'GO:2000028': 2913, 'GO:0009266': 1390, 'GO:0047484': 1506, 'GO:0010119': 806, 'GO:0048586': 606, 'GO:0050896': 1477, 'GO:0004103': 1937, 'GO:0006656': 1523, 'GO:0004305': 51, 'GO:0016773': 2323, 'GO:0006657': 2660, 'GO:0034485': 1319, 'GO:0004439': 1559, 'GO:0004445': 124, 'GO:0046856': 757, 'GO:0004437': 2814, 'GO:0008622': 2149, 'GO:0006396': 422, 'GO:0031124': 3117, 'GO:0051731': 2947, 'GO:0000448': 1327, 'GO:0010431': 2456, 'GO:0005635': 154, 'GO:0007568': 53, 'GO:0046907': 3137, 'GO:0006625': 1358, 'GO:0110102': 1659, 'GO:1901259': 2671, 'GO:0006399': 738, 'GO:0070180': 2186, 'GO:1902775': 457, 'GO:0035101': 2378, 'GO:0031491': 59, 'GO:0004819': 2517, 'GO:0006425': 3000, 'GO:0048481': 1288, 'GO:0010029': 1471, 'GO:0070483': 3299, 'GO:0017172': 1732, 'GO:0016702': 1308, 'GO:0047800': 2942, 'GO:0009637': 1792, 'GO:0009881': 2283, 'GO:0050794': 2803, 'GO:0018298': 2001, 'GO:0004871': 2629, 'GO:0018105': 977, 'GO:0035556': 1631, 'GO:0019139': 2446, 'GO:0071949': 411, 'GO:0008762': 1977, 'GO:0009735': 1587, 'GO:0009725': 1503, 'GO:0009690': 416, 'GO:0016614': 3142, 'GO:0016706': 383, 'GO:0008948': 2957, 'GO:0006108': 2722, 'GO:0004471': 569, 'GO:0006090': 1399, 'GO:0004470': 2911, 'GO:0051287': 403, 'GO:0004473': 224, 'GO:0005536': 2249, 'GO:0046835': 2995, 'GO:0008865': 2550, 'GO:0019158': 1425, 'GO:0004396': 1718, 'GO:0001678': 1392, 'GO:0051156': 939, 'GO:0006006': 3301, 'GO:0004340': 1501, 'GO:0019318': 2710, 'GO:0002182': 639, 'GO:0030036': 408, 'GO:0015629': 1947, 'GO:0007010': 1357, 'GO:0005856': 138, 'GO:0005758': 3048, 'GO:0005750': 1147, 'GO:0006627': 1518, 'GO:0005741': 3239, 'GO:0006002': 737, 'GO:0061615': 2277, 'GO:0005945': 2181, 'GO:0003872': 241, 'GO:0051233': 2891, 'GO:0005819': 3336, 'GO:0035174': 1105, 'GO:0007052': 3070, 'GO:0035404': 2672, 'GO:0043987': 2290, 'GO:0005876': 1050, 'GO:0000779': 446, 'GO:0035175': 1768, 'GO:0032133': 2033, 'GO:0031616': 1690, 'GO:0032465': 2829, 'GO:0009867': 1492, 'GO:0016573': 2307, 'GO:0006338': 2408, 'GO:0003713': 3275, 'GO:0035066': 1895, 'GO:0009693': 2051, 'GO:0016847': 2102, 'GO:0001510': 523, 'GO:0008173': 2665, 'GO:0030488': 3134, 'GO:0030696': 619, 'GO:0051726': 1513, 'GO:0000003': 1612, 'GO:0017053': 183, 'GO:0009574': 2269, 'GO:0009524': 239, 'GO:0007105': 1657, 'GO:2000031': 1217, 'GO:0009682': 2763, 'GO:2000022': 2844, 'GO:0009862': 1664, 'GO:0010200': 2030, 'GO:0002237': 1337, 'GO:0010942': 1670, 'GO:0001666': 672, 'GO:0016563': 3086, 'GO:0009627': 2411, 'GO:0009625': 1940, 'GO:0009863': 3023, 'GO:0008219': 1694, 'GO:0106167': 1406, 'GO:0003743': 1674, 'GO:0002188': 289, 'GO:0001731': 521, 'GO:0006413': 272, 'GO:0031540': 432, 'GO:0016132': 597, 'GO:0010268': 3026, 'GO:0016709': 207, 'GO:0009961': 1198, 'GO:0030422': 1149, 'GO:0001172': 1822, 'GO:0031380': 191, 'GO:0003968': 2971, 'GO:0016779': 2945, 'GO:0006897': 1765, 'GO:0018107': 1557, 'GO:0016298': 530, 'GO:0004638': 614, 'GO:0006189': 3063, 'GO:0043727': 2802, 'GO:0006164': 720, 'GO:0000153': 486, 'GO:0019787': 2357, 'GO:0019776': 2661, 'GO:1905392': 556, 'GO:0048263': 1098, 'GO:0010075': 1262, 'GO:0010014': 426, 'GO:0010087': 467, 'GO:0080060': 704, 'GO:0009855': 828, 'GO:0043227': 1361, 'GO:0015036': 2444, 'GO:0051028': 256, 'GO:0044613': 2187, 'GO:0044615': 2082, 'GO:0006999': 2120, 'GO:0017056': 2395, 'GO:0003697': 999, 'GO:0005543': 1780, 'GO:0031965': 1847, 'GO:0006878': 1266, 'GO:0010476': 2883, 'GO:0048653': 2535, 'GO:0009640': 194, 'GO:0010045': 2340, 'GO:0009959': 829, 'GO:0009639': 3100, 'GO:0006297': 1708, 'GO:0006271': 534, 'GO:0003887': 3097, 'GO:0043625': 666, 'GO:1904161': 1918, 'GO:0010588': 427, 'GO:1900865': 3003, 'GO:0010305': 2859, 'GO:0006353': 1183, 'GO:0043232': 285, 'GO:0004300': 1703, 'GO:0004165': 2206, 'GO:0004478': 511, 'GO:0006556': 2894, 'GO:2000601': 2148, 'GO:0071933': 1705, 'GO:0010148': 1534, 'GO:0010118': 498, 'GO:0034237': 1048, 'GO:0031227': 35, 'GO:0006506': 612, 'GO:0000026': 585, 'GO:0052926': 3288, 'GO:0006487': 1641, 'GO:0052918': 2121, 'GO:0019904': 775, 'GO:0008104': 805, 'GO:0030014': 3283, 'GO:0052634': 2060, 'GO:0009685': 881, 'GO:0045487': 873, 'GO:0015189': 2868, 'GO:0051938': 603, 'GO:0015813': 2880, 'GO:1903401': 2034, 'GO:0006865': 3309, 'GO:0003333': 2506, 'GO:0005313': 1771, 'GO:0015171': 750, 'GO:0009992': 2074, 'GO:0006672': 1145, 'GO:0009860': 2238, 'GO:0090406': 1990, 'GO:0006668': 3159, 'GO:1902456': 2534, 'GO:0071633': 2124, 'GO:0005249': 464, 'GO:0034220': 1613, 'GO:0005244': 1919, 'GO:0071805': 1267, 'GO:0006813': 759, 'GO:0005267': 1013, 'GO:0050891': 2743, 'GO:0005216': 2592, 'GO:0034765': 1276, 'GO:0008963': 7, 'GO:0006814': 2226, 'GO:0006874': 2424, 'GO:0070588': 1383, 'GO:0051592': 3087, 'GO:0035725': 1958, 'GO:0005432': 1544, 'GO:0015369': 1454, 'GO:0055074': 692, 'GO:0071472': 1201, 'GO:0000987': 1173, 'GO:0005652': 949, 'GO:2000032': 251, 'GO:0016413': 825, 'GO:1990538': 2551, 'GO:1990937': 1964, 'GO:0015211': 1571, 'GO:0005345': 3084, 'GO:0015860': 633, 'GO:1904823': 3082, 'GO:0036529': 945, 'GO:1903189': 2491, 'GO:0036524': 2561, 'GO:0106046': 3337, 'GO:0008429': 1923, 'GO:0009909': 1086, 'GO:0015910': 1074, 'GO:0042760': 1628, 'GO:0005324': 865, 'GO:0051568': 1844, 'GO:0034968': 791, 'GO:0009294': 2495, 'GO:0018024': 1569, 'GO:0060629': 598, 'GO:0061458': 2258, 'GO:0045141': 1490, 'GO:0008810': 996, 'GO:0030245': 2653, 'GO:0070181': 1989, 'GO:0051119': 105, 'GO:0102053': 1359, 'GO:0080123': 3025, 'GO:0102057': 1329, 'GO:0102058': 1042, 'GO:0009864': 495, 'GO:0009694': 3145, 'GO:0004930': 346, 'GO:0007189': 3310, 'GO:0019222': 1589, 'GO:0006820': 2432, 'GO:0009705': 1494, 'GO:0015743': 794, 'GO:0015095': 588, 'GO:0015085': 1410, 'GO:0010270': 1452, 'GO:0033314': 2771, 'GO:0030174': 1251, 'GO:0008553': 1014, 'GO:0033178': 2564, 'GO:0046961': 2895, 'GO:0015996': 8, 'GO:0009926': 1303, 'GO:0010020': 2543, 'GO:1990542': 2760, 'GO:0050661': 836, 'GO:0051607': 396, 'GO:0009901': 3055, 'GO:0004499': 2355, 'GO:0103075': 537, 'GO:1903826': 709, 'GO:0006561': 1756, 'GO:0061459': 271, 'GO:0006844': 3110, 'GO:0006972': 2979, 'GO:0089709': 2325, 'GO:1903352': 1312, 'GO:0005290': 1252, 'GO:0000064': 2981, 'GO:0043022': 1750, 'GO:0015940': 617, 'GO:0003864': 2334, 'GO:0009269': 3064, 'GO:0008308': 1819, 'GO:0015698': 758, 'GO:0006873': 2697, 'GO:0010373': 1864, 'GO:0034256': 1450, 'GO:0042170': 2787, 'GO:0010304': 2123, 'GO:0047995': 2964, 'GO:0048037': 93, 'GO:0016618': 88, 'GO:0009854': 242, 'GO:0030267': 646, 'GO:0001732': 1332, 'GO:0033290': 818, 'GO:0005852': 1036, 'GO:0002183': 2820, 'GO:0016282': 1852, 'GO:0035091': 130, 'GO:0043130': 635, 'GO:0043328': 1462, 'GO:0040029': 2610, 'GO:0048497': 610, 'GO:0052164': 930, 'GO:0032956': 2861, 'GO:0030838': 2962, 'GO:0005938': 52, 'GO:0031532': 44, 'GO:0016477': 2989, 'GO:0042995': 2200, 'GO:0007163': 3153, 'GO:0019901': 780, 'GO:0030865': 153, 'GO:0007266': 11, 'GO:0000902': 468, 'GO:0008360': 2464, 'GO:0051302': 2433, 'GO:0007015': 2935, 'GO:0019899': 2271, 'GO:0031347': 159, 'GO:0047617': 1596, 'GO:0000151': 1862, 'GO:0009116': 121, 'GO:0006307': 838, 'GO:0016070': 3314, 'GO:0042781': 1764, 'GO:0008033': 3158, 'GO:0072684': 2509, 'GO:0035652': 2244, 'GO:0030276': 1384, 'GO:0030136': 1925, 'GO:0000105': 305, 'GO:0004399': 2951, 'GO:0008652': 3261, 'GO:0007049': 2698, 'GO:0007088': 447, 'GO:0016538': 2380, 'GO:0000307': 1791, 'GO:0000079': 2368, 'GO:0045787': 2624, 'GO:0051301': 1232, 'GO:0008284': 1836, 'GO:0030244': 880, 'GO:0051716': 1854, 'GO:1990468': 1115, 'GO:0043981': 3006, 'GO:0043982': 2666, 'GO:0044154': 716, 'GO:0043983': 2473, 'GO:0070776': 3302, 'GO:1990467': 1292, 'GO:0000123': 2885, 'GO:0035267': 2308, 'GO:0043972': 2232, 'GO:0043968': 1982, 'GO:0044772': 1340, 'GO:0006265': 3298, 'GO:0000712': 837, 'GO:0003918': 2503, 'GO:0003916': 1739, 'GO:0000819': 15, 'GO:0005793': 2881, 'GO:0048280': 1260, 'GO:0012507': 928, 'GO:0004430': 1009, 'GO:0046854': 3103, 'GO:0000419': 1133, 'GO:0070921': 748, 'GO:0006306': 1062, 'GO:0005049': 522, 'GO:0006611': 1973, 'GO:0051017': 788, 'GO:0009902': 2281, 'GO:0005884': 2296, 'GO:0060359': 1590, 'GO:0010167': 2882, 'GO:0009610': 156, 'GO:0016161': 3075, 'GO:0102229': 1370, 'GO:0051539': 98, 'GO:0016868': 2956, 'GO:0008696': 1190, 'GO:0046656': 104, 'GO:0004084': 1952, 'GO:0035672': 217, 'GO:0051087': 2709, 'GO:0051085': 506, 'GO:0046943': 434, 'GO:0015605': 1781, 'GO:0015718': 2358, 'GO:0007178': 694, 'GO:0009755': 1866, 'GO:0001653': 2367, 'GO:0009790': 2341, 'GO:0009788': 2478, 'GO:0019216': 3014, 'GO:0019478': 296, 'GO:0000049': 1268, 'GO:0051499': 373, 'GO:0051500': 2977, 'GO:0004402': 306, 'GO:0031490': 2128, 'GO:0005667': 1391, 'GO:0003680': 3131, 'GO:0010228': 1830, 'GO:0004450': 2640, 'GO:0006739': 1855, 'GO:0006102': 1640, 'GO:0031298': 1342, 'GO:0071704': 2204, 'GO:0000727': 412, 'GO:0032508': 2917, 'GO:0000811': 1166, 'GO:0004659': 14, 'GO:0009653': 2075, 'GO:0033609': 600, 'GO:0045735': 2527, 'GO:2000280': 1334, 'GO:0010497': 1857, 'GO:0046564': 1903, 'GO:0035308': 1294, 'GO:0004864': 1150, 'GO:0008253': 3292, 'GO:0034196': 969, 'GO:1990052': 49, 'GO:0009707': 3042, 'GO:0070300': 3067, 'GO:0005762': 1423, 'GO:0072344': 1539, 'GO:0009933': 36, 'GO:0005689': 1224, 'GO:0046910': 1057, 'GO:0016032': 378, 'GO:0006310': 718, 'GO:0004806': 78, 'GO:0008970': 687, 'GO:0070863': 1555, 'GO:0030127': 107, 'GO:0016050': 231, 'GO:0003400': 2233, 'GO:0070971': 783, 'GO:0061024': 883, 'GO:0062208': 971, 'GO:0010072': 2586, 'GO:0140311': 2335, 'GO:0045490': 3332, 'GO:0042545': 1531, 'GO:0045330': 1487, 'GO:0005868': 1955, 'GO:0045505': 2749, 'GO:0030286': 624, 'GO:0051959': 33, 'GO:0003774': 116, 'GO:0015630': 733, 'GO:0007017': 571, 'GO:2000582': 2196, 'GO:0005875': 1300, 'GO:0008097': 369, 'GO:0017070': 2589, 'GO:0005688': 2794, 'GO:0120115': 623, 'GO:0005681': 564, 'GO:0000956': 1182, 'GO:0097526': 261, 'GO:0000387': 2585, 'GO:0045841': 3340, 'GO:0072686': 1560, 'GO:0009629': 2625, 'GO:0009911': 1486, 'GO:0000914': 70, 'GO:0000911': 265, 'GO:0006747': 2504, 'GO:0003919': 822, 'GO:0003690': 327, 'GO:0032422': 1264, 'GO:0006986': 2744, 'GO:0048658': 356, 'GO:0000244': 2717, 'GO:2000630': 3126, 'GO:0071013': 1969, 'GO:0046540': 2274, 'GO:2000636': 566, 'GO:0005977': 1677, 'GO:0006011': 527, 'GO:0070569': 1099, 'GO:0003983': 2960, 'GO:0009638': 67, 'GO:0120009': 320, 'GO:0015914': 2304, 'GO:0008526': 2634, 'GO:0045550': 2644, 'GO:0015995': 221, 'GO:0016628': 516, 'GO:0051188': 2555, 'GO:0102067': 401, 'GO:0005347': 923, 'GO:0015867': 1880, 'GO:0016071': 1573, 'GO:0006403': 1983, 'GO:0000339': 796, 'GO:0048255': 594, 'GO:0010494': 3341, 'GO:0070370': 435, 'GO:0016209': 2889, 'GO:0051920': 535, 'GO:0034599': 1400, 'GO:0008379': 168, 'GO:0015174': 200, 'GO:0080009': 2202, 'GO:0006139': 1231, 'GO:0043414': 1346, 'GO:0036396': 74, 'GO:0090304': 1401, 'GO:0070122': 302, 'GO:0000502': 102, 'GO:0008541': 2921, 'GO:0070536': 3181, 'GO:0070628': 1024, 'GO:0061578': 589, 'GO:0030162': 541, 'GO:0005639': 1290, 'GO:0007129': 714, 'GO:0043495': 719, 'GO:0006998': 685, 'GO:0030234': 809, 'GO:0009846': 2595, 'GO:0010274': 37, 'GO:0006552': 1740, 'GO:0004419': 3252, 'GO:0046951': 1001, 'GO:0016833': 2045, 'GO:0005092': 2970, 'GO:0005968': 1979, 'GO:2000541': 1043, 'GO:0004663': 2571, 'GO:0010152': 1082, 'GO:0009609': 2958, 'GO:0008283': 2115, 'GO:0000812': 148, 'GO:0043486': 1160, 'GO:0050897': 3148, 'GO:0004869': 3334, 'GO:0006424': 2565, 'GO:0017102': 3013, 'GO:0004818': 1926, 'GO:0004176': 1611, 'GO:0051117': 3125, 'GO:0009368': 3065, 'GO:0006515': 2071, 'GO:0019781': 2286, 'GO:0032446': 2601, 'GO:0036211': 282, 'GO:0008641': 2090, 'GO:0009877': 3297, 'GO:0098552': 1959, 'GO:0043248': 565, 'GO:0006406': 2514, 'GO:0000209': 1887, 'GO:0030071': 2252, 'GO:0031145': 2750, 'GO:0035194': 84, 'GO:0048856': 2493, 'GO:0004449': 298, 'GO:0008320': 940, 'GO:0044070': 445, 'GO:0005742': 2348, 'GO:0007219': 2590, 'GO:0042500': 1776, 'GO:0016485': 2606, 'GO:0005798': 1045, 'GO:0004635': 1786, 'GO:0004636': 1962, 'GO:0004743': 2967, 'GO:0030955': 361, 'GO:0032869': 2723, 'GO:0006491': 647, 'GO:0010506': 2674, 'GO:0043067': 2449, 'GO:0017177': 1749, 'GO:0140326': 2616, 'GO:0006754': 895, 'GO:0140603': 3305, 'GO:0015662': 1460, 'GO:0045332': 1084, 'GO:0005388': 2582, 'GO:0051169': 3325, 'GO:0032544': 1621, 'GO:0042802': 481, 'GO:0019706': 2923, 'GO:0018230': 117, 'GO:0006612': 2797, 'GO:0036068': 2288, 'GO:0006636': 2841, 'GO:0010207': 1598, 'GO:1901401': 2330, 'GO:0009668': 268, 'GO:0048529': 561, 'GO:0031408': 568, 'GO:0008180': 2138, 'GO:0000338': 1295, 'GO:0005315': 536, 'GO:0009624': 3233, 'GO:0000175': 22, 'GO:0000467': 2353, 'GO:0004527': 3136, 'GO:0008310': 2373, 'GO:1902626': 1274, 'GO:0009314': 1733, 'GO:0007229': 3028, 'GO:0043023': 2530, 'GO:0000054': 2021, 'GO:0030687': 2662, 'GO:0000470': 2795, 'GO:0042256': 328, 'GO:0030117': 2347, 'GO:0006891': 1283, 'GO:0030126': 1222, 'GO:0030663': 2352, 'GO:0005200': 640, 'GO:0051258': 3139, 'GO:0000226': 23, 'GO:0031966': 2725, 'GO:0051047': 2276, 'GO:0010954': 1550, 'GO:0034620': 1785, 'GO:0044322': 1064, 'GO:1904211': 732, 'GO:0031293': 3274, 'GO:0034644': 3295, 'GO:0043687': 232, 'GO:0010555': 69, 'GO:0031977': 1078, 'GO:0016018': 2873, 'GO:0042026': 2084, 'GO:0009832': 1553, 'GO:0009898': 517, 'GO:0000815': 3105, 'GO:0010184': 3290, 'GO:0032922': 869, 'GO:0009958': 2042, 'GO:0090229': 2562, 'GO:0004729': 1404, 'GO:0006783': 2994, 'GO:0006782': 2312, 'GO:0006779': 2580, 'GO:0004568': 142, 'GO:0006032': 34, 'GO:0016998': 1824, 'GO:0080146': 3225, 'GO:0019448': 1908, 'GO:0070814': 171, 'GO:0010540': 2366, 'GO:0009630': 2135, 'GO:0008559': 1483, 'GO:0010315': 2482, 'GO:0009809': 2245, 'GO:0050801': 2690, 'GO:0006730': 167, 'GO:0015948': 2300, 'GO:0080110': 524, 'GO:0048654': 1636, 'GO:0008287': 843, 'GO:0061711': 601, 'GO:0070265': 1700, 'GO:0052386': 3282, 'GO:0045493': 3188, 'GO:0009044': 113, 'GO:0046556': 1899, 'GO:0031222': 1837, 'GO:0090447': 1651, 'GO:0008408': 3285, 'GO:0044262': 229, 'GO:0004775': 1277, 'GO:0003878': 1848, 'GO:0046912': 2737, 'GO:0006085': 1354, 'GO:0090058': 2431, 'GO:0080153': 2598, 'GO:0031359': 1287, 'GO:1990573': 2741, 'GO:0055064': 3093, 'GO:1902476': 725, 'GO:0055075': 1202, 'GO:0015379': 2338, 'GO:0006884': 2806, 'GO:0015377': 1126, 'GO:2000649': 3041, 'GO:0055078': 2178, 'GO:0031402': 1867, 'GO:0015079': 3331, 'GO:0010233': 1368, 'GO:0006821': 2705, 'GO:0052689': 993, 'GO:0004725': 1742, 'GO:0035335': 2627, 'GO:0008138': 1849, 'GO:0051923': 419, 'GO:0008146': 921, 'GO:0031405': 1443, 'GO:0016407': 2632, 'GO:0043754': 3213, 'GO:0046949': 2638, 'GO:0042732': 2182, 'GO:0048040': 1760, 'GO:0070403': 2144, 'GO:0033320': 1219, 'GO:0010416': 3294, 'GO:0008115': 1894, 'GO:0006744': 2370, 'GO:0017004': 295, 'GO:0052793': 2931, 'GO:0009646': 1485, 'GO:0015462': 1793, 'GO:0016464': 1622, 'GO:0006605': 1438, 'GO:0017038': 2118, 'GO:0071806': 2035, 'GO:0090110': 2197, 'GO:0016236': 760, 'GO:0010584': 1648, 'GO:0004427': 335, 'GO:0006796': 2013, 'GO:0071011': 609, 'GO:0046394': 2980, 'GO:0031386': 164, 'GO:0019941': 2602, 'GO:0016668': 198, 'GO:0045252': 2299, 'GO:0004148': 3269, 'GO:1990714': 2822, 'GO:0048354': 2336, 'GO:0010405': 1911, 'GO:0009059': 2699, 'GO:0018258': 2936, 'GO:0008378': 1177, 'GO:0080147': 2003, 'GO:0034645': 1411, 'GO:1901137': 982, 'GO:0061157': 2576, 'GO:1990247': 2700, 'GO:0033926': 643, 'GO:0005987': 3150, 'GO:0004575': 596, 'GO:0004564': 2092, 'GO:0010333': 1511, 'GO:0009678': 1060, 'GO:0030289': 41, 'GO:0009960': 850, 'GO:0005669': 2914, 'GO:0140492': 470, 'GO:0071108': 2566, 'GO:0009856': 2966, 'GO:0003756': 1726, 'GO:0010048': 3099, 'GO:0016207': 1582, 'GO:0106290': 1089, 'GO:0033549': 2484, 'GO:0008692': 855, 'GO:0044281': 2758, 'GO:0003857': 2452, 'GO:0080050': 1787, 'GO:0048450': 1646, 'GO:0051103': 766, 'GO:0006471': 3280, 'GO:0003950': 1289, 'GO:0006273': 165, 'GO:1990404': 3276, 'GO:0070212': 2529, 'GO:0003910': 2767, 'GO:0006807': 2028, 'GO:0006904': 1642, 'GO:0060321': 580, 'GO:0090522': 9, 'GO:0006887': 2769, 'GO:0000145': 915, 'GO:0006893': 365, 'GO:0070062': 2819, 'GO:0004364': 3199, 'GO:0042221': 1813, 'GO:0009636': 1186, 'GO:0043295': 3215, 'GO:0006896': 301, 'GO:0030123': 1547, 'GO:0050613': 934, 'GO:0072583': 482, 'GO:0031982': 978, 'GO:0072318': 1519, 'GO:0006898': 1218, 'GO:0046488': 2453, 'GO:0080030': 3207, 'GO:0080031': 2654, 'GO:0080032': 3077, 'GO:0009696': 3286, 'GO:0009229': 946, 'GO:0004788': 731, 'GO:0030975': 1398, 'GO:0006772': 3115, 'GO:0009527': 1943, 'GO:0019740': 2242, 'GO:0048307': 1441, 'GO:0032934': 201, 'GO:0004556': 505, 'GO:0004352': 781, 'GO:0006538': 1627, 'GO:0016174': 2413, 'GO:0009566': 2342, 'GO:0043020': 2262, 'GO:0050665': 684, 'GO:0050664': 912, 'GO:0010266': 236, 'GO:0015937': 708, 'GO:0004140': 2707, 'GO:1901576': 2228, 'GO:0004144': 3219, 'GO:0044249': 907, 'GO:0008374': 695, 'GO:0036172': 3112, 'GO:0004417': 2724, 'GO:0009228': 2173, 'GO:0048830': 2604, 'GO:0046825': 1254, 'GO:0006405': 1966, 'GO:0051168': 1159, 'GO:0042565': 2198, 'GO:0010015': 761, 'GO:0070816': 3167, 'GO:0008353': 1798, 'GO:0032968': 151, 'GO:0004693': 721, 'GO:0140658': 2904, 'GO:0016589': 3168, 'GO:0016584': 3149, 'GO:0016818': 2179, 'GO:0016585': 884, 'GO:0010256': 1863, 'GO:0046685': 2127, 'GO:0009615': 1752, 'GO:0010181': 3079, 'GO:0016629': 2613, 'GO:0050589': 2430, 'GO:0010023': 2054, 'GO:0009718': 1686, 'GO:0004816': 2011, 'GO:0006421': 1229, 'GO:0052716': 3061, 'GO:0008471': 3200, 'GO:0046274': 1259, 'GO:0045604': 500, 'GO:2000024': 602, 'GO:0045995': 2694, 'GO:0010077': 756, 'GO:0009423': 2311, 'GO:0009073': 2539, 'GO:0004764': 280, 'GO:0019632': 2926, 'GO:0003855': 1209, 'GO:0009607': 1838, 'GO:0044403': 2324, 'GO:0005546': 3033, 'GO:0034398': 16, 'GO:0000973': 1170, 'GO:0044614': 2265, 'GO:0010117': 1741, 'GO:0015112': 436, 'GO:0050982': 3090, 'GO:0071260': 1532, 'GO:0008381': 1484, 'GO:0005261': 1828, 'GO:0042391': 1893, 'GO:0006289': 1101, 'GO:0043047': 849, 'GO:0006268': 2332, 'GO:0005662': 1568, 'GO:0007004': 2174, 'GO:0002939': 3016, 'GO:0008175': 1029, 'GO:0006400': 2990, 'GO:0009019': 1366, 'GO:0052906': 3007, 'GO:0080156': 300, 'GO:1902065': 1593, 'GO:0034051': 824, 'GO:0010193': 1914, 'GO:0017017': 1118, 'GO:0008330': 1228, 'GO:0010225': 1429, 'GO:0043407': 2968, 'GO:0043409': 2328, 'GO:0005544': 1758, 'GO:0071249': 848, 'GO:0006437': 1335, 'GO:0004831': 1095, 'GO:0043953': 1617, 'GO:0065002': 1195, 'GO:0009567': 1204, 'GO:0009977': 2692, 'GO:0015450': 2410, 'GO:0043235': 29, 'GO:0033281': 1794, 'GO:0010242': 1317, 'GO:0009523': 593, 'GO:0009654': 752, 'GO:0016790': 2454, 'GO:0000036': 586, 'GO:0016297': 2065, 'GO:0048026': 1313, 'GO:0015086': 2594, 'GO:0070574': 2600, 'GO:0046873': 2796, 'GO:0034755': 1079, 'GO:0055072': 3129, 'GO:0071421': 1446, 'GO:0006826': 189, 'GO:0006863': 1709, 'GO:0042908': 2403, 'GO:0042910': 370, 'GO:0006855': 1712, 'GO:1990961': 3101, 'GO:0043269': 1924, 'GO:0051300': 700, 'GO:0048443': 2259, 'GO:0035195': 1514, 'GO:0006379': 19, 'GO:0016442': 3009, 'GO:0006368': 2008, 'GO:0032783': 2969, 'GO:0003711': 2438, 'GO:0008023': 2246, 'GO:0034243': 2113, 'GO:0009840': 526, 'GO:0080082': 1135, 'GO:0102483': 380, 'GO:0080083': 1321, 'GO:0008422': 739, 'GO:0004565': 1645, 'GO:0080079': 1422, 'GO:0033907': 2393, 'GO:0047668': 1007, 'GO:0080081': 988, 'GO:0031098': 2985, 'GO:0007112': 936, 'GO:0000165': 172, 'GO:0031435': 1910, 'GO:0032147': 1891, 'GO:0051019': 3031, 'GO:0004708': 1579, 'GO:0000187': 2754, 'GO:0010311': 983, 'GO:0033014': 1136, 'GO:0051123': 1724, 'GO:0046695': 2973, 'GO:0061629': 1788, 'GO:0051090': 1315, 'GO:0006367': 2650, 'GO:0006352': 293, 'GO:0000124': 1461, 'GO:0016251': 960, 'GO:0005761': 3300, 'GO:0009853': 2140, 'GO:0043879': 3019, 'GO:0010008': 1865, 'GO:0080171': 851, 'GO:0006903': 1208, 'GO:1990019': 789, 'GO:0007032': 2039, 'GO:0050821': 879, 'GO:0031647': 2614, 'GO:0006862': 1375, 'GO:0035352': 170, 'GO:0051724': 634, 'GO:0005085': 866, 'GO:0030008': 2, 'GO:0005801': 1707, 'GO:0000463': 264, 'GO:0005871': 2497, 'GO:0009558': 1256, 'GO:0032367': 3338, 'GO:0055037': 1884, 'GO:0032432': 1440, 'GO:0051639': 3163, 'GO:0010069': 431, 'GO:0033612': 319, 'GO:0009865': 607, 'GO:0019277': 1039, 'GO:0050511': 2409, 'GO:0046527': 547, 'GO:0030259': 85, 'GO:0022408': 118, 'GO:0052692': 173, 'GO:0004557': 860, 'GO:0044255': 2151, 'GO:0005763': 2122, 'GO:0009744': 2425, 'GO:0009743': 3223, 'GO:0044183': 313, 'GO:0000380': 1144, 'GO:0043484': 187, 'GO:0009887': 681, 'GO:0048440': 3278, 'GO:0009888': 2485, 'GO:0010492': 2993, 'GO:0010267': 2773, 'GO:0009616': 1026, 'GO:0048467': 1314, 'GO:0042138': 508, 'GO:0016765': 1239, 'GO:0045333': 2037, 'GO:0048034': 2116, 'GO:0008495': 514, 'GO:0004311': 927, 'GO:0006384': 2239, 'GO:0000127': 1444, 'GO:0070898': 3209, 'GO:0047769': 469, 'GO:0009094': 3190, 'GO:0009807': 2677, 'GO:0003899': 1585, 'GO:0005666': 2006, 'GO:0006360': 108, 'GO:0006383': 859, 'GO:0005665': 1833, 'GO:0005736': 570, 'GO:1902555': 334, 'GO:1902494': 1815, 'GO:0034470': 1572, 'GO:0010073': 2996, 'GO:0004435': 360, 'GO:0048507': 3328, 'GO:0008352': 1215, 'GO:0051013': 1901, 'GO:0007019': 2061, 'GO:0071004': 1721, 'GO:0005685': 2815, 'GO:0098662': 751, 'GO:0000796': 2732, 'GO:0007076': 1633, 'GO:0044547': 2392, 'GO:0072587': 489, 'GO:0000148': 321, 'GO:0003843': 627, 'GO:0006075': 2800, 'GO:0009556': 1309, 'GO:0005227': 1496, 'GO:0017119': 2950, 'GO:0036265': 2603, 'GO:0008176': 449, 'GO:0106004': 1704, 'GO:0043527': 2477, 'GO:0019852': 1388, 'GO:0006270': 3156, 'GO:0017116': 807, 'GO:1902975': 2303, 'GO:0000347': 2461, 'GO:0003678': 554, 'GO:1900864': 1108, 'GO:0019199': 3311, 'GO:0030570': 699, 'GO:0045905': 2329, 'GO:0043985': 935, 'GO:0070079': 342, 'GO:0033749': 2224, 'GO:0033746': 2513, 'GO:0070078': 1328, 'GO:0080134': 891, 'GO:0015706': 3208, 'GO:0052546': 1377, 'GO:0009826': 115, 'GO:0060773': 2531, 'GO:0007154': 2916, 'GO:0061136': 955, 'GO:0016075': 662, 'GO:0090501': 197, 'GO:0004525': 2853, 'GO:0007035': 519, 'GO:0043291': 1930, 'GO:0080005': 1584, 'GO:0036361': 246, 'GO:0047661': 2317, 'GO:0043686': 1734, 'GO:0031365': 394, 'GO:0042586': 1339, 'GO:0018206': 1143, 'GO:0071218': 726, 'GO:0030544': 1886, 'GO:0015691': 876, 'GO:0055073': 1578, 'GO:0006537': 1006, 'GO:0016639': 399, 'GO:0004354': 1412, 'GO:0005844': 918, 'GO:0004828': 1299, 'GO:0006434': 3312, 'GO:0006408': 1912, 'GO:0042743': 2886, 'GO:0000323': 417, 'GO:0006624': 2440, 'GO:0010179': 735, 'GO:0009850': 3001, 'GO:0005788': 2089, 'GO:0031902': 2208, 'GO:0042147': 1998, 'GO:0016102': 1326, 'GO:0005528': 1607, 'GO:0031204': 3024, 'GO:0031205': 3140, 'GO:0006616': 2472, 'GO:0005784': 141}\n"
          ]
        }
      ],
      "source": [
        "mapped_iric = pd.read_csv(mapped_iric_path, sep = '\\t')\n",
        "display(mapped_iric)\n",
        "\n",
        "GO_to_map = mapped_iric.set_index('object')['mapped_object'].to_dict()\n",
        "map_to_GO = {value: key for key, value in GO_to_map.items()}\n",
        "\n",
        "\n",
        "# Checking dict :\n",
        "looks_ok: bool = True\n",
        "print(len(list(mapped_iric['object'])))\n",
        "for i in range(len(list(mapped_iric['object']))):\n",
        "    if GO_to_map[mapped_iric['object'][i]]!=mapped_iric['mapped_object'][i]:\n",
        "        looks_ok = False\n",
        "    \n",
        "print('Dict looks ok :', looks_ok)\n",
        "print(map_to_GO)\n",
        "print(GO_to_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10001/10001 [00:04<00:00, 2026.39it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>predicate</th>\n",
              "      <th>object</th>\n",
              "      <th>mapped_subject</th>\n",
              "      <th>mapped_predicate</th>\n",
              "      <th>mapped_object</th>\n",
              "      <th>mapped_alt_tails</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0031267</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2505</td>\n",
              "      <td>[2022, 2237, 1720, 76, 515, 1419]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0006886</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2022</td>\n",
              "      <td>[2505, 2237, 1720, 76, 515, 1419]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005622</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>2237</td>\n",
              "      <td>[2505, 2022, 1720, 76, 515, 1419]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005623</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>1720</td>\n",
              "      <td>[2505, 2022, 2237, 76, 515, 1419]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0090630</td>\n",
              "      <td>2270</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "      <td>[2505, 2022, 2237, 1720, 515, 1419]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>OsNippo01g223000</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005784</td>\n",
              "      <td>1980</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "      <td>[3024, 3140, 181, 2022, 3092, 2472, 100, 2418,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005634</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>2657</td>\n",
              "      <td>[160, 2811, 362]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005737</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "      <td>[2657, 2811, 362]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0003676</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>2811</td>\n",
              "      <td>[2657, 160, 362]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>OsNippo01g223050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0090305</td>\n",
              "      <td>1922</td>\n",
              "      <td>0</td>\n",
              "      <td>362</td>\n",
              "      <td>[2657, 160, 2811]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10001 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                subject      predicate      object  mapped_subject  \\\n",
              "0      OsNippo01g010050  gene ontology  GO:0031267            2270   \n",
              "1      OsNippo01g010050  gene ontology  GO:0006886            2270   \n",
              "2      OsNippo01g010050  gene ontology  GO:0005622            2270   \n",
              "3      OsNippo01g010050  gene ontology  GO:0005623            2270   \n",
              "4      OsNippo01g010050  gene ontology  GO:0090630            2270   \n",
              "...                 ...            ...         ...             ...   \n",
              "9996   OsNippo01g223000  gene ontology  GO:0005784            1980   \n",
              "9997   OsNippo01g223050  gene ontology  GO:0005634            1922   \n",
              "9998   OsNippo01g223050  gene ontology  GO:0005737            1922   \n",
              "9999   OsNippo01g223050  gene ontology  GO:0003676            1922   \n",
              "10000  OsNippo01g223050  gene ontology  GO:0090305            1922   \n",
              "\n",
              "       mapped_predicate  mapped_object  \\\n",
              "0                     0           2505   \n",
              "1                     0           2022   \n",
              "2                     0           2237   \n",
              "3                     0           1720   \n",
              "4                     0             76   \n",
              "...                 ...            ...   \n",
              "9996                  0            141   \n",
              "9997                  0           2657   \n",
              "9998                  0            160   \n",
              "9999                  0           2811   \n",
              "10000                 0            362   \n",
              "\n",
              "                                        mapped_alt_tails  \n",
              "0                      [2022, 2237, 1720, 76, 515, 1419]  \n",
              "1                      [2505, 2237, 1720, 76, 515, 1419]  \n",
              "2                      [2505, 2022, 1720, 76, 515, 1419]  \n",
              "3                      [2505, 2022, 2237, 76, 515, 1419]  \n",
              "4                    [2505, 2022, 2237, 1720, 515, 1419]  \n",
              "...                                                  ...  \n",
              "9996   [3024, 3140, 181, 2022, 3092, 2472, 100, 2418,...  \n",
              "9997                                    [160, 2811, 362]  \n",
              "9998                                   [2657, 2811, 362]  \n",
              "9999                                    [2657, 160, 362]  \n",
              "10000                                  [2657, 160, 2811]  \n",
              "\n",
              "[10001 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "mapped_iric['mapped_alt_tails'] = mapped_iric.progress_apply(\n",
        "                                             lambda row: \n",
        "                                             mapped_iric.loc[(mapped_iric['mapped_subject'] == row['mapped_subject']) & \n",
        "                                                             (mapped_iric['mapped_predicate'] == row['mapped_predicate']) & \n",
        "                                                             (mapped_iric['mapped_object'] != row['mapped_object']), \n",
        "                                                             'mapped_object'].values, \n",
        "                                             axis=1)\n",
        "display(mapped_iric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(2270, 0): {515, 2022, 2505, 1419, 76, 1720, 2237}, (439, 0): {1094, 2918, 1161, 2316, 3128, 2524}, (1667, 0): {132, 2918, 1736, 1806, 2287, 2415, 2490}, (2502, 0): set(), (644, 0): set(), (3258, 0): set(), (3271, 0): {832, 1730, 2190, 2511, 340, 212, 1178, 2458, 1821, 863, 1120, 997, 1710, 2678, 2237}, (3266, 0): {1542, 903, 653, 13, 3092, 932, 2734, 181, 2364, 2876, 3144, 2126, 2511, 2768, 1261, 1520, 496, 2418, 368, 1913, 2554, 1022}, (1223, 0): {160, 2657, 2678, 2938}, (1635, 0): {2772, 2678, 343, 2745, 314, 2811}, (177, 0): {2657, 3138, 1835, 2192, 1140, 181, 1751, 1564, 2047}, (308, 0): set(), (1488, 0): {3138, 2498, 196, 1158, 3165, 1821, 3230, 286, 2549, 2876}, (310, 0): {2668, 1744, 1047, 2455, 3102}, (2704, 0): {3265, 2253, 1493, 2838, 347, 160, 2657, 2728, 1263, 1394, 2237}, (814, 0): {3138, 550, 1127, 2823, 1744, 1588}, (2791, 0): {3092, 181}, (753, 0): {3138, 1235, 3092, 343, 2520, 798, 3102, 418, 1960, 235, 1588, 247, 2168}, (2416, 0): {1826, 100, 1157, 1416, 3120, 538}, (1347, 0): {1, 707, 3092, 1748, 2337, 2273, 2402, 490, 2415, 127}, (518, 0): {1478, 3144, 1805, 785, 1875, 3092, 3222, 605, 3173, 871, 2415, 2418, 181, 1397}, (1552, 0): {2657, 1673, 2607, 1599}, (3211, 0): {2496, 2309, 2055, 2823, 1033, 3342, 3092, 2991, 2415, 2301}, (2081, 0): {160, 32, 1191, 1610, 3342, 2745, 2772, 343, 2553}, (659, 0): {2657, 1394, 2728}, (2229, 0): {2657, 997, 13, 2511, 307, 2458, 254}, (450, 0): {724, 343, 2649, 2657, 611, 2728, 2414, 1394, 181, 510}, (1825, 0): {2657, 1810, 1716, 2553, 26, 1051, 255}, (2920, 0): {2657, 2793, 2064, 1168, 2005, 2295}, (2670, 0): {1606, 1110}, (21, 0): {2784, 357, 2823, 1512, 2268}, (1638, 0): {2657, 1797, 1927, 1352, 2609, 1493, 62}, (1661, 0): {160, 833, 2695, 1192, 1417, 846, 668}, (456, 0): {3083, 740}, (3109, 0): {160, 1090, 2248, 1515, 1807, 1010, 3092, 181, 223}, (1698, 0): {314, 2287}, (664, 0): {997, 2630, 13, 2511, 1041, 307, 340}, (458, 0): {1537, 3206, 2823, 3083, 26, 3098, 1055, 549, 2991, 314, 1616, 465, 343, 345, 2521, 1500, 2526, 2657, 740, 2419, 630, 2682, 893}, (2712, 0): {1810, 32, 2339, 1191, 2479, 1843, 188, 1474, 2114, 1220, 1610, 459, 2772, 343, 864, 2657, 243, 1525, 1656, 2553}, (180, 0): {3138, 972, 847, 786, 343, 2519, 1499, 798, 608, 418, 2918, 2287, 1588, 2679, 2745, 314, 1595, 638}, (1109, 0): {2466, 100, 1929, 2636, 3092, 343, 2810, 2780}, (367, 0): {3043, 3092, 374, 525}, (330, 0): {352, 418, 3138, 2437, 398}, (2591, 0): set(), (2209, 0): {1408, 3138, 1543, 3030, 1238, 984, 3289, 2906, 3036, 160, 2542, 1714}, (2489, 0): {1860, 1797, 2326, 2657, 1253, 2728, 2609, 2422, 56, 62}, (2024, 0): {1860, 1253, 1797, 2728, 2609, 2422, 2326, 56, 62}, (1950, 0): {3138, 2823, 71, 1737, 1744, 2448, 471, 1757, 2657, 2745, 2876}, (275, 0): {2657, 804, 878, 1394, 2775, 472, 2397}, (1258, 0): {2657, 1394, 3092}, (3073, 0): {3092, 213, 1684, 1946, 160, 162, 3043, 2220, 2415, 887, 1338, 1148, 2109}, (581, 0): set(), (2667, 0): {3206, 1606, 846, 220, 2780, 233, 2350, 1913, 2618, 2876}, (1719, 0): {3206, 2823, 2055, 1033, 3083, 3092, 2521, 740, 1897, 2991, 2415, 314, 2301}, (1666, 0): {160, 844, 92, 2768, 2618, 539, 2876, 2906}, (1030, 0): set(), (2828, 0): {184, 1273, 2972}, (270, 0): {3138, 2192, 1397, 2166, 283}, (2896, 0): {1731, 81, 3155, 1178, 2906, 2843, 990, 487, 1961, 2678}, (3254, 0): {2657, 2728, 2932, 2745, 2811, 62}, (680, 0): {2657, 1797, 1927, 2609, 147, 1396, 1493, 62}, (2879, 0): {2657, 997, 294, 520, 1691, 125, 3038}, (250, 0): set(), (599, 0): set(), (979, 0): {2657, 2906}, (2588, 0): {801, 587, 1748, 3092, 127}, (318, 0): {160, 2657, 343, 3342}, (1898, 0): {3272, 2107, 540}, (994, 0): {2657, 1225, 208, 1714, 1238, 2751}, (1948, 0): {3272, 2107, 540}, (2314, 0): {774, 2823, 2055, 2759, 2511, 3092, 2521, 2458, 2991, 314, 2301}, (2818, 0): {3206, 2823, 2759, 2055, 3083, 2511, 3092, 2521, 2458, 1691, 1821, 740, 292, 2991, 314, 2301}, (1004, 0): {3206, 2823, 2055, 3083, 3092, 2521, 1051, 740, 2991, 2415, 314, 2301}, (2548, 0): {2309, 3206, 2759, 2055, 2823, 3083, 2521, 740, 2991, 314, 2301}, (397, 0): {740, 3206, 2055, 2823, 3083, 2991, 2521, 314, 2301}, (410, 0): {3206, 2823, 2055, 3083, 3092, 2521, 1051, 740, 2991, 2415, 314, 2301}, (2789, 0): {2309, 3206, 2823, 2759, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (1341, 0): {3206, 2055, 2823, 2759, 3083, 3092, 2521, 740, 2991, 314, 2301}, (2954, 0): {740, 2759}, (1881, 0): {3206, 2055, 2759, 2823, 3083, 2511, 2521, 2458, 1691, 1821, 292, 740, 2991, 314, 2301}, (998, 0): {292, 2823, 2521, 2458, 1691}, (2628, 0): {2309, 2055, 2823, 2991, 2521, 2301}, (1942, 0): {2309, 3206, 2055, 2759, 2823, 3083, 3092, 2521, 740, 2991, 314, 2301}, (2540, 0): {2309, 3206, 2055, 2759, 2823, 3083, 2521, 740, 2991, 314, 2301}, (3059, 0): {2055, 2823, 2991, 2521, 2301}, (1975, 0): {2876, 1991}, (1132, 0): {2436, 3206, 2759, 2823, 2055, 1226, 3083, 2511, 2521, 740, 2991, 314, 2301}, (2025, 0): {2309, 3206, 2759, 2055, 2823, 3083, 3092, 2521, 740, 2991, 314, 2301}, (2014, 0): {2309, 3206, 2823, 2759, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (2106, 0): {2823, 2521, 2759}, (1655, 0): {3092, 2759}, (2313, 0): {2309, 3206, 2759, 2055, 2823, 3083, 2521, 740, 2991, 314, 2301}, (222, 0): {3206, 2055, 2759, 2823, 3083, 2521, 740, 2991, 314, 2301}, (1773, 0): {3206, 2055, 2759, 2823, 3083, 3092, 2521, 740, 2991, 314, 2301}, (2145, 0): {2309, 3206, 2055, 2823, 3083, 2521, 740, 2991, 314, 2301}, (1382, 0): {2563, 3080, 2824, 144, 2850, 3123, 181, 2745, 314, 2237, 575, 2248, 1226, 2511, 976, 1744, 2772, 863, 1128, 746, 2036}, (1652, 0): {2309, 3206, 2823, 2759, 2055, 1226, 3083, 2521, 740, 2991, 314, 2301}, (2164, 0): {2309, 3206, 2823, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (1322, 0): {3206, 2759, 2055, 2823, 1226, 3083, 2511, 2521, 740, 2991, 314, 2301}, (2094, 0): {2823, 2055, 2991, 2521, 2301}, (80, 0): {2309, 3206, 2823, 2055, 3083, 2521, 740, 2991, 314, 2301}, (3020, 0): {2309, 3206, 2823, 2759, 2055, 3083, 3092, 2521, 1821, 740, 2991, 314, 2301}, (1437, 0): {2309, 3206, 2823, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (943, 0): {2337, 587, 2415, 1748, 3092}, (3160, 0): {2379, 846, 2768, 1682, 2906, 608, 1637, 48, 1464, 2618, 379, 2876}, (2812, 0): {2823, 277, 218, 2976, 2657, 1071, 2678, 314, 2811, 2237}, (1378, 0): {3206, 3144, 1993, 3279, 3092, 532, 1626, 922, 2657, 1767, 2345, 944, 2418, 626, 628, 1715, 181, 703}, (262, 0): {1856, 3206, 778, 525, 3092, 3043, 743, 560, 2618, 3005}, (3018, 0): {3043, 3206, 778, 525, 560, 3092, 2618, 3005}, (47, 0): {3043, 3206, 778, 525, 560, 3092, 2618, 3005}, (1104, 0): {3043, 3206, 778, 525, 560, 3092, 2618, 3005}, (2545, 0): {3043, 3206, 778, 525, 560, 2618, 3005}, (1449, 0): {0, 1665, 3138, 2823, 840, 269, 846, 18, 2906, 925, 863, 160, 2160, 2676, 1588, 3062, 314, 2943}, (2684, 0): {390, 2823, 263, 2377, 846, 160, 872, 2676, 3062, 2487, 2297, 314, 2811}, (339, 0): {2918, 2287}, (2552, 0): {3206, 904, 3273, 3277, 2639, 656, 2906, 795, 160, 2657, 870}, (2746, 0): {3092, 2772, 2553, 2925}, (372, 0): set(), (1823, 0): set(), (961, 0): {2401, 3044, 1766, 2823, 2415, 3092, 405, 1748, 314}, (1702, 0): {3206, 2088, 525, 3092, 919}, (1992, 0): {160, 2657, 1191, 1433, 3092, 1525, 2553}, (1181, 0): {160, 2657, 2925, 2553, 2772, 1525, 343, 1433}, (3218, 0): {2657, 1525, 1433}, (509, 0): {2657, 1525, 1433}, (2264, 0): {3206, 1610, 2772, 343, 1433, 32, 2657, 160, 1191, 2925, 2745, 1525, 2553}, (2680, 0): {3206, 2772, 343, 1433, 160, 2657, 2925, 2745, 1525, 2553}, (2048, 0): {2147, 3237, 1669, 2284, 1333, 3030}, (2663, 0): {2147, 1669, 3237, 2284, 1333, 379, 540, 1821}, (186, 0): {2147, 1669, 3237, 2284, 1333}, (2833, 0): {2147, 1669, 3237, 2284, 205, 2511, 1333, 3030, 887}, (2016, 0): {1605, 1669, 13, 1037, 3030, 2147, 997, 3237, 2284, 307, 1333, 120}, (3216, 0): {2147, 3237, 1669, 2284, 1333}, (2538, 0): {2147, 1669, 3237, 2284, 1333}, (388, 0): {160, 2657, 1191, 2553, 2925, 2772, 1525, 343, 1433}, (1671, 0): {3206, 2772, 343, 1433, 160, 2657, 1191, 2925, 2745, 1525, 2553, 2237}, (2318, 0): {3206, 2772, 343, 1433, 160, 2657, 1191, 2925, 2745, 1525, 2553}, (3306, 0): {608, 2657, 641, 1164, 1586, 3030, 2745}, (2556, 0): {2657, 3, 2573, 3281, 2327, 185, 343}, (1956, 0): {1220, 997, 1517, 337, 3089, 2553, 2906, 1691}, (902, 0): {2918, 1806, 2287, 2000, 2418, 3092, 181, 2745, 2490}, (2715, 0): {2918, 1806, 2287, 2000, 2418, 3092, 181, 2745, 2490}, (1796, 0): {929, 2900}, (1662, 0): {2918, 1806, 2287, 2000, 2418, 213, 2490}, (2435, 0): {1091, 1158, 1806, 2063, 3092, 213, 604, 2918, 2287, 2490, 2418, 181, 2745, 1338, 443}, (1634, 0): {3138, 550, 1192, 1588, 925}, (957, 0): {228, 2287, 3096, 1945, 314, 2876}, (667, 0): {2147, 3237, 1669, 2284, 1333}, (1693, 0): {2823, 1226, 2511, 2301, 1821}, (952, 0): {2596, 845, 2772, 1397, 374, 2745, 2906, 1851, 1374}, (2117, 0): {2657, 2728, 2253, 1394, 343, 56}, (2912, 0): {1600, 3201, 128, 1281, 973, 1238, 2907, 2587, 286, 995, 1974, 2745, 2811}, (2222, 0): {1600, 1281, 973, 1238, 2907, 2587, 286, 995, 362, 2745, 2811}, (3132, 0): {3138, 2691, 2953, 2443, 3092, 418, 103, 1960, 629, 3130}, (2155, 0): {2657, 3043, 1736, 2415, 2418, 3092, 181, 3135}, (2764, 0): {2481, 3092, 181, 1167}, (1689, 0): {900, 1489, 2772, 1433, 2657, 3172, 2854, 1196, 56, 2745, 1723, 2237, 2811}, (2599, 0): {1730, 212, 3092, 863}, (1548, 0): set(), (1565, 0): {1345, 579, 3092, 1748, 2459, 1691, 3135, 2207, 1504, 608, 3180, 494, 2415, 3004, 2620, 2623}, (2788, 0): {2657, 161, 1226, 343, 540}, (1372, 0): {1409, 1807, 149, 1238, 1374, 94, 673, 2618, 2427, 2876}, (1988, 0): {13, 1747, 3092, 1821, 2657, 2728, 1394, 307, 56, 2557}, (1457, 0): {3015, 2248, 1543, 3092, 1238, 1307, 605, 1889, 1447, 1714, 2418, 181}, (2057, 0): {2147, 3237, 1669, 1543, 2284, 1333, 1821}, (648, 0): {2147, 3237, 1669, 1543, 2284, 1714, 1333}, (332, 0): {3092, 343}, (1687, 0): set(), (1770, 0): {2657, 1797, 2869, 174}, (1456, 0): {3265, 137, 2059, 2253, 2772, 2657, 1394, 2483, 56, 2811, 2237, 3327}, (1027, 0): {418, 3138, 1414, 1588, 3098, 987, 1279}, (1967, 0): set(), (2581, 0): {587, 1748, 3092, 2398, 2337, 1960, 938, 3243, 2415, 3323, 1468, 127}, (1311, 0): {418, 1238}, (616, 0): {3092, 3206, 2088, 525}, (1426, 0): {3092, 3206, 2088, 525}, (1473, 0): set(), (1984, 0): {642, 1549, 2254, 2384, 785, 2768, 3092, 730, 544, 2918, 1510, 2924, 2287, 2418, 1778, 181, 3128}, (1434, 0): {3202, 1226, 2253, 161, 2657, 2851, 2086, 2029, 1394, 2930, 2549, 56, 254}, (1692, 0): {1409, 1934, 2511, 1238, 1885, 1374, 673, 2618, 2427, 2876}, (3029, 0): {1409, 1934, 2511, 1238, 1885, 605, 1374, 673, 2427, 2876}, (1044, 0): {1778, 2745, 2522, 267}, (933, 0): {736, 160, 2856, 2192, 1041, 1140, 340, 2458, 2399}, (158, 0): {192, 2192, 1041, 340, 2458, 2399, 544, 736, 160, 2856, 1140}, (3008, 0): set(), (2719, 0): {736, 160, 2856, 2192, 1041, 1140, 340, 2458, 2399}, (2215, 0): {2306, 2192, 1041, 340, 2458, 2399, 160, 736, 1442, 2856, 1140}, (1439, 0): {2306, 2192, 1041, 340, 2458, 2399, 736, 160, 1442, 2856, 1140}, (691, 0): {3092, 2759}, (366, 0): {3206, 2823, 2759, 2055, 3083, 3092, 2521, 740, 2415, 2991, 314, 2301}, (2807, 0): {2309, 3206, 2055, 2759, 2823, 3083, 3092, 2521, 740, 2991, 314, 2301}, (1680, 0): {2309, 3206, 2823, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (1878, 0): set(), (3232, 0): {2055, 2823, 2511, 2991, 2549, 2521, 2301}, (529, 0): {3206, 2055, 2823, 2759, 3083, 3092, 2521, 740, 2991, 314, 2301}, (1018, 0): {2309, 3206, 2823, 2759, 2055, 3083, 3092, 2521, 740, 2991, 314, 2301}, (567, 0): set(), (1197, 0): {1238, 823, 238}, (1151, 0): {1093, 1193, 1397, 2745, 2618, 2876}, (2405, 0): {2418, 3092, 181, 3032, 605}, (2333, 0): set(), (61, 0): {1888, 2275, 1002, 3092, 2876, 703}, (906, 0): {1714, 2643, 423}, (377, 0): {162, 3043, 740, 1157, 2415, 3092, 343, 3004}, (2176, 0): {2248, 1226, 203, 13, 340, 212, 278, 1821, 863, 736, 1583, 307, 2549, 2678, 1407}, (2078, 0): {2657, 2728, 46, 1394, 56, 1435, 2557}, (2421, 0): {3081, 13, 1746, 533, 2458, 1435, 3102, 1567, 2657, 2728, 1394, 1077, 56, 1658, 2557}, (2915, 0): {2401, 2657, 2823, 3217, 405, 314}, (980, 0): {2401, 2657, 1603, 2823, 3217, 3092, 405, 314}, (892, 0): set(), (303, 0): {3202, 1814, 1960, 846}, (395, 0): {3072, 1817}, (1546, 0): {418, 3138, 1543, 1714, 3092, 661, 2260, 629, 1238}, (1106, 0): {2657, 528, 1811, 376, 2906}, (28, 0): {324, 2442, 1931, 525, 2063, 213, 160, 1960, 3050, 2605, 3119, 2748}, (2626, 0): {2657, 2728, 1649, 1394, 343}, (3114, 0): {1, 707, 587, 1748, 3092, 2273, 2337, 490, 2415, 127}, (3250, 0): {1609, 587, 1421, 3342, 1748, 3092, 2132, 3040, 2337, 163, 3176}, (440, 0): {1092, 2153, 846, 591, 595, 1178}, (1210, 0): {1092, 2153, 846, 591, 595, 1178}, (227, 0): {1092, 2153, 846, 591, 595, 1178}, (671, 0): {3092, 3273, 1790}, (827, 0): {2379, 2668}, (2012, 0): {3203, 1316, 1542, 1643, 2937, 2043, 3004}, (2631, 0): set(), (3244, 0): {2657, 391}, (2141, 0): {2848, 1575, 1939, 1525, 3287}, (119, 0): {1476, 2621}, (381, 0): set(), (1816, 0): set(), (2272, 0): set(), (896, 0): set(), (2559, 0): set(), (985, 0): {3206, 2826, 778, 525, 1297, 1234, 3092, 1374, 3043, 2983, 1840}, (2386, 0): {2657, 475, 1356}, (3246, 0): {1543, 1714, 3092, 1238, 3289}, (1644, 0): {1168, 929, 2900, 1807}, (649, 0): {2876, 1034, 2254, 1682, 2584, 2906, 160, 2476, 1397, 1020, 2558}, (3255, 0): set(), (3270, 0): {1710, 2064, 2195, 2005, 2678}, (2241, 0): set(), (3118, 0): set(), (3330, 0): {1221, 2823, 3272, 343, 540}, (663, 0): {2657, 452, 2728, 1263, 2961}, (1448, 0): {1221, 2823, 3272, 540}, (1296, 0): {2657, 2243, 452, 2728, 1263, 2961, 3060, 2838}, (2250, 0): set(), (1200, 0): {544, 2745, 1806}, (1265, 0): {3074, 3144, 2254, 974, 209, 785, 3092, 2458, 160, 551, 2287, 2415, 948, 2100, 2356, 2999, 573}, (3010, 0): {1385, 1997, 2928, 2745, 1180}, (1475, 0): {2177, 3144, 1482, 206, 2254, 2768, 3092, 917, 1433, 1629, 2528, 551, 1129, 2223, 2418, 181, 3320}, (1083, 0): {2177, 3144, 1482, 206, 2254, 2768, 1807, 3092, 917, 1629, 2528, 551, 1129, 2223, 2418, 181, 3320}, (2742, 0): {2177, 3144, 1482, 206, 2254, 1807, 2768, 3092, 917, 1629, 2528, 551, 1129, 2223, 2418, 181, 3320}, (2681, 0): {2177, 3144, 1482, 206, 2254, 1807, 2768, 3092, 917, 1629, 2528, 551, 1129, 2223, 2418, 181, 3320}, (2019, 0): {2017, 1889, 550, 235, 1902, 1427, 3092, 2101, 605}, (2125, 0): {1536, 1803, 1745, 1683, 863, 3111, 2346, 942, 2863, 2415, 1714}, (2701, 0): {1536, 1803, 1745, 1683, 3092, 863, 3111, 2346, 942, 2415, 2350, 2863, 1714}, (2578, 0): {2618, 314, 2287}, (126, 0): {1174, 343, 846}, (2656, 0): {3206, 2823, 2055, 3083, 3092, 343, 2521, 740, 2991, 314, 2301}, (3141, 0): set(), (1100, 0): set(), (678, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (2119, 0): {1394, 2772, 2811}, (2289, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (1116, 0): {3206, 2823, 2055, 3083, 2511, 3092, 343, 2521, 2458, 1821, 740, 2991, 503, 314, 2301}, (77, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (776, 0): set(), (1081, 0): {3206, 2823, 2055, 3083, 3092, 343, 2521, 740, 2991, 314, 2301}, (1762, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (3318, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (2417, 0): {1665, 3138, 2823, 846, 3092, 350, 863, 160, 2160, 2676, 314, 1019}, (2837, 0): {2618, 1238}, (1380, 0): set(), (2963, 0): set(), (252, 0): {608, 2827, 2736, 1238, 1023}, (2361, 0): {1537, 1221, 3206, 2823, 2055, 3083, 3092, 2521, 94, 2658, 740, 2991, 2415, 314, 2301}, (1696, 0): {1669, 1498, 2235}, (2216, 0): {2309, 3206, 2823, 1736, 2055, 3083, 3092, 2521, 157, 3102, 740, 2415, 2991, 314, 2301, 1727}, (3143, 0): {3264, 2063, 2772, 1178, 476, 2657, 355, 1777, 2678, 314, 2811}, (768, 0): {2657, 1394, 343, 2728}, (109, 0): {3206, 778, 2826, 525, 1297, 1234, 3092, 3043, 2983, 1840}, (140, 0): {2053, 181, 1751, 826, 1470}, (867, 0): {2180, 1799, 3342, 2579, 1556, 343, 2906, 160, 2657, 100, 937, 3179, 1453, 371}, (1563, 0): {1744, 1972, 2678, 343}, (1360, 0): {1543, 1255, 1714, 2546, 1238, 2263, 3004}, (1535, 0): set(), (705, 0): {1665, 2823, 863, 160, 2160, 2932, 3317, 1142, 1271, 2676, 314}, (1808, 0): {3092, 343, 2415}, (2142, 0): {2657, 3265, 875, 784, 1394, 2869, 2745}, (713, 0): {1797, 1479, 1610, 2251, 2772, 3092, 343, 2458, 32, 2657, 2609, 1394, 2422, 56, 2745}, (323, 0): {292, 2253, 2557}, (2516, 0): {1234, 2415}, (2730, 0): {32, 1191, 1610, 2553, 2772, 343, 2745}, (3307, 0): {2018, 2918, 2287, 3185, 948, 1913, 2618, 2780}, (2185, 0): set(), (1216, 0): {2691, 997, 2906}, (3011, 0): {641, 2691, 2827, 2458, 608, 2657, 1394, 1778, 56, 125, 62, 1023}, (64, 0): {1920, 2635, 2126, 2768, 1554, 3191, 2745, 2906, 2876}, (253, 0): {3092, 343, 2415}, (2382, 0): {3092, 343, 2415}, (2292, 0): {1107, 1238, 2458, 2906, 540, 1373, 990, 120, 160, 2657, 3304, 3248, 307, 1976, 3002, 2622}, (10, 0): {3092, 343, 2415}, (2480, 0): {3092, 343, 2415}, (2651, 0): set(), (195, 0): {2415, 343}, (1620, 0): {3092, 2549, 343, 2415}, (1469, 0): {160, 2657, 2691, 997, 493, 340, 920, 2458, 1691}, (1121, 0): {3092, 343, 2415}, (1804, 0): {3092, 343, 2415}, (1932, 0): set(), (477, 0): set(), (658, 0): {3138, 2678, 314, 2811}, (858, 0): {2976, 2823, 314, 2005, 1238, 405, 2678, 218, 2811}, (96, 0): {3138, 2574, 657, 1238, 1879, 1442, 3122, 819, 1588, 2745, 2876}, (2062, 0): {1463, 343, 2906, 2831}, (504, 0): {3265, 2253, 2458, 1821, 544, 2657, 997, 2728, 621, 1394, 2549, 56, 2557, 510}, (2294, 0): {160, 2657, 2659, 997, 493, 1293, 343, 920}, (460, 0): {1971, 343}, (1906, 0): {1281, 3092, 1238, 2587, 286, 160, 2657, 362, 2415, 2745, 2811}, (341, 0): {423, 3144, 2218, 496, 1714, 3231}, (810, 0): {1540, 1871, 2063, 2397, 2657, 1960, 1194, 1517, 1649, 631}, (1601, 0): {3304, 1107, 1238, 1976, 540, 990}, (1870, 0): {1153, 3154, 723, 1748, 3092, 2974, 670, 2337, 3043, 2805}, (1005, 0): set(), (1660, 0): {2420, 620}, (2376, 0): {3342, 343}, (1146, 0): {882, 740, 2573, 3135}, (2965, 0): {882, 740, 2573, 3135}, (2052, 0): {160, 1820, 587, 1293, 2447, 688, 2524}, (2105, 0): {2248, 3092}, (1058, 0): {32, 1191, 553, 1610, 2553, 2772, 3092, 343, 2745}, (463, 0): {384, 3203, 454, 2063, 343, 1688, 2906, 1889, 3170, 1831, 2731, 2860, 2673, 951}, (3121, 0): {160, 3206, 525, 3092, 2046}, (2492, 0): set(), (3049, 0): set(), (885, 0): {160, 582, 6, 3342, 2255, 1782, 1691, 925}, (17, 0): {1536, 2786, 2948, 3308, 2554, 2876, 415}, (852, 0): {3138, 997, 1097, 2547, 379, 1821, 1374}, (3220, 0): {2657, 2678, 2811, 476, 2237}, (3247, 0): {3206, 2055, 2823, 3083, 3092, 343, 2521, 740, 2415, 2991, 314, 2301}, (2315, 0): {544, 45}, (1395, 0): {343, 846, 3342}, (2158, 0): {2309, 3206, 2823, 2055, 1033, 3083, 3092, 343, 2521, 740, 2991, 2415, 314, 2301}, (1236, 0): {343, 1353, 3113, 778}, (1592, 0): set(), (2898, 0): set(), (744, 0): {2528, 1957, 2095, 3092, 181, 2804}, (2310, 0): {258, 3138, 970, 587, 531, 1748, 3092, 3161, 2974, 2337, 418, 3043, 101, 3243, 1068, 749, 629, 127}, (625, 0): {2823, 2064, 1238, 343, 218, 2976, 288, 2657, 2678, 314, 2811}, (2072, 0): {2498, 3144, 1465, 267, 3092, 1238, 343, 1624, 2908, 800, 2536, 2415, 816, 2549, 2745, 2618, 2876, 2941}, (2150, 0): {2439, 1736, 1161, 2130, 163, 2918, 3176, 2745, 1850, 379}, (2462, 0): set(), (1367, 0): {3268, 1738, 2219, 1238, 2618, 2427, 1374}, (1869, 0): {688, 48}, (2031, 0): {2657, 2678, 2811}, (763, 0): {641, 2823, 329, 587, 2508, 2827, 3092, 1748, 405, 1023, 608, 2401, 3044, 1766, 314, 3135}, (2468, 0): {2253, 857, 665, 347, 3102, 2657, 2728, 3184, 2609, 1394, 56, 1658, 2687}, (2568, 0): {2248, 846, 1806, 2009, 2458, 284, 160, 1889, 3043, 2918, 2287, 1850}, (874, 0): {2316, 2765, 3293, 2918, 2287, 2418, 181, 3128, 2172, 637, 2302}, (1832, 0): {1178, 2728, 1928, 1394, 2772, 2836, 2745, 2906, 2811}, (2381, 0): {1537, 3206, 2055, 2823, 3083, 2766, 1433, 2521, 1882, 160, 2657, 740, 2991, 2415, 314, 2301}, (478, 0): {1417, 1293, 846, 2010, 2524, 668, 2153, 688, 2745, 3071}, (2465, 0): set(), (2877, 0): set(), (729, 0): {89, 2804, 1625, 2618}, (2400, 0): {3206, 2248, 525, 2257, 2804, 1625, 2618, 1630}, (1521, 0): {3206, 2248, 525, 2257, 2804, 1625, 2618, 1630}, (245, 0): {89, 3206, 2248, 525, 2257, 2804, 1625, 2618, 1630}, (249, 0): {3342, 2772, 2678, 343, 1432}, (3240, 0): set(), (1497, 0): {2248, 3092}, (1365, 0): {2657, 1415, 2728, 1356, 1394, 56, 2557, 799}, (273, 0): {2657, 1493, 343}, (2394, 0): {1543, 1035, 2832, 147, 925, 2597, 1714, 1845, 2745, 2751, 2884, 2766, 2772, 1238, 343, 2906, 3169, 2657, 2280, 2412, 2032, 1525, 375}, (1418, 0): {2823, 587, 2511, 1679, 1748, 405, 3092, 3044, 1766, 2415, 314, 894}, (3107, 0): {1536, 2948, 2443, 787, 1558, 1562, 160, 297, 2605, 2351, 1717, 698, 2876, 2906, 1505, 2786, 2532, 2533, 1137, 2554, 2429}, (2525, 0): {2657, 846}, (2567, 0): set(), (905, 0): {1056, 608, 2415, 3092, 1023}, (2757, 0): {2112, 3078, 2890, 3342, 2906, 1244, 2657, 2278, 1524, 2686}, (2955, 0): {2657, 1220, 2766, 1394, 343}, (3253, 0): {1714, 423}, (3192, 0): {32, 1610, 2772, 3092, 343}, (1242, 0): {1489, 2363, 2415}, (797, 0): set(), (63, 0): {1606, 1110, 2939}, (2515, 0): {778, 653, 400, 1169, 1363, 3092, 343, 2655, 97, 100, 2022, 1130, 499}, (693, 0): set(), (3124, 0): {3138, 1480, 3092, 279, 1306, 219, 545, 418, 100, 2415, 1455, 1588, 2235}, (387, 0): {3206, 2248, 3050, 525, 2257, 2804, 1625, 2618, 1630}, (2839, 0): {1280, 1861, 2824, 2633, 778, 653, 400, 3092, 1245, 608, 100, 2022, 1128, 1769, 2415, 2418, 3004, 2237, 2686}, (1214, 0): set(), (2711, 0): set(), (2776, 0): {2560, 260, 773, 2823, 2952, 1545, 3227, 1697, 2213, 2728, 2858, 2474, 1072, 820, 2872, 314, 1595, 963, 710, 72, 2901, 2020, 3045, 630, 3319, 2040}, (134, 0): {1394, 2383}, (3164, 0): {2657, 2823, 2799, 405, 314, 2906}, (3034, 0): {192, 801, 587, 1748, 1941, 3092, 2874, 127}, (3177, 0): {2657, 1649, 2779}, (3226, 0): {3268, 1821, 606, 3102, 669, 2913, 1506, 2657, 798, 292, 806, 1003, 1196, 1390, 1658, 3198}, (1846, 0): {1477, 1226, 13, 2253, 540, 1821, 2657, 997, 2728, 1394, 307, 56}, (2648, 0): {160, 2660, 2055, 3083, 1937, 2323, 51, 1523, 628}, (2211, 0): {2657, 2793, 2005, 2295, 343}, (2007, 0): {2876, 1319, 819, 757, 1238, 1559, 124, 2814}, (1059, 0): {160, 2657, 2149, 1394, 2838, 2458}, (311, 0): {2918, 1094, 1161, 2316, 2287, 3092, 3128, 2745, 2524}, (777, 0): {2918, 1094, 1161, 2316, 2287, 3092, 3128, 2745, 2524}, (2406, 0): {1730, 3092, 212, 863}, (2445, 0): set(), (2212, 0): set(), (1021, 0): {2657, 2947, 422, 2823, 3083, 3117, 1327, 314}, (1981, 0): {3137, 1226, 13, 2192, 1041, 340, 2456, 2458, 154, 2399, 736, 160, 2856, 2930, 1140, 53}, (3116, 0): {2466, 405, 2823, 1358}, (2374, 0): {3138, 2823, 457, 2186, 846, 1744, 405, 1238, 218, 1178, 2976, 738, 2671, 2678, 314, 1659, 2811}, (967, 0): {2378, 784, 2961, 2005, 343, 26, 2657, 2728, 1194, 1394, 630, 59, 2557}, (1073, 0): {1665, 2823, 1288, 2377, 2517, 2906, 863, 160, 2160, 2676, 3000, 314}, (2390, 0): set(), (2023, 0): {3206, 1606, 2350, 1110, 2939}, (2842, 0): {2691, 997, 2549, 343, 2458, 1821, 1471}, (1619, 0): {3299, 1732, 2918, 2745, 1308, 2942}, (66, 0): {2657, 1477, 2728, 1226, 2253, 1394, 56, 540}, (989, 0): {1477, 3080, 1226, 2253, 2458, 540, 1821, 736, 2657, 997, 2728, 1394, 53, 2549, 56}, (2139, 0): {1792, 2657, 1477, 2629, 2283, 2001, 1394, 2803, 2686}, (2897, 0): {2772, 2811, 2237}, (485, 0): set(), (1063, 0): {3206, 2823, 2055, 3083, 977, 2521, 1631, 740, 2991, 314, 2301}, (2785, 0): set(), (2940, 0): set(), (2893, 0): set(), (950, 0): set(), (1772, 0): {1220, 3142, 2446, 411, 3231, 416, 1503, 2918, 3185, 1587, 1977, 2876}, (1017, 0): {1220, 2772, 343, 2458, 2207, 608, 2657, 103, 2728, 1394, 307, 2811}, (2975, 0): {2918, 2765, 2287, 3128, 2745, 1945, 383}, (1012, 0): {2657, 1673, 2607, 1599}, (771, 0): {3138, 2957, 403, 2911, 224, 2722, 418, 228, 2918, 2287, 569, 1399, 2745, 314}, (1241, 0): set(), (2426, 0): {3342, 343}, (964, 0): {3092, 343}, (2152, 0): {2657, 2918, 1308, 2287}, (3316, 0): {2657, 1220, 2766, 1394, 343, 62}, (2002, 0): {3206, 2823, 3083, 1425, 1682, 2323, 2710, 939, 2995, 1718, 314, 2249, 846, 2906, 1501, 1374, 740, 3301, 1392, 2550}, (1428, 0): {343, 2458, 1356, 2415}, (82, 0): {1730, 976, 212, 1174, 278, 2237, 639}, (322, 0): {160, 138, 1357, 882, 408, 1947}, (2523, 0): {1888, 3165}, (1054, 0): {2691, 1220, 1226, 2511, 340, 2458, 1821, 2207, 736, 2657, 997, 2728, 1394, 2549, 56, 3002, 379}, (842, 0): {3138, 2823, 846, 2772, 661, 2458, 3040, 3239, 3048, 2153, 1518, 2415, 1714, 2745, 2618, 1147, 2876}, (42, 0): {2181, 3206, 2823, 3083, 1682, 160, 737, 740, 2277, 241, 2618, 2745, 314, 2876}, (1237, 0): {1537, 3206, 2823, 3336, 2055, 3083, 2829, 1050, 1690, 2991, 314, 446, 2891, 1105, 2005, 2521, 740, 1768, 2672, 2033, 2290, 2301, 3070}, (2073, 0): {2657, 3265, 2728, 1196, 2253, 1649, 1394, 343}, (382, 0): {2657, 997, 3080, 2728, 1492, 2772, 2745, 2811}, (3183, 0): {2307, 1797, 3275, 2772, 3092, 343, 3102, 2657, 2913, 1895, 2408, 2728, 1649, 1394, 56, 2811, 2557}, (1759, 0): {2745, 1586}, (2716, 0): set(), (1402, 0): set(), (199, 0): set(), (133, 0): {1536, 2051, 2948, 2605, 2736, 2768, 2102, 2554, 2876}, (914, 0): {3206, 3273, 523, 3277, 1178, 870, 422, 2665, 619, 2549, 2678, 3134}, (325, 0): set(), (889, 0): {2657, 1797, 2728, 1513, 1612, 371, 183, 2557}, (2077, 0): {2657, 357, 239, 1777, 343, 1657, 2269}, (3054, 0): {1664, 2436, 1158, 1670, 510, 1037, 3086, 1940, 2844, 540, 1694, 544, 161, 672, 160, 1337, 2107, 1217, 1860, 1226, 2763, 2766, 2511, 3023, 343, 3032, 736, 2657, 997, 2411, 2030, 2036, 120, 1406}, (2835, 0): {160, 668, 1417, 1293, 688, 2745, 2524}, (734, 0): {2657, 1394, 2772, 56, 2811}, (2790, 0): {3206, 2823, 2055, 1033, 3083, 2521, 740, 2415, 2991, 314, 2301}, (30, 0): {3092, 2415}, (1305, 0): {160, 289, 1730, 521, 1674, 272, 1178, 987}, (492, 0): {2657, 2511, 432, 1394, 62}, (834, 0): {400, 3043, 3092, 181}, (2331, 0): {3092, 2876, 2618, 3004}, (2992, 0): {1161, 2316, 3092, 533, 1433, 1198, 307, 3128, 2745, 1094, 1478, 3144, 1356, 2511, 207, 3026, 597, 2524, 3173, 2918, 2287}, (353, 0): {2823, 2064, 405, 1238, 218, 2906, 987, 1757, 2976, 288, 160, 2657, 2678, 314, 2811}, (473, 0): set(), (2079, 0): {1669, 3142, 2446, 411, 3231, 416, 292, 2918, 2287, 3185, 2876, 3327}, (841, 0): {2945, 3206, 1807, 1168, 2971, 1822, 929, 2657, 997, 2678, 1149, 191}, (3182, 0): {2945, 3206, 1807, 1168, 2971, 1822, 2657, 929, 2678, 1149, 191}, (715, 0): {2945, 3206, 1807, 1168, 2971, 1822, 929, 2678, 1149, 191}, (811, 0): {2055, 2823, 3083, 977, 1557, 2521, 160, 2657, 740, 1765, 2991, 2301}, (2512, 0): {2657, 2972}, (152, 0): {2657, 2972}, (1171, 0): {530, 3043, 3092, 181}, (1080, 0): {3138, 2823, 2126, 720, 2768, 614, 2160, 2802, 3063, 2745, 314, 2876}, (1763, 0): {454, 1688, 2906, 160, 100, 2661, 486, 2860, 2673, 2357}, (1695, 0): {704, 3265, 1098, 2253, 467, 2657, 426, 556, 1262, 1394, 375, 56, 828, 2557}, (86, 0): {160, 2444, 688, 1361, 3092}, (2612, 0): {256, 2120, 2633, 2187, 2579, 2395, 2657, 2082, 100, 999, 937, 1394, 1780, 1847, 2811}, (87, 0): {2657, 2728, 1394, 56, 2557}, (2076, 0): {1806, 1266, 2772, 1720, 2745, 1023}, (686, 0): set(), (210, 0): {3206, 2088, 525, 3092, 919}, (2066, 0): {2055, 2823, 2991, 2521, 2301}, (358, 0): set(), (2792, 0): {1669, 2629, 1736, 2522, 2237}, (1389, 0): {3092, 1820, 132, 2524}, (1213, 0): {2883, 1142, 2207}, (1827, 0): {3265, 3268, 1288, 2253, 2326, 1433, 347, 2657, 292, 2535, 2728, 1196, 2609, 1394, 2422, 56, 2557, 62}, (234, 0): {194, 579, 13, 2772, 2458, 1821, 2207, 608, 2657, 2340, 1394, 307, 2549, 2237}, (193, 0): {2112, 2657, 657, 2906}, (1602, 0): {801, 587, 3092, 1748, 1941, 127}, (3303, 0): {1415, 2728, 1356, 1394, 56, 2557}, (2738, 0): {608, 3092, 2549, 2458, 1821}, (2291, 0): {3100, 829}, (112, 0): {2657, 391, 1194, 1708, 534, 3097, 666, 1918}, (1915, 0): set(), (1653, 0): {3138, 3342, 1746, 467, 2772, 343, 1432, 427, 2859, 2678, 3003}, (479, 0): set(), (2026, 0): {1730, 2195, 3092, 212, 278, 285, 863, 160, 2678, 3004, 2237}, (862, 0): {2876, 1703, 1913, 2618, 2780, 2206}, (1298, 0): {1669, 2439, 1161, 2130, 163, 2918, 3176, 2287, 2745, 379}, (2902, 0): {1537, 3206, 2055, 2823, 638, 3083, 2521, 740, 2991, 314, 2301, 2686}, (1949, 0): {1537, 2309, 3206, 2055, 2823, 638, 3083, 3092, 2521, 740, 45, 2991, 2549, 314, 2301, 2686}, (3104, 0): {2055, 2823, 3083, 2991, 3092, 343, 2301}, (1965, 0): set(), (129, 0): set(), (1066, 0): {2823, 2894, 2745, 2906, 511}, (3051, 0): set(), (546, 0): {2337, 587, 2415, 1748, 3092}, (215, 0): {2337, 587, 2415, 1748, 3092}, (2137, 0): {2337, 587, 2415, 1748, 3092}, (466, 0): set(), (868, 0): {3080, 138, 1226, 1807, 343, 408, 1048, 1821, 160, 2148, 997, 292, 2535, 1705, 2219, 882, 498, 1534}, (3027, 0): {3138, 844, 2063, 2768, 1746, 2906, 539, 92, 2618, 2235, 2876}, (2637, 0): {3206, 585, 2121, 525, 1939, 3092, 3288, 35, 612, 1957, 1641, 2095, 2418, 181, 374}, (696, 0): {160, 805, 997, 775, 2549, 2458, 1821, 2686}, (722, 0): {2678, 314, 2811}, (2369, 0): {32, 1610, 2925, 3283, 2772, 343, 2553}, (1131, 0): {2060, 2765, 1945, 798, 2918, 873, 2287, 881, 2745, 383}, (2167, 0): {2880, 2506, 587, 1748, 3092, 603, 2337, 1771, 3309, 750, 2415, 2034, 2868}, (1355, 0): {1605, 1797, 1927, 340, 343, 2458, 1821, 2657, 997, 2728, 2609, 1394}, (1111, 0): {1990, 2124, 3092, 3159, 2074, 3043, 997, 2534, 496, 498, 1145, 2238}, (558, 0): {1161, 605, 3135}, (3335, 0): {1613, 464, 1748, 3092, 2458, 1821, 3102, 2592, 1442, 759, 494, 2415, 498, 1267, 1013, 2743, 1276, 1919}, (437, 0): {3092, 7}, (442, 0): {1094, 1478, 1161, 2316, 3092, 2524, 2918, 2287, 3128, 2745}, (73, 0): {1094, 1478, 1161, 2316, 3092, 2524, 2918, 2287, 3128, 2745}, (2544, 0): {1478, 1094, 1161, 2316, 3092, 2524, 2918, 2287, 3128, 2745}, (1248, 0): {1094, 1478, 1161, 2316, 3092, 2524, 2918, 2287, 3128, 2745}, (2899, 0): {2657, 510, 2728, 2253, 1394, 56, 62}, (1272, 0): {2976, 2823, 314, 405, 2678, 277, 1238, 218, 2811}, (1466, 0): {1544, 267, 3087, 3092, 2974, 1958, 1454, 1201, 2226, 2356, 692, 1595, 3135, 1736, 329, 1748, 1504, 1383, 3180, 494, 2415, 2549, 2424}, (2282, 0): {56, 1394}, (3052, 0): {32, 1610, 3092, 2772, 343}, (1774, 0): {32, 1610, 3092, 2772, 343, 2745}, (2865, 0): {3265, 1797, 1927, 2253, 1173, 2326, 1433, 347, 2657, 2728, 2609, 2557, 62}, (1576, 0): {32, 1610, 3092, 2772, 343, 2553}, (1999, 0): {2253, 1433, 2458, 1821, 2657, 2728, 1394, 949, 56, 251, 2557}, (2878, 0): {1238, 3144, 286}, (898, 0): set(), (1858, 0): {3144, 286}, (1049, 0): {3092, 3144, 286}, (1163, 0): {3144, 286}, (2500, 0): {1714, 2546}, (480, 0): {1238, 3144, 286}, (2261, 0): {3144, 443, 3004, 286}, (190, 0): {3144, 443, 286}, (2808, 0): {3144, 286}, (455, 0): {3144, 443, 286}, (2569, 0): {1238, 3144, 286}, (1508, 0): {3144, 286}, (178, 0): {3043, 3206, 778, 1964, 2551, 825}, (1379, 0): {1571, 3082, 3084, 3092, 633, 127}, (1678, 0): {160, 2561, 2657, 3337, 945, 2906, 2491}, (348, 0): {160, 2561, 2657, 3337, 945, 2906, 2491}, (1873, 0): {2657, 1394, 56, 62}, (2110, 0): {160, 2487, 2248, 1293, 3342, 343, 1720}, (2375, 0): {1923, 1658, 1086}, (2762, 0): {3138, 774, 2823, 587, 2508, 3092, 1748, 405, 1628, 2401, 865, 2466, 3044, 1766, 1074, 1913, 314}, (2093, 0): {3206, 3273, 3277, 2772, 791, 343, 2458, 2657, 1569, 2736, 1844, 2745, 1658, 2495}, (652, 0): {2772, 343, 2745}, (2817, 0): set(), (2870, 0): {100, 778, 587, 400, 3092}, (2770, 0): {2657, 1490, 2258, 598, 343}, (3127, 0): {2481, 3092, 1167}, (2038, 0): {1409, 1669, 1238, 2653, 94, 1374, 673, 996, 1840, 2618, 2427, 2876}, (817, 0): {1409, 1220, 1669, 3092, 1238, 2653, 1374, 94, 673, 996, 1840, 2618, 2427, 2876}, (2537, 0): {1730, 1989, 1226, 2511, 340, 212, 2458, 1821, 863, 160, 997, 1710, 2487}, (1320, 0): {1220, 3080, 105, 3243, 2415, 1587, 3092, 3323, 2207}, (3205, 0): {1220, 2436, 2823, 3145, 1359, 3025, 1042, 3089, 798, 160, 495, 2160, 1329, 314}, (1933, 0): {160, 3310, 2415, 1589, 346}, (385, 0): {2432, 1613, 1748, 3092, 1494, 794}, (204, 0): {1345, 1410, 2379, 588, 3092, 2522, 1383, 235, 1452, 2356}, (79, 0): {2657, 1251, 2858, 1194, 1649, 2771, 2682}, (1876, 0): {2564, 494, 2895, 1014, 2398}, (3329, 0): {3092, 1221, 94}, (968, 0): {3144, 286}, (1065, 0): {1238, 3144, 286}, (135, 0): set(), (2949, 0): {2823, 3080, 8, 2991, 3092, 343, 314, 2301}, (3146, 0): {3092, 1221, 94}, (25, 0): {1537, 1158, 2823, 3206, 3083, 3092, 1303, 2458, 1821, 2991, 3002, 314, 1220, 1221, 2766, 2521, 2658, 740, 997, 2415, 2301}, (3175, 0): {1537, 1221, 3206, 2823, 2055, 3083, 3092, 2521, 94, 2658, 740, 2991, 2415, 314, 2301}, (1566, 0): {1537, 1221, 3206, 2055, 2823, 3083, 3092, 2521, 94, 2658, 740, 2991, 2415, 314, 2301}, (316, 0): {2657, 2728, 1394, 503, 56, 2557}, (916, 0): set(), (1581, 0): set(), (1302, 0): {2337, 2760, 1609, 1421, 3092, 2132}, (2205, 0): {2793, 2005}, (2887, 0): {836, 1094, 396, 2063, 537, 2786, 997, 2918, 2535, 3055, 3185, 2355}, (981, 0): {1421, 3342, 271, 3092, 2325, 1312, 2337, 2979, 2981, 3110, 2868, 709, 587, 846, 1748, 1756, 3040, 1252, 3309, 2034}, (1541, 0): {2195, 1750, 2824, 2237}, (3174, 0): {3206, 617, 846, 3191, 2876, 2334}, (122, 0): {3206, 3273, 617, 846, 3191, 2745, 2876, 2334}, (441, 0): {1442, 2436, 2511, 2415, 3064, 2906}, (2175, 0): {3092, 2415}, (947, 0): {2418, 3092, 181, 3032, 605}, (402, 0): {2401, 2823, 3239, 405, 314}, (2840, 0): {2697, 1613, 494, 2415, 1748, 3092, 758, 1819}, (1040, 0): {160, 2657, 1864, 1356, 2803, 724, 533}, (425, 0): set(), (2727, 0): {3138, 3080, 8, 2123, 3342, 1235, 3092, 418, 2787, 2918, 1450, 235, 314, 2287, 2618}, (2577, 0): {3206, 2823, 2055, 1033, 3083, 3092, 2521, 740, 2415, 2991, 314, 2301}, (856, 0): {1280, 1861, 2824, 2633, 3092, 1245, 2657, 100, 2022, 1128, 2415, 3004, 2237, 2686, 3135}, (2457, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 2287, 3128, 2745}, (2864, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 2287, 3128, 2745}, (3260, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 2287, 3128, 2745}, (1995, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 2287, 3128, 2745}, (12, 0): {160, 3092, 846}, (4, 0): {343, 846, 3342}, (1522, 0): set(), (2256, 0): {646, 403, 2964, 88, 2906, 93, 228, 2918, 2287, 242, 2618, 314}, (1994, 0): {2657, 1797, 1927, 2609, 1394}, (2027, 0): {2820, 1674, 1036, 272, 605, 863, 160, 818, 1332, 1852}, (1829, 0): set(), (701, 0): {130, 100, 2022, 1462, 1720, 635, 2237}, (578, 0): {3268, 3081, 1356, 724, 533, 2657, 610, 2854, 2610, 2363}, (2362, 0): {1280, 2433, 3203, 2824, 138, 11, 780, 2962, 3092, 408, 2200, 153, 540, 159, 2464, 160, 930, 932, 44, 2861, 2989, 52, 314, 2237, 192, 1861, 2630, 2511, 3153, 468, 343, 2271, 2657, 100, 1128, 2415, 2935, 2686}, (420, 0): set(), (312, 0): {1596, 3092}, (2801, 0): {1862, 2925, 343, 2553, 2876}, (910, 0): {121, 2876}, (1909, 0): {337, 3089, 2553}, (727, 0): {2657, 1797, 1927, 2609, 1394, 1493, 62}, (336, 0): {2657, 1220, 2766, 1394, 343, 62}, (1623, 0): {3206, 2055, 2823, 3083, 977, 1557, 2521, 160, 2657, 740, 1765, 2991, 314, 2301, 2686}, (1905, 0): set(), (909, 0): {1730, 976, 212, 1174, 987, 2237}, (299, 0): {2657, 1394, 838}, (2718, 0): {160, 2657, 838, 1394, 3314, 2678, 2811, 2876, 2237}, (1038, 0): {801, 587, 127, 1748, 1941, 3092, 2874, 3004, 3135}, (1031, 0): {801, 587, 127, 1748, 3092, 1941, 2874, 3004, 3135}, (1285, 0): set(), (835, 0): {1764, 2509, 846, 1238, 3158, 2745}, (333, 0): {192, 130, 2244, 1925, 1157, 3092, 162, 3043, 2022, 1384, 1769, 635}, (650, 0): {337, 3089, 2553, 2458, 1691, 1821}, (2919, 0): {3138, 2951, 403, 2772, 2906, 160, 418, 228, 2918, 2219, 2287, 305, 1588, 2745, 2618, 1595, 3261}, (1087, 0): {2368, 2624, 447, 2698, 780, 2380, 1232, 160, 2657, 1513, 1836, 2991, 1971, 1791}, (2396, 0): {997, 3206, 525, 880, 3092}, (1921, 0): {132, 1854, 2415}, (2183, 0): {2307, 2308, 2885, 716, 1292, 3006, 2005, 1115, 2657, 3302, 2473, 2666, 875, 2232, 1982}, (317, 0): {2368, 2624, 447, 2698, 780, 2380, 1232, 160, 2657, 1513, 1836, 2991, 1971, 1340, 1791}, (315, 0): {160, 2657, 2055, 2823, 2415, 2991, 2521, 2301}, (2365, 0): {1220, 997, 337, 3089, 2553, 1691}, (2319, 0): {2657, 1394, 2728}, (2423, 0): {837, 2503, 2823, 1739, 15, 2961, 2657, 3298, 2728, 1397, 314}, (179, 0): {3092, 2415}, (430, 0): {2881, 778, 587, 653, 400, 1169, 3092, 605, 160, 928, 1515, 1260, 2418, 499}, (861, 0): {1009, 2323, 343, 3103}, (815, 0): {3206, 1610, 846, 2772, 343, 1433, 32, 2657, 160, 1191, 2925, 2553, 2160, 1525, 2745}, (3147, 0): {3138, 235}, (406, 0): {929, 1062, 748, 1133, 2900, 2649}, (1900, 0): {1161, 605}, (557, 0): set(), (2470, 0): {1799, 1736, 2505, 522, 3342, 3092, 343, 154, 2906, 2022, 1453, 1973}, (2131, 0): {2657, 3265, 1394, 1492, 56, 540, 62}, (1618, 0): {160, 2657, 2678, 1178, 2811, 799}, (2909, 0): {2281, 1293, 688, 788, 2296, 2524}, (259, 0): set(), (39, 0): set(), (1954, 0): {2657, 2728, 2766, 1394, 343, 2557, 62, 1503}, (2852, 0): {160, 138}, (2706, 0): {2882, 13, 2063, 343, 156, 1821, 736, 2657, 997, 2728, 1394, 1587, 3060, 2549, 1590, 56, 2557}, (3171, 0): {673, 3075, 1370, 1934, 1238, 2618, 2427, 2876, 1374}, (717, 0): {98, 3092, 2415}, (2056, 0): {1888, 160, 819, 2956}, (812, 0): {1730, 212, 863}, (110, 0): set(), (1783, 0): set(), (3039, 0): {1952, 1190, 104, 1520, 2554, 2618, 2876, 2906}, (2778, 0): {490, 2415, 1748, 3092, 217}, (1608, 0): set(), (986, 0): {864, 2793, 2005, 2295, 343}, (813, 0): set(), (3204, 0): {544, 2192, 1140, 2709, 181, 1751, 506, 2906}, (143, 0): {3138, 587, 1748, 3092, 2974, 2337, 418, 3043, 3243, 434, 629, 1781, 2358, 127}, (444, 0): {2309, 3206, 2823, 2055, 1033, 3083, 3092, 1051, 157, 2991, 694, 314, 2367, 1736, 1866, 343, 2521, 740, 2415, 2301}, (651, 0): {520, 1417, 1293, 3023, 340, 1492, 668, 2524, 160, 2657, 2341, 493, 2478, 688, 2745}, (2494, 0): {2657, 1394, 2772, 56, 2811, 2237}, (2210, 0): {846, 3014}, (2619, 0): {840, 1238, 286, 160, 2977, 738, 296, 1268, 373, 2678, 3062, 2676}, (577, 0): {1167, 2481, 3092, 181, 343, 1821}, (3251, 0): {1730, 3092, 212, 863}, (2857, 0): set(), (3322, 0): {2307, 2885, 3206, 3275, 2128, 2772, 343, 347, 2657, 875, 2350, 1391, 1394, 306, 2869, 2745, 2557}, (1761, 0): {56, 1394}, (2847, 0): {418, 3138, 235}, (690, 0): {2657, 2728, 1394, 56, 2557}, (2846, 0): {2657, 1830, 1394, 56, 3131}, (2320, 0): {1226, 2253, 2458, 540, 1821, 736, 2657, 997, 2728, 1394, 2549, 56}, (1343, 0): {3206, 2823, 2055, 1033, 3083, 3092, 2521, 2458, 1821, 740, 2991, 2415, 314, 2301}, (344, 0): set(), (2611, 0): {2697, 1613, 494, 2415, 3092, 1748, 758, 1819}, (3321, 0): set(), (1052, 0): {846, 2640, 403, 2649, 92, 228, 2918, 1640, 2287, 3191, 2745, 1855}, (248, 0): {391, 1166, 1945, 412, 2204, 32, 2657, 2917, 1194, 1342}, (1647, 0): {997, 3206, 14, 48, 307, 1588, 1499, 2207}, (462, 0): {1669, 1158, 147, 3092, 213, 2458, 2075, 743, 1840, 443}, (2471, 0): {1669, 743, 2075, 1840, 3092, 443}, (2798, 0): {1857, 1669, 1736, 600, 2527, 2235, 1903, 1334, 2745, 443, 638}, (2572, 0): {160, 2890, 1294, 2415, 1150}, (1538, 0): {608, 1586, 2745, 2458, 1821}, (1085, 0): {2745, 1586}, (975, 0): {1220, 1164, 2253, 3030, 2207, 608, 2657, 292, 2535, 1394, 1587}, (3296, 0): {1888, 1238, 2745, 3292}, (1122, 0): {3138, 969, 846, 3092, 2649, 3042, 1960, 49, 181, 3067, 3135}, (3257, 0): {1730, 846, 1423, 212, 2487, 1178, 2237, 863}, (488, 0): {1539, 3206, 2772, 1750, 343, 1191, 2553, 2745, 2811, 2237}, (1187, 0): {1537, 2309, 3206, 2055, 2823, 3083, 2521, 2906, 160, 740, 36, 2991, 2549, 314, 2301, 2686}, (1639, 0): {2728, 1224, 2772, 2745, 2811}, (712, 0): {2657, 2766, 2772, 1077, 2811}, (3186, 0): {3206, 2823, 1736, 2055, 3083, 3092, 2521, 157, 740, 2415, 2991, 314, 2301, 1727}, (1424, 0): {2309, 3206, 2055, 2823, 1033, 3083, 3092, 2521, 740, 2991, 2415, 314, 2301}, (1243, 0): {1057, 184, 1273, 2972}, (590, 0): {2180, 1799, 3342, 2579, 1556, 343, 2906, 160, 2657, 100, 2022, 937, 3179, 1453, 371, 378}, (1275, 0): set(), (3053, 0): {2657, 549, 391, 2728, 1194, 718, 630, 343, 26}, (767, 0): {3144, 78, 687, 3092, 1238, 2458}, (574, 0): {1280, 520, 2824, 653, 783, 400, 1555, 3043, 100, 2022, 231, 1128, 107, 883, 181, 2233, 314, 2237}, (1743, 0): {971, 343, 472, 2586, 2335, 2415, 1394, 2932, 1142, 2363}, (1938, 0): set(), (386, 0): {1057, 3332, 1487, 1238, 1531, 184, 1273, 443, 2972}, (2360, 0): set(), (615, 0): set(), (2813, 0): {400, 1169, 1363, 3092, 343, 2655, 97, 100, 2022, 1130, 499}, (2683, 0): {2876, 1238}, (1675, 0): {2876, 1238}, (3187, 0): {138, 2196, 1300, 733, 160, 33, 1955, 357, 624, 116, 571, 2749}, (1076, 0): set(), (548, 0): {160, 1730, 369, 212, 2487, 987, 2237, 863}, (562, 0): {3138, 2823, 2772, 277, 1238, 2005, 405, 1178, 2976, 2401, 288, 2728, 2793, 2678, 2745, 314, 2811}, (2641, 0): {261, 2585, 3098, 476, 2589, 1182, 2657, 422, 487, 1961, 2794, 623, 564, 2678, 1279}, (563, 0): {1669, 2629, 1736, 2522, 2237}, (1968, 0): {2625, 1220, 1158, 2824, 3340, 1486, 343, 1560, 2207, 160, 2657, 1334, 314, 1086}, (1883, 0): {2657, 2728, 2772, 2745, 1178, 2811}, (2279, 0): set(), (592, 0): {2657, 2728, 2772, 2745, 1178, 2811}, (1591, 0): {2436, 2309, 3206, 2055, 2823, 1033, 3083, 2511, 2521, 740, 292, 2991, 2415, 2549, 314, 2301}, (502, 0): {70, 2055, 3206, 2823, 265, 3083, 3342, 2521, 605, 2784, 740, 2991, 314, 2301}, (413, 0): {2504, 822, 1945, 2618, 2876}, (1075, 0): {3342, 3092, 2678, 343, 1432}, (1282, 0): {2823, 3272, 343, 2107, 540}, (3076, 0): {1797, 327, 1479, 2253, 1178, 605, 608, 2657, 160, 2728, 1264, 2609, 2422}, (2407, 0): {32, 1610, 2772, 343, 2745}, (2642, 0): {1746, 3092, 2458, 1821, 736, 2657, 2659, 2728, 2415, 1394, 2549, 2744, 2557, 510}, (424, 0): set(), (793, 0): {1110, 1606}, (1203, 0): {1110, 1606}, (1008, 0): {1110, 1606}, (1119, 0): set(), (175, 0): set(), (2862, 0): {525, 3043, 3092, 2605}, (2049, 0): {1669, 2439, 1161, 2130, 163, 356, 2918, 3176, 2287, 2745, 379}, (351, 0): {1221, 1669, 2439, 1161, 2130, 405, 163, 2918, 3176, 2287, 443, 2745, 379}, (1430, 0): {343, 1178, 846, 3342}, (1179, 0): {2657, 1797, 3275, 174, 510}, (1495, 0): {3342, 2900, 343, 476, 2717, 2657, 2274, 422, 1969, 3126, 566, 2237}, (1529, 0): {2657, 2728, 1394, 56, 3102}, (1841, 0): {160, 2945, 3206, 1099, 1677, 527, 2960, 2618}, (3262, 0): set(), (2146, 0): set(), (3245, 0): {320, 2304, 2634, 587, 2237, 127}, (3229, 0): {516, 401, 2644, 2455, 411, 221, 2918, 235, 948, 2555}, (2349, 0): {1280, 2824, 400, 160, 3043, 100, 2022, 1128, 2415, 314, 2237}, (1230, 0): {2437, 1748, 3092, 1880, 923}, (2203, 0): {256, 1603, 1157, 1658, 343, 2456, 3098, 1178, 2906, 1691, 160, 2657, 1573, 493, 2678, 314, 2811, 1983}, (772, 0): {1600, 160, 2657, 995, 999, 2253, 2678}, (2888, 0): {2657, 1178, 1961, 3341, 594, 435, 2678, 2906, 796}, (3224, 0): {32, 1191, 1610, 2553, 2772, 343, 2745}, (2475, 0): {160, 1397, 2876, 2605}, (3017, 0): {2889, 1293, 846, 2130, 535, 608, 160, 163, 3176, 2153, 168, 2287, 1400, 379}, (1246, 0): {200, 3092}, (831, 0): {1346, 3273, 74, 3277, 1231, 1178, 2202, 2657, 870, 1401, 2811}, (2903, 0): {1024, 1542, 589, 2260, 343, 2597, 102, 2921, 3181, 302, 1845}, (2441, 0): {2147, 1333, 541, 3231}, (2165, 0): set(), (1775, 0): {714, 1290, 719, 1490, 3092, 154, 2458, 736, 997, 549, 2856, 685}, (2217, 0): {3206, 2823, 2055, 1033, 3083, 2521, 740, 2415, 2991, 314, 2301}, (1139, 0): {267, 665, 2522, 2657, 2595, 2086, 809, 2415, 2745, 3004}, (2986, 0): {2337, 1748, 3092}, (543, 0): {3138, 2503, 2823, 1739, 846, 2961, 3298, 418, 2728, 1397, 2745, 314}, (1701, 0): {1409, 1238, 94, 2618, 2427, 2876, 1374}, (1016, 0): set(), (2067, 0): {2657, 1714, 3092, 343, 3289}, (1978, 0): {3144, 1001, 1740, 846, 1807, 3252, 2618, 2876, 2045}, (274, 0): {1280, 2505, 2890, 2571, 1997, 400, 1043, 2970, 2906, 160, 2657, 2595, 1082, 1979, 2238}, (1291, 0): {540, 2823}, (762, 0): {3272, 2107, 540, 2958}, (57, 0): {2115, 1160, 3081, 1232, 148, 2456, 1691, 540, 544, 2657, 2408, 1513, 875, 493, 1390, 1394, 1715, 3002, 59}, (3324, 0): {2147, 3237, 1669, 3334, 2284, 3148, 1778, 540}, (745, 0): {1669, 2439, 1736, 1161, 2130, 163, 2918, 3176, 2287, 2745, 1850, 379}, (2188, 0): {1009, 2323, 3103}, (24, 0): {3092, 846}, (2391, 0): {1665, 2565, 3013, 2823, 1926, 2377, 846, 2517, 2906, 863, 160, 2160, 2932, 2676, 1142, 3000, 314}, (2987, 0): {418, 1960, 1611, 1714, 1588, 3125, 2071, 3289, 3065}, (645, 0): {2823, 459, 3089, 282, 160, 2657, 2601, 2090, 2286, 2160, 314, 2876}, (1172, 0): {3297, 3092, 3206, 525}, (1325, 0): {132, 1820, 1959, 1806, 3092, 2524}, (202, 0): {2657, 102, 2921, 2514, 565, 1055}, (2944, 0): {3206, 2823, 2252, 337, 3089, 1887, 2207, 2657, 2160, 314, 2750}, (2752, 0): set(), (2103, 0): set(), (2740, 0): {1751, 3342, 2192, 1140, 181, 343}, (2344, 0): {2786, 836, 2918, 1094, 2287, 3185, 2355}, (1944, 0): set(), (1935, 0): {836, 2918, 2287, 3185, 2355}, (38, 0): {1669, 743, 2075, 1840, 3092, 443}, (926, 0): {160, 929, 1600, 84, 2678, 343, 2811, 2493}, (542, 0): {160, 929, 1600, 84, 2678, 343, 2811}, (1206, 0): {3138, 2823, 846, 403, 2772, 92, 228, 2918, 1640, 298, 3191}, (83, 0): {2432, 846, 3092, 1748, 1819, 100, 3239, 940, 2348, 826, 445}, (3178, 0): {1543, 2248, 778, 3092, 1045, 1238, 984, 2590, 2657, 3043, 423, 2606, 2415, 1776, 2418, 1714, 181}, (1318, 0): {2823, 1238, 1962, 314, 305, 1588, 2618, 1786, 2876, 3261}, (660, 0): {160, 1191, 2553, 2772, 343, 2745}, (2321, 0): {3206, 2823, 3083, 1682, 2967, 160, 2723, 740, 361, 1520, 3191, 2745, 314, 2876}, (1403, 0): {647, 1226, 2449, 1749, 343, 2458, 540, 736, 2659, 2674, 2549, 181, 3002}, (491, 0): {2657, 1394, 2728, 56}, (2404, 0): {2304, 2823, 3092, 405, 2582, 3044, 3305, 3180, 127, 2415, 1460, 3191, 2616, 2745, 314, 1084, 895}, (1812, 0): set(), (742, 0): {2657, 2086, 2458, 3002, 1821}, (2714, 0): {160, 2657, 522, 2579, 1973, 3325}, (40, 0): {343, 2248, 2876}, (2927, 0): {1730, 418, 1960, 235, 1621}, (1713, 0): {481, 1543, 2972, 1714, 1238, 3289, 3036}, (1809, 0): {3206, 1353, 2772, 3092, 3289, 3043, 2923, 2797, 2350, 117, 181, 443}, (2866, 0): {268, 2064, 2455, 2841, 1178, 2330, 798, 418, 550, 1960, 2728, 2346, 561, 568, 2745, 3128, 1598, 3138, 3014, 2379, 847, 1744, 2511, 219, 221, 2918, 235, 2287, 2288}, (3152, 0): {160, 2138, 3342, 1295, 724, 343, 472, 2906}, (2098, 0): {2657, 2595, 809, 267, 2415, 665, 2745, 2522, 3004}, (155, 0): {2337, 1748, 3092}, (60, 0): {3138, 1748, 3092, 536, 2780, 798, 3233, 418, 2337, 235, 494, 629}, (211, 0): set(), (1580, 0): {2889, 1226, 2253, 13, 343, 2458, 540, 1821, 736, 2657, 997, 2728, 1394, 307, 56, 2557}, (3106, 0): {3136, 2373, 2353, 2772, 22, 2811, 2237}, (2214, 0): set(), (1681, 0): {343, 2728, 846, 3342}, (3166, 0): {1674, 272, 160, 1590, 2882, 1476, 1733, 328, 3028, 2005, 1750, 2906, 1757, 863, 2657, 2530, 2021, 2662, 2795, 1274, 1407}, (755, 0): set(), (1046, 0): {1394, 3102}, (226, 0): {2368, 2657, 1513, 2698, 2380, 780, 1232, 1340}, (636, 0): {192, 1283, 1222, 2695, 778, 653, 3342, 400, 3092, 160, 3043, 100, 2022, 2347, 2352}, (266, 0): {132, 1820, 2524, 1806}, (2645, 0): {640, 3139, 2695, 2824, 138, 23, 160, 357, 1128, 1512, 1971, 2745, 314, 571}, (655, 0): {2725, 3206}, (2703, 0): set(), (991, 0): {3274, 1550, 3092, 2772, 1785, 984, 3289, 732, 3295, 2528, 2276, 1064, 232, 2415, 1714, 2745, 2237}, (1350, 0): {228, 2287, 3096, 1945, 314, 2876}, (2753, 0): {228, 2287, 3096, 1945, 314, 2876}, (2708, 0): {2055, 2823, 2301, 2991}, (3091, 0): {1857, 1669, 1736, 1226, 2511, 1821, 2527, 736, 2235, 2549, 1334, 2745, 443, 638}, (1892, 0): {2192, 1174, 2458, 283, 1178, 160, 2084, 1960, 2346, 1587, 1588, 1078, 2873, 2235, 69, 1605, 1226, 997, 235, 1778, 1140, 1397, 2166, 379}, (512, 0): {2657, 3265, 806, 2728, 1196, 2253, 1649, 1553, 1394}, (1233, 0): {3105, 1826, 100, 517, 1157, 1416, 3120, 538}, (1735, 0): {2433, 2625, 2562, 194, 2766, 1553, 3290, 3100, 798, 2657, 869, 743, 2728, 1394, 56, 2042, 62}, (1459, 0): {3138, 2312, 2379, 2580, 221, 418, 2918, 1960, 2287, 3185, 2994, 314, 1404}, (2821, 0): {2657, 1220, 2766, 1394, 343, 62}, (331, 0): {1824, 673, 34, 2759, 142, 1238, 2618, 2427, 1374}, (2777, 0): set(), (364, 0): {1477, 1226, 13, 2253, 3023, 540, 1821, 1503, 2657, 997, 2728, 1394, 307, 56}, (3037, 0): {1606, 1110}, (2199, 0): {1543, 423, 3144, 1714, 2643, 1238}, (2501, 0): {2691, 171, 2768, 1908, 3225, 2876, 1821}, (1936, 0): {2618, 2554, 2876}, (2946, 0): {1220, 2823, 1483, 2508, 147, 3092, 405, 2135, 2458, 1821, 3044, 997, 2415, 2482, 1587, 2366}, (428, 0): set(), (2713, 0): {2245, 3206, 1606, 2350, 1110, 2939}, (136, 0): {1537, 2309, 3206, 2823, 2055, 638, 3083, 2521, 1631, 160, 2657, 740, 45, 2991, 2549, 314, 2301, 2686}, (1154, 0): {736, 2657, 2690, 154, 138, 2458, 3002, 2906}, (2156, 0): {2657, 2728, 1394, 2772, 2745, 2557}, (484, 0): {2245, 3206, 2823, 2894, 2005, 3030, 2906, 160, 167, 2351, 2415, 2549, 2745, 314, 443, 2300, 511}, (3162, 0): {524, 2449, 343, 2657, 356, 1636, 2535, 2728, 2219, 1394, 62}, (2108, 0): set(), (1663, 0): {163, 2918, 1736, 3176, 1161, 2287, 2745, 1850, 379}, (1304, 0): {1669, 2439, 1736, 1161, 2130, 163, 2918, 3176, 2287, 2745, 1850, 379}, (111, 0): {1669, 2439, 1736, 1161, 2130, 163, 2918, 3176, 2287, 2549, 2745, 1850, 379}, (2929, 0): set(), (1970, 0): {2112, 843, 2574, 1934, 657, 1238, 3122, 819, 2745, 2876}, (3151, 0): {3206, 846, 1811, 661, 3158, 601, 2350, 1714, 376, 2745}, (240, 0): {2309, 2823, 2055, 3083, 2415, 2991, 2521, 1051, 2301}, (1000, 0): {1537, 3206, 2823, 2055, 3083, 2521, 2906, 160, 740, 2991, 314, 2301, 2686}, (2583, 0): {3332, 1669, 1605, 1226, 1738, 3282, 1238, 1821, 1374, 1700, 1840, 2618, 3002, 2427}, (2236, 0): {1409, 1669, 1238, 1850, 1374, 1899, 1837, 113, 3188, 2618, 2427}, (2170, 0): {2337, 1748, 3092}, (1491, 0): {544, 2401, 2823, 2511, 307, 3092, 405, 314}, (2507, 0): {2337, 587, 2415, 3092, 1748}, (1199, 0): {2657, 2728, 2253, 46, 1394, 343, 56}, (2070, 0): {1888, 2350, 1679, 819, 3092, 1651, 2618}, (1685, 0): set(), (1725, 0): {160, 2657, 362, 1231, 3285, 2811, 2237}, (2157, 0): {160, 2657, 2341, 362, 1231, 3285, 2811, 2237}, (1348, 0): {3206, 2823, 3144, 1354, 2254, 2768, 2906, 92, 160, 229, 2350, 2737, 2618, 1848, 2745, 314, 2876, 1277}, (1801, 0): {2418, 94, 3342}, (2830, 0): set(), (2171, 0): {343, 2728, 846, 3342}, (674, 0): {343, 2728, 846, 3342}, (2189, 0): {846, 343, 2728, 3342}, (5, 0): set(), (1834, 0): {2657, 1797, 1927, 2728, 2253, 2609, 1394, 56, 2431}, (1859, 0): {418, 3138, 2598}, (2518, 0): {1409, 1669, 1934, 142, 3092, 1238, 1374, 2618, 2427, 2876}, (354, 0): set(), (281, 0): {1287, 2506, 1613, 750, 3092}, (3094, 0): {2925, 46, 2772, 3092, 503, 2553}, (802, 0): {13, 2458, 1821, 2207, 736, 544, 997, 45, 2549, 379}, (2133, 0): {160, 1730, 1583, 212, 278, 2237, 863}, (1207, 0): {587, 1748, 3093, 725, 3092, 2337, 2338, 1126, 494, 1202, 2741, 2806, 759, 1720, 758, 1468}, (2488, 0): {3092, 1145, 238}, (216, 0): set(), (1795, 0): {2657, 3265, 2728, 1196, 2253, 1394, 343}, (1533, 0): {272, 1674, 1178}, (3189, 0): {2557, 2253}, (679, 0): {1960, 235}, (2855, 0): {2178, 3331, 2705, 3092, 2458, 2459, 544, 1958, 1202, 307, 2226, 1867, 1748, 1368, 1504, 3041, 361, 3180, 494, 2415, 759}, (3221, 0): set(), (2499, 0): {2823, 3272, 2107, 540}, (2892, 0): {608, 2657, 1477, 1586, 2745}, (1431, 0): {993, 3144, 78, 1238, 1465}, (2849, 0): {1780, 213, 2941}, (287, 0): {2309, 3206, 2055, 2823, 3083, 2521, 740, 1897, 2991, 2415, 314, 2301}, (924, 0): {2309, 2823, 2055, 1033, 1226, 2511, 2521, 1691, 2991, 2415, 314, 2301}, (1650, 0): {32, 1191, 1610, 2772, 1525, 3092, 343, 2553}, (3056, 0): {2337, 938, 2415, 1748, 3092}, (407, 0): {32, 1610, 3092, 2772, 343, 2553}, (1351, 0): {2112, 2627, 2055, 1742, 2323, 1238, 1817, 799, 160, 1888, 740, 997, 806, 3122, 819, 1849}, (230, 0): {160, 419, 3206, 921}, (2809, 0): {184, 1273, 2972}, (821, 0): {1057, 3332, 1487, 1238, 1531, 184, 1273, 443, 2972}, (2343, 0): {3332, 1487, 1238, 1531, 2972, 160, 1057, 3233, 2415, 443, 184, 1273, 1850, 2235}, (901, 0): {1409, 1669, 1238, 2653, 1374, 673, 996, 1840, 2618, 2427, 2876}, (2143, 0): {672, 2657, 2728, 1394, 1747, 56, 2557}, (1331, 0): {3206, 2632, 3213, 846, 2638, 1110, 93, 160, 1443, 2350, 2618}, (2834, 0): {160, 289, 521, 2190, 1807, 2678}, (583, 0): {105, 3243, 2415, 3092, 3323}, (1286, 0): {544, 2549, 846, 2511}, (2783, 0): {530, 3092}, (1672, 0): {530, 3092}, (886, 0): {1219, 2182, 2876, 3080, 778, 2126, 2768, 3096, 1945, 1691, 3294, 160, 1760, 2144, 162, 2415, 314, 3004}, (675, 0): {3185, 2918, 1894, 2287}, (1711, 0): {2678, 314, 2811}, (2735, 0): {102, 1055}, (1113, 0): {530, 3092}, (741, 0): {530, 3092}, (2266, 0): {530, 3092}, (779, 0): {2370, 3202, 740, 295, 8, 3083}, (853, 0): {1669, 1840, 2931, 1238, 1850}, (244, 0): set(), (68, 0): set(), (1189, 0): {1793, 2118, 2823, 1737, 2059, 1485, 1744, 3092, 1622, 1438, 100, 2022, 1960, 2035, 1588, 314}, (497, 0): {138, 778, 1419, 653, 783, 400, 3092, 2197, 2464, 160, 928, 181, 2999, 2233, 2745, 192, 2890, 2772, 2906, 100, 356, 2022, 107, 1648, 2418, 760}, (501, 0): set(), (1156, 0): set(), (2450, 0): {160, 335, 1238, 3191, 2745, 2906, 2013}, (1301, 0): {2657, 609, 1777, 343, 3098, 476}, (1393, 0): {2309, 3206, 2055, 2823, 1033, 3083, 3092, 2521, 740, 2415, 2991, 314, 2301}, (2617, 0): {3206, 2055, 2823, 1033, 3083, 2521, 1691, 740, 292, 2415, 2991, 314, 2301}, (2434, 0): {3138, 1744, 2458, 3098, 987, 418, 1588, 2549, 2678, 1972, 1279}, (3249, 0): {3206, 2823, 1684, 1821, 160, 2980, 167, 2351, 2745, 314, 443, 3261, 2245, 2894, 2005, 2906, 2415, 2549, 887, 2300, 511}, (1481, 0): {2811, 2772}, (1162, 0): {1669, 2439, 1736, 1161, 2511, 2130, 163, 2918, 3176, 2287, 2745, 1850, 379}, (2083, 0): {2439, 1736, 1161, 3092, 163, 2918, 3176, 2287, 2745, 1850, 379}, (392, 0): {2245, 1669, 2439, 1736, 1161, 2130, 163, 2918, 3176, 2287, 2745, 1850, 379}, (123, 0): {1669, 2439, 1736, 1161, 2130, 3030, 540, 163, 2918, 3176, 2287, 2745, 1850, 379}, (1102, 0): {2055, 2823, 1226, 2991, 343, 2521, 2043, 2301}, (1069, 0): set(), (572, 0): set(), (1458, 0): {2190, 1178, 1051, 2458, 1821, 160, 418, 164, 2602, 2745, 2237, 1730, 1736, 1226, 2511, 212, 343, 987, 863, 2657, 3043, 997, 2415, 2553, 379}, (2774, 0): {3269, 198, 1293, 846, 2524, 160, 2918, 2287, 3185, 1720, 314, 2299}, (899, 0): {1411, 2822, 778, 2699, 525, 2003, 3092, 982, 1177, 94, 2336, 3043, 2804, 181, 374, 1911, 2936}, (706, 0): {1888, 2350, 1679, 819, 1651, 3092, 2618}, (106, 0): {1669, 1738, 1840, 1238, 2618, 2427, 1374}, (1987, 0): {2337, 1609, 1421, 3092, 2132, 1748}, (676, 0): {160, 2700, 2576, 2678, 1178}, (555, 0): {3092, 1588, 3144, 286}, (3012, 0): {1238, 286}, (702, 0): {2280, 2218, 2751, 1714, 2263, 3231}, (1376, 0): {3144, 286}, (2756, 0): {1090, 3043, 778, 1455, 400, 3092}, (677, 0): {643, 846, 3150, 596, 1238, 2458, 1374, 2092, 2618, 2427, 2876}, (3228, 0): {2060, 2765, 798, 2918, 873, 2287, 881, 3128, 2745, 383}, (1800, 0): {1543, 1255, 1714, 2546, 1238, 2263, 3004}, (1614, 0): set(), (3242, 0): {1543, 1255, 1714, 2546, 1238, 2263, 3004}, (1138, 0): set(), (453, 0): {2823, 3272, 2107, 540}, (2389, 0): set(), (1472, 0): {2768, 3191, 1511, 2618}, (2354, 0): {1060, 494, 335, 3092, 2398}, (1227, 0): {371, 3078, 41, 3342}, (1165, 0): {3269, 198, 1293, 846, 2524, 160, 2918, 2287, 3185, 1720, 314, 2299}, (1784, 0): {2824, 653, 783, 400, 1555, 3043, 100, 2022, 231, 1128, 107, 883, 181, 2233, 314, 2237}, (214, 0): set(), (2934, 0): {1797, 1352, 850, 2838, 1691, 925, 2657, 2914, 2728, 2869}, (55, 0): {1157, 2566, 589, 3092, 2260, 470, 343, 2597, 3181, 302, 1845}, (2463, 0): set(), (2240, 0): set(), (1527, 0): set(), (438, 0): {1669, 1293, 2447, 2192, 2966, 2524, 1726, 688, 1397, 181, 2238}, (507, 0): {3265, 1797, 1927, 3081, 2253, 1173, 2326, 1433, 3099, 347, 2657, 2728, 2609, 1394, 2557, 62}, (2739, 0): {2657, 1797, 1927, 2728, 2609, 2557, 62}, (1247, 0): set(), (1699, 0): set(), (790, 0): set(), (2201, 0): {2401, 3044, 1766, 2823, 2415, 1748, 405, 3092, 314}, (2702, 0): {1089, 1582, 1717, 2618, 2876}, (1668, 0): set(), (3315, 0): {2180, 1799, 3342, 2579, 1556, 343, 2906, 160, 2657, 100, 2022, 937, 1453, 371}, (2451, 0): {1352, 2253, 2772, 347, 1821, 2657, 997, 1196, 1391, 1649, 1394, 56}, (1152, 0): set(), (1117, 0): set(), (75, 0): set(), (3057, 0): set(), (1904, 0): set(), (2428, 0): {2112, 2627, 2055, 3083, 1742, 2323, 2772, 1238, 1817, 160, 1888, 740, 997, 3122, 819, 2484, 1849, 2237}, (764, 0): set(), (1917, 0): set(), (50, 0): set(), (2689, 0): {3231, 2280, 2218, 2263, 2751}, (2747, 0): {1543, 2280, 2218, 2751, 1714, 1238, 2263, 3231}, (3263, 0): {3231, 2280, 2218, 2263, 2751}, (1706, 0): {2758, 3144, 2768, 2452, 855, 3096, 1945, 2780, 2206, 2144, 228, 2918, 1703, 1129, 2287, 2618, 1913, 314, 2876}, (1951, 0): {1730, 1226, 13, 2195, 212, 340, 278, 2906, 987, 1821, 863, 736, 1710, 307, 2549, 2678, 314, 2237}, (451, 0): set(), (1853, 0): {641, 2630, 2827, 2511, 2458, 94, 608, 160, 2657, 997, 2030, 307, 120, 1337, 1023}, (1507, 0): {160, 2657, 2112, 2574, 657, 3122, 2005, 1238, 2745}, (2058, 0): {2918, 1094, 1161, 2316, 2287, 3092, 3128, 2745, 2524}, (632, 0): {1110, 1606}, (1729, 0): {3272, 2107, 540}, (474, 0): {3206, 1610, 2772, 343, 32, 2657, 2925, 1646, 2553, 2745, 1787}, (913, 0): {2945, 3206, 1289, 3276, 525, 2767, 3280, 2005, 2526, 160, 2657, 2529, 165, 766}, (1890, 0): {2945, 3206, 1289, 1610, 3276, 525, 2767, 3280, 2005, 2526, 160, 2529, 32, 2657, 165, 2728, 2028, 2811, 766}, (2460, 0): {2819, 580, 1736, 9, 1807, 2769, 915, 3092, 2906, 2595, 2022, 1642, 365, 2415, 2238}, (3259, 0): {1669, 1498, 2235}, (2015, 0): {2918, 2765, 2287, 2745, 1945, 383}, (2733, 0): {2918, 2765, 2287, 2745, 1945, 383}, (2467, 0): {3206, 3215, 1683, 1813, 343, 2458, 160, 1186, 379, 3199}, (682, 0): {1799, 2248, 1547, 587, 400, 3092, 160, 100, 2022, 301}, (2371, 0): {2728, 2657}, (1789, 0): {2528, 516, 934, 785, 948, 3092}, (1270, 0): {2657, 2947, 422, 2823, 3083, 3117, 1327, 314}, (2696, 0): {2248, 3342, 2772, 2678, 343, 1432}, (291, 0): set(), (2486, 0): set(), (2720, 0): {160, 1218, 482, 1384, 2248, 1519, 978, 1751}, (2285, 0): {1553, 2453, 3002}, (3069, 0): set(), (2184, 0): {3077, 3207, 3145, 3092, 3286, 2654}, (803, 0): {2207, 2657, 1225, 208, 1714, 1238, 984, 1658, 2751}, (654, 0): {2823, 329, 587, 1748, 405, 3092, 2401, 3044, 1766, 314, 3135}, (2984, 0): {3206, 2823, 3083, 2906, 731, 160, 740, 3115, 946, 1398, 314}, (2004, 0): {3042, 100, 2824, 3092, 1238, 1943, 2745, 314}, (225, 0): set(), (1985, 0): {2242, 3138, 1161, 1417, 587, 1820, 1441, 98, 2402, 418, 2918, 2287, 1588, 2745}, (1184, 0): {201, 2906}, (3088, 0): {505, 267, 1374}, (966, 0): {2248, 3342, 2678, 343, 1432}, (2154, 0): {1669, 2629, 1736, 2522, 2237}, (765, 0): {2629, 1669, 1736, 2522, 2237}, (1257, 0): {3138, 2248, 1452, 3342, 3092, 2487, 343}, (959, 0): {1730, 1226, 2190, 2511, 340, 212, 2458, 987, 1821, 863, 997, 1710, 2678, 2237}, (2387, 0): {2823, 3272, 2107, 540}, (2729, 0): set(), (2221, 0): {314, 1627, 781, 846}, (2845, 0): {1221, 2823, 3272, 540}, (1096, 0): {2823, 3272, 343, 2107, 540}, (2825, 0): {2823, 3272, 343, 2107, 540}, (3196, 0): set(), (2646, 0): {2691, 267, 912, 3092, 540, 163, 2342, 2856, 684, 45, 3128, 2745, 1220, 2630, 2766, 2511, 2262, 343, 2524, 736, 997, 2918, 3176, 236, 2413, 493, 2287, 2415, 3185}, (3058, 0): {3138, 708, 3206, 2823, 3083, 2707, 2780, 740, 314, 3004}, (3108, 0): {3206, 1606, 907, 2350, 3219, 2228, 695, 2618}, (3085, 0): {3206, 1606, 907, 2350, 3219, 2228, 695}, (2111, 0): {98, 3092, 2415}, (2159, 0): {2724, 2823, 3112, 3083, 946, 3191, 2745, 314, 2173}, (622, 0): {1158, 1159, 2759, 2823, 522, 267, 2198, 2521, 160, 2657, 1254, 2604, 1966, 1973, 2678, 761}, (2193, 0): set(), (2136, 0): {3206, 2823, 2759, 2055, 267, 3083, 3092, 343, 2521, 740, 1897, 2415, 2991, 314, 2301}, (1868, 0): {2309, 3206, 2055, 2823, 2759, 267, 3083, 3092, 343, 2521, 740, 1897, 2415, 2991, 314, 2301}, (2267, 0): {2309, 3206, 2759, 2823, 2055, 3083, 267, 3092, 2521, 740, 1897, 2415, 2991, 314, 2301}, (2978, 0): {2309, 3206, 2759, 2823, 2055, 3083, 267, 3092, 343, 2521, 740, 1897, 2991, 2415, 314, 2301}, (150, 0): {2055, 2823, 2415, 2991, 3092, 2521, 2301}, (1387, 0): {1744, 314}, (931, 0): {987, 1397, 2678, 314, 2811}, (2087, 0): {3206, 1798, 2823, 2055, 3083, 721, 151, 2521, 3167, 2657, 740, 1513, 2991, 314, 2301, 1791}, (146, 0): {2179, 2823, 3149, 405, 1238, 343, 2904, 3168, 2657, 2976, 2020, 2408, 2728, 875, 884, 314, 59, 2811}, (2755, 0): {836, 1094, 2918, 2287, 3185, 2355}, (2608, 0): {3092, 1863}, (2231, 0): {3092, 1863}, (1344, 0): {160, 1293, 2127, 688, 2745, 2010, 2524}, (2933, 0): {1542, 1810, 343, 1752, 1051, 2339, 2925, 2479, 1525, 2553}, (1025, 0): {1542, 1810, 2458, 1051, 1821, 2339, 997, 2925, 2479, 1525, 2553}, (953, 0): {3331, 361, 494, 1267, 3092, 759}, (1436, 0): {540, 343}, (2322, 0): {160, 1186, 3206, 3215, 1683, 1813, 343, 120, 3199}, (2905, 0): {3079, 3144, 2254, 2458, 2918, 1129, 2287, 2613, 568, 2618, 2876}, (3256, 0): {3079, 3144, 2254, 2918, 1129, 2287, 2613, 568, 2618, 2876}, (854, 0): {160, 1186, 3206, 3215, 1683, 1813, 343, 3199}, (2647, 0): {160, 1683, 3199, 3215}, (1526, 0): {160, 1683, 3199, 3215}, (3066, 0): {160, 3206, 3215, 1683, 1813, 343, 3199}, (839, 0): {160, 1186, 3206, 3215, 1683, 1813, 343, 3199}, (3021, 0): {160, 1186, 3206, 3215, 1683, 1813, 343, 3199}, (2998, 0): {160, 3215, 1683, 1813, 343, 3199}, (513, 0): {2436, 2054, 2765, 3217, 1686, 1691, 1308, 3293, 798, 736, 2918, 3050, 2287, 307, 3128, 2745, 2430, 383}, (276, 0): {1665, 2823, 1229, 846, 2011, 863, 160, 2676, 314, 2811}, (237, 0): {2657, 1730, 1394, 2728}, (363, 0): set(), (3234, 0): {160, 1186, 3206, 3215, 1683, 1813, 343, 3199}, (2050, 0): {2248, 3342, 2678, 343, 1432}, (1114, 0): {1394, 327, 1183}, (3047, 0): {3200, 1669, 1806, 2918, 1259, 2287, 3061, 2745, 2490, 2235, 1023}, (2230, 0): {32, 1191, 1610, 2553, 2772, 343, 2745}, (20, 0): {1861, 2694, 2824, 1736, 724, 2005, 1178, 602, 1882, 1757, 3172, 1128, 756, 500, 2237}, (3212, 0): {544, 2192, 1140, 181, 1751}, (2191, 0): {836, 2311, 280, 1945, 2918, 2539, 2926, 2618, 1209, 314, 1595, 2876, 3261}, (1839, 0): {1945, 2311, 314, 2926, 280, 1209, 2618, 2876}, (1877, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 2287, 3128, 2745}, (1528, 0): {2368, 1797, 1736, 2890, 2698, 780, 2380, 1232, 2458, 3167, 2657, 422, 1394}, (3333, 0): {2657, 1542, 1051, 1525, 181, 343, 635}, (393, 0): {163, 2918, 2439, 3176, 1161, 1736, 2745, 1850, 379}, (1323, 0): set(), (1123, 0): {2728, 2745, 2811}, (1516, 0): set(), (2388, 0): {1669, 1226, 396, 2958, 2511, 3023, 1492, 2324, 540, 3231, 544, 103, 1838, 307}, (359, 0): {1110, 1606}, (3035, 0): {1669, 2511, 1838, 3231}, (43, 0): {3138, 2823, 1611, 1714, 3092, 661}, (897, 0): set(), (1240, 0): {2769, 915, 100, 3033}, (404, 0): {864, 1542}, (697, 0): {256, 2180, 2633, 587, 2511, 16, 1170, 2579, 2265, 2395, 2657, 100, 1453, 1966, 2678}, (290, 0): {3206, 2823, 2055, 1033, 3083, 3092, 2521, 740, 2415, 2991, 314, 2301}, (941, 0): {2697, 1741, 1613, 1748, 3092, 2458, 1819, 798, 160, 2657, 494, 2415, 758, 1720, 1658}, (1896, 0): {2337, 2882, 707, 2273, 490, 436, 3092, 1748}, (792, 0): set(), (2988, 0): {1828, 1893, 1484, 1613, 3090, 3092, 2459, 1532}, (3210, 0): {3092, 1863}, (1818, 0): {3092, 1863}, (782, 0): {3092, 1863}, (1028, 0): {1101, 718, 849, 26, 2332, 1055, 1568, 2657, 549, 2213, 2728, 1194, 2174}, (1577, 0): {1029, 3206, 3016, 3273, 3277, 846, 3158, 1366, 160, 2657, 870, 2153, 2990, 3134, 3007}, (1067, 0): {1281, 300, 846, 3342, 2678, 343, 1432}, (1676, 0): {3202, 1429, 2968, 2328, 2458, 160, 3122, 819, 2484, 824, 1593, 1849, 2112, 1228, 1238, 1118, 1888, 2657, 2415, 1400, 1914, 379}, (1175, 0): set(), (429, 0): set(), (1134, 0): set(), (2227, 0): {3044, 2823, 2415, 3092, 1748}, (2593, 0): {3043, 3206, 778, 1964, 3092, 2551, 825}, (1371, 0): set(), (1597, 0): {3138, 1744, 3098, 2458, 1178, 987, 418, 1972, 2549, 1588, 2678, 1279}, (2910, 0): {160, 736, 1226, 267, 2549, 2458, 1821, 1758}, (2134, 0): {1394, 3102}, (414, 0): {3043, 3206, 778, 525, 560, 3092, 2618, 3005}, (182, 0): {1157, 2566, 589, 3092, 2260, 470, 343, 2597, 3181, 302, 1845}, (3291, 0): set(), (2761, 0): set(), (309, 0): {1665, 3138, 2823, 1095, 2377, 846, 2906, 925, 863, 160, 2160, 2676, 1588, 2678, 1335, 314}, (2875, 0): set(), (3194, 0): {3138, 1794, 2692, 1799, 1617, 1235, 3092, 29, 418, 100, 550, 2410, 1195, 235, 1455, 1204, 3130}, (139, 0): {2455, 3203, 1317, 267, 752, 593, 1879, 1598}, (911, 0): {418, 3138, 3144, 1129, 586, 2254, 2065, 1238, 2454}, (769, 0): {2918, 2316, 2765, 2287, 3128, 2745, 3293, 383}, (830, 0): {2657, 2243, 452, 2728, 875, 1263, 2961, 2838, 2397}, (888, 0): {1313, 564, 2678, 314, 987, 2811}, (1594, 0): {2657, 2243, 452, 2728, 1263, 2961, 2838}, (1413, 0): {3092, 1821, 2594, 1446, 2600, 1586, 1079, 3129, 2107, 2620, 189, 1345, 1221, 587, 2511, 2796, 494, 2415, 2036, 127}, (3236, 0): {1571, 3082, 3084, 1709, 3092, 633, 127}, (956, 0): {2337, 2403, 1712, 370, 1748, 3092, 3101, 2974}, (65, 0): {3206, 3191, 2635, 2876}, (1420, 0): {846, 343, 3342}, (992, 0): {2657, 1924, 809, 2890, 267, 329, 2745, 2522, 700}, (54, 0): {343, 846, 3342}, (1336, 0): {343, 2487, 2248, 3342}, (2615, 0): {3009, 1514, 2219, 2511, 19, 3092, 2259, 1691}, (711, 0): set(), (58, 0): {2113, 2657, 2246, 2438, 1394, 371, 2008, 2969, 510}, (2675, 0): {1586, 3138, 1477, 2745}, (3313, 0): {1543, 1611, 2379, 526, 1238, 2071, 3289, 3036, 1714, 3125, 3065}, (1310, 0): {1409, 1422, 1934, 1174, 1238, 2393, 988, 1850, 1374, 739, 1321, 1645, 1135, 1007, 380, 2618, 2427, 2876}, (1061, 0): {2657, 161, 343, 540}, (1953, 0): {2657, 161, 343, 540}, (1604, 0): set(), (770, 0): set(), (2652, 0): {1537, 2309, 3206, 2823, 3083, 13, 2458, 160, 936, 2985, 1579, 172, 2991, 314, 1727, 2496, 2754, 1226, 2686, 343, 3031, 2521, 983, 1891, 740, 239, 2549, 1910, 2301, 254}, (90, 0): {1136, 343, 221}, (2688, 0): {1797, 3342, 2966, 2838, 2973, 1315, 293, 2728, 1461, 1724, 2238, 960, 1352, 3275, 2650, 2657, 2914, 875, 1788, 510}, (338, 0): set(), (2726, 0): {2827, 13, 2458, 1821, 2207, 608, 544, 736, 997, 2549, 1023}, (421, 0): {3300, 1423}, (176, 0): {160, 2657, 102, 2695, 2921, 565, 1525, 2906}, (2359, 0): {2657, 2803, 724}, (1249, 0): {3138, 3092, 3019, 2140}, (2570, 0): {2192, 1140, 181, 1751, 2042}, (1112, 0): {1865, 1547, 3342, 400, 851, 3092, 789, 160, 3043, 100, 2022, 2347, 301, 2039, 1208, 2043}, (2097, 0): {608, 1220, 1587, 3030, 2207}, (1278, 0): {3265, 137, 2059, 2253, 2772, 2657, 1394, 2483, 56, 2237, 3327}, (1842, 0): {544, 1477, 879, 2709, 2614, 540}, (95, 0): {587, 3342, 2132, 1748, 3092, 1375, 3040, 2337, 170, 634}, (1802, 0): {2691, 997, 657, 3122, 2745}, (2099, 0): {2, 587, 653, 400, 3092, 2906, 160, 866, 3043, 36, 1707, 2219, 181}, (1088, 0): {2657, 264, 2795, 1710, 2064, 2195, 2005}, (448, 0): {2497, 2823, 265, 2698, 1232, 405, 2268, 3230, 2784, 357, 1512, 1256, 2219, 239, 314}, (1916, 0): {1730, 1407, 2195, 212, 2005, 278, 2678, 1274, 863}, (1155, 0): {162, 3338, 3342, 343, 1884}, (1015, 0): {1432, 2678, 343, 2248}, (3326, 0): {2823, 2573, 788, 343, 3163, 1440, 160, 882, 2296, 314}, (1509, 0): {2309, 1990, 2055, 2823, 3083, 3092, 343, 2521, 157, 2415, 2991, 431, 1588, 1850, 2301}, (1872, 0): {2415, 343, 319}, (2085, 0): set(), (584, 0): {1226, 2415, 2511, 2906, 607}, (1874, 0): {3206, 525, 1039, 85, 2204, 94, 1374, 547, 2409, 2804}, (552, 0): {2401, 3044, 1766, 2823, 2415, 3092, 1748, 405, 314}, (1381, 0): {3332, 1605, 1669, 1738, 2511, 1238, 1850, 1374, 1840, 118, 2618, 2427}, (1754, 0): {405, 2823, 314, 2799}, (877, 0): set(), (2575, 0): {850, 1394, 56}, (754, 0): {2657, 2728, 2932, 2745, 2811, 62}, (1250, 0): {1409, 2759, 1934, 1807, 1238, 860, 1374, 673, 173, 2427, 1911, 2618, 443, 2876}, (2041, 0): set(), (2293, 0): {2339, 1862, 2925, 1394, 343}, (2816, 0): {1394, 2772}, (1728, 0): {2823, 3272, 2107, 540}, (954, 0): {530, 3092, 2151, 3144}, (1369, 0): {544, 2192, 1140, 181, 1751}, (1211, 0): {540, 2823}, (2305, 0): {3206, 2055, 2823, 3083, 3092, 343, 2521, 740, 2991, 314, 2301}, (1907, 0): {1730, 3138, 1989, 2122, 212, 863, 160, 418, 1710, 2678, 2487, 2237}, (747, 0): {1220, 13, 2511, 3223, 343, 2649, 2458, 2207, 608, 2657, 160, 2728, 1649, 307, 56, 2425, 510}, (2162, 0): {565, 343, 313, 3342}, (304, 0): {1489, 2363, 2415}, (559, 0): {1489, 2363, 2415}, (2163, 0): {1489, 2363, 2415}, (257, 0): {3264, 3138, 26, 3098, 476, 160, 2657, 418, 355, 1279, 2678, 1144, 314, 187, 2237, 2811}, (2247, 0): {2657, 2005}, (2685, 0): {1649, 2728, 2557}, (3022, 0): {2945, 1026, 3206, 396, 2971, 1822, 544, 929, 1314, 1062, 681, 2993, 2485, 191, 3278, 846, 1746, 84, 2773, 2657, 2658, 997, 1390, 2678, 508, 1149}, (1561, 0): {514, 2116, 846, 14, 3092, 1239, 927, 2725, 2994, 2037}, (1445, 0): {1730, 3138}, (1032, 0): {1444, 3209, 2239}, (2664, 0): {160, 3138, 469, 3190}, (613, 0): {1736, 3096, 1945, 2906, 3043, 228, 2415, 2287, 2677, 314, 2876}, (2068, 0): {1606, 3206, 1549, 2254, 2350, 3092, 2618, 2876}, (3133, 0): {1352, 2772, 2006, 859, 2728, 1833, 108, 1585, 2745, 570, 2557}, (2541, 0): {1417, 1293, 846, 2010, 2524, 668, 2153, 688, 1720, 2745, 3071}, (1986, 0): {1572, 422, 334, 1815, 987}, (728, 0): {3328, 360, 3144, 2996, 2686, 1631}, (1755, 0): {160, 2784, 138, 1901, 2061, 1215}, (2169, 0): set(), (1551, 0): set(), (1574, 0): {2248, 235, 3342, 2487, 343}, (1176, 0): {355, 2678, 1721, 476, 2815}, (1103, 0): set(), (461, 0): {3331, 1748, 3092, 2458, 2459, 1504, 361, 3180, 494, 2415, 751, 2226, 1586, 759}, (169, 0): {160, 1633, 489, 2698, 2732, 1232, 1649, 2961, 2392}, (1467, 0): {321, 547, 2415, 2800, 627, 3092}, (2867, 0): set(), (2997, 0): {1409, 1669, 2876, 1934, 1238, 1309, 94, 1374, 1645, 2427, 443, 2618, 2235, 3004}, (1615, 0): {160, 2657, 2678, 1178, 799}, (2096, 0): {2800, 321, 627, 3092}, (2225, 0): set(), (166, 0): {1220, 2823, 587, 2508, 1748, 405, 3092, 1821, 2401, 3044, 997, 1766, 2415, 314}, (2044, 0): {1220, 2823, 587, 2508, 3092, 1748, 405, 2458, 1821, 2401, 3044, 997, 1766, 2415, 314}, (1185, 0): {2848, 1575, 1939, 2772, 1525, 3287, 2237}, (1011, 0): {3002, 1613, 494, 2415, 3092, 181, 1496, 2458, 2459}, (2234, 0): {1157, 2950, 1610, 3092, 2772, 343, 32, 162, 1191, 301, 1525, 2553, 2043}, (3068, 0): {1157, 2950, 1610, 2772, 3092, 343, 32, 162, 1191, 301, 1525, 2553, 2043}, (1212, 0): {2055, 2823, 2991, 2415, 3092, 2521, 314, 2301}, (1502, 0): {449, 870, 1704, 2603, 2477, 2990, 3134}, (349, 0): {1537, 3206, 2823, 1158, 2055, 638, 3083, 157, 160, 45, 2991, 1590, 314, 2882, 2521, 1631, 2657, 740, 2415, 2301, 2686}, (145, 0): {2918, 2765, 2287, 3128, 2745, 1945, 383}, (890, 0): {343, 3272, 2107, 540}, (908, 0): {3092, 2823, 540}, (1125, 0): set(), (2469, 0): {1280, 2824, 138, 11, 780, 2962, 3092, 2200, 153, 408, 2464, 160, 44, 2989, 2861, 52, 2237, 192, 3153, 468, 2657, 1128, 2415, 2935}, (1349, 0): set(), (3197, 0): {2657, 1394, 2728}, (31, 0): {1572, 422, 334, 1815, 987}, (3046, 0): {3080, 105, 3243, 2415, 3092, 3323}, (1053, 0): {1537, 2309, 3206, 2823, 2055, 3083, 2574, 657, 2991, 3122, 819, 2745, 2618, 314, 2876, 2112, 1238, 2521, 2906, 2657, 740, 1388, 2301}, (2161, 0): {2657, 2511, 1397, 2549, 2906, 1374}, (3267, 0): {736, 2657, 3265, 510, 2253, 1394, 56, 62}, (3095, 0): {1669, 2439, 1161, 2130, 163, 2918, 3176, 2287, 2745, 379}, (1530, 0): {1238, 286}, (326, 0): {1094, 2918, 1161, 2316, 207, 3092, 3128, 2524}, (1405, 0): {2918, 1094, 1161, 2316, 207, 3092, 3128, 2524}, (1188, 0): {2823, 2698, 405, 534, 2332, 412, 2461, 2976, 807, 2728, 1194, 2219, 554, 2745, 314, 3156, 1238, 2906, 2397, 2657, 2401, 2917, 999, 631, 2303}, (2510, 0): {2309, 2055, 2823, 2991, 2415, 3092, 2521, 2301}, (2782, 0): {846, 1108, 2678, 343, 1432}, (1124, 0): {2309, 3206, 2823, 2055, 1033, 3083, 205, 3092, 2521, 1824, 740, 3311, 2991, 2415, 314, 2301}, (1364, 0): {2337, 1748, 3092, 2415}, (2922, 0): {1394, 2728, 2811}, (2298, 0): {2337, 587, 2415, 1586, 3092, 1748}, (808, 0): {846, 343}, (683, 0): {846, 343, 3342}, (2104, 0): {3332, 493, 2768, 2745, 699}, (3157, 0): {2883, 2253, 2513, 1173, 342, 343, 665, 2329, 160, 2657, 935, 2224, 3184, 1328, 2610, 125, 383}, (2669, 0): {2823, 3272, 2107, 540}, (958, 0): {2885, 2511, 784, 724, 347, 1821, 2397, 159, 2657, 2728, 306, 2869, 2745, 891}, (99, 0): {2882, 1220, 3208, 2063, 848, 1748, 3092, 1494, 2337, 2402, 2415, 307, 436, 1077, 1590, 2363}, (91, 0): {3332, 1738, 1238, 1374, 1377, 2531, 2235, 115, 2618, 2427}, (1330, 0): {405, 2823, 846}, (1324, 0): {1730, 2728, 1394, 343, 2780}, (131, 0): {2823, 2064, 405, 1238, 343, 218, 987, 1757, 288, 160, 2657, 2976, 2678, 314, 2811}, (2080, 0): {130, 2916, 1157, 1807, 3092, 343}, (3241, 0): {256, 2823, 405, 2458, 3098, 2976, 2728, 2219, 314, 1238, 343, 218, 736, 2657, 2917, 997, 2535, 2549, 2678, 2811, 1279}, (965, 0): {2823, 405, 1238, 343, 218, 2976, 2657, 2535, 2219, 2678, 314, 2811}, (2091, 0): {1024, 1542, 1543, 1238, 343, 2657, 3169, 2597, 1714, 1845, 1525, 955, 2751}, (2982, 0): set(), (3195, 0): set(), (618, 0): {1281, 197, 2064, 662, 1238, 2587, 160, 2657, 995, 2853, 422, 362, 2678, 2745, 1149}, (1451, 0): {160, 138}, (2129, 0): {1930, 519}, (2721, 0): {519, 1930, 343}, (3339, 0): {1934, 2574, 657, 3092, 1584, 3122, 819, 1588, 694, 2745, 2876, 2112, 843, 1235, 2005, 1238, 1888, 3191, 2168, 638}, (1654, 0): set(), (576, 0): {160, 2772, 2745, 1178, 2237}, (1570, 0): {1536, 1193, 2028, 2317, 246, 2618}, (409, 0): set(), (114, 0): {33, 1955, 605, 624, 2196, 1300, 571, 2749}, (1722, 0): {3138, 1734, 394, 846, 1238, 863, 418, 1588, 1143, 3128, 2745, 1850, 1339}, (3238, 0): set(), (1779, 0): {2192, 1939, 3092, 726, 1751, 1886, 544, 2418, 1140, 181, 506}, (1963, 0): {1, 2337, 707, 2273, 490, 1748, 3092, 127}, (1996, 0): set(), (3284, 0): {3077, 3207, 3145, 3286, 2654}, (27, 0): {3077, 3207, 3145, 3286, 2654}, (433, 0): set(), (2693, 0): {267, 3087, 3092, 2974, 1578, 1454, 2356, 1595, 3004, 3135, 1748, 213, 1494, 608, 1504, 1383, 876, 3180, 494, 2424}, (1269, 0): set(), (1362, 0): {160, 1683, 1813, 343, 2906, 3199}, (3235, 0): {1536, 1412, 2918, 314, 1006, 399, 846, 2287, 2906}, (2385, 0): {918, 343, 1583}, (1632, 0): {1280, 1861, 2824, 2633, 778, 653, 400, 3092, 1245, 100, 2022, 1128, 2415, 2418, 314, 2237, 2686}, (2069, 0): {1714, 2772, 661, 2745, 2876}, (2959, 0): set(), (1284, 0): {1665, 2823, 1299, 2906, 160, 2160, 3312, 2676, 1268, 314}, (3214, 0): {160, 2657, 100, 1912, 2678, 1720}, (2194, 0): set(), (1141, 0): {2886, 1744, 343, 1178, 2458, 3098, 1757, 160, 997, 1394, 1778, 1279}, (1205, 0): {417, 1543, 2280, 2440, 1714, 1238, 2263, 3004, 605}, (3193, 0): set(), (1386, 0): {2436, 3145, 2089, 181, 1238, 3001, 2618, 735}, (1070, 0): {1283, 653, 400, 1169, 3092, 160, 2208, 928, 301, 2248, 1998, 1363, 2906, 2655, 97, 3043, 100, 2022, 1260, 2418, 499, 2043}, (689, 0): {1094, 1161, 2316, 207, 3092, 2524, 2918, 1326, 2287, 3128, 2745}, (2871, 0): {160, 3342, 2192, 1397, 2166, 343, 283, 2047}, (2372, 0): set(), (2781, 0): {160, 1607, 2192, 3092, 1397, 2166, 343, 283, 2047}, (1753, 0): {160, 103, 1607, 2192, 3092, 1397, 2166, 283, 2047}, (962, 0): set(), (1980, 0): {3140, 141, 3024, 3092, 866, 100, 2022, 2472, 2418, 181}, (1922, 0): {160, 2657, 362, 2811}}\n"
          ]
        }
      ],
      "source": [
        "mapped_alt_tails = {}\n",
        "\n",
        "for index, row in mapped_iric.iterrows():\n",
        "    key = (row['mapped_subject'], row['mapped_predicate'])\n",
        "    if key not in mapped_alt_tails:\n",
        "        mapped_alt_tails[key] = set()\n",
        "    mapped_alt_tails[key].update(set(row['mapped_alt_tails']))\n",
        "\n",
        "print(mapped_alt_tails)\n",
        "\n",
        "for key, value in mapped_alt_tails.items():\n",
        "    mapped_alt_tails[key]=np.array(list(value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datatsets look OK ? (val, train, test) : True True True\n",
            "Data(edge_index=[2, 8001], edge_attr=[8001], num_nodes=3343, edge_label=[2000], edge_label_index=[2, 2000])\n",
            "Data(edge_index=[2, 9001], edge_attr=[9001], num_nodes=3343, edge_label=[2000], edge_label_index=[2, 2000])\n",
            "Data(edge_index=[2, 8001], edge_attr=[8001], num_nodes=3343, edge_label=[8001], edge_label_index=[2, 8001])\n"
          ]
        }
      ],
      "source": [
        "val_data = torch.load(val_path)\n",
        "test_data = torch.load(test_path)\n",
        "train_data = torch.load(train_path)\n",
        "\n",
        "print(\"Datatsets look OK ? (val, train, test) :\",\n",
        "val_data.validate(),\n",
        "test_data.validate(),\n",
        "train_data.validate())\n",
        "\n",
        "print(val_data)\n",
        "print(test_data)\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of heads in dataset  : 1312\n",
            "difference heads - subjects : 0\n",
            "difference heads - object   : 1312\n",
            "Numer of tails in dataset   : 1769\n",
            "Difference tails - subjects : 1769\n",
            "Differece tails - objects   : 0\n"
          ]
        }
      ],
      "source": [
        "subs = set(list(mapped_iric['mapped_subject']))\n",
        "objs = set(list(mapped_iric['mapped_object']))\n",
        "\n",
        "heads = set(train_data.edge_index[0].tolist())\n",
        "tails = set(train_data.edge_index[1].tolist())\n",
        "\n",
        "print('Number of heads in dataset  :', len(heads))\n",
        "print('difference heads - subjects :', len(heads-subs))\n",
        "print('difference heads - object   :', len(heads-objs))\n",
        "\n",
        "print('Numer of tails in dataset   :', len(tails))\n",
        "print('Difference tails - subjects :', len(tails-subs))\n",
        "print('Differece tails - objects   :', len(tails-objs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " tensor([2227,  902, 3106,  ..., 2998,  543, 2821]) \n",
            " tensor([0, 0, 0,  ..., 0, 0, 0]) \n",
            " tensor([2823, 1806, 2353,  ..., 3215, 2728, 1394])\n"
          ]
        }
      ],
      "source": [
        "# Here I create a batch on wich i will test my losses. I need a loader and a model to create it.\n",
        "\n",
        "complex_model = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "loader = complex_model.loader(\n",
        "    head_index = train_data.edge_index[0],\n",
        "    tail_index = train_data.edge_index[1],\n",
        "    rel_type = train_data.edge_attr,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,)\n",
        "\n",
        "batchy = next(iter(loader))\n",
        "hi, rt, ti = batchy[0], batchy[1], batchy[2]\n",
        "print('\\n',hi,'\\n',rt,'\\n', ti)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining LinLoss() functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<nxontology.ontology.NXOntology object at 0x7f8a35412520>\n"
          ]
        }
      ],
      "source": [
        "nxo = from_file('/home/elliot/Documents/ESL2024/data/go-basic.json.gz')\n",
        "nxo.freeze()\n",
        "print(nxo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lin_sim_on_mapped_terms(mapped_term1, mapped_term2):\n",
        "    term1 = map_to_GO[mapped_term1]\n",
        "    term2 = map_to_GO[mapped_term2]\n",
        "    if (term1 in nxo.graph._node and term2 in nxo.graph._node):\n",
        "        sim = nxo.similarity(term1, term2).lin \n",
        "        return sim\n",
        "    else:\n",
        "        return 0  \n",
        "\n",
        "def best_lim_sim_for_triple(head, rel, tail)-> torch.Tensor:\n",
        "    max_lin_sim=0\n",
        "    for alt_tail in mapped_alt_tails[(head, rel)]:\n",
        "\n",
        "        if (map_to_GO[tail] in nxo.graph._node\n",
        "            and\n",
        "            map_to_GO[alt_tail] in nxo.graph._node):\n",
        "                \n",
        "                sim = nxo.similarity(map_to_GO[tail], map_to_GO[alt_tail]).lin # Pourrait être amélioré : actuellement, on calcule plein de similarités différentes.\n",
        "                \n",
        "                if max_lin_sim < sim < 1:\n",
        "                    max_lin_sim = sim\n",
        "    \n",
        "    return max_lin_sim\n",
        "\n",
        "\n",
        "def best_lin_sims_for_batch(head_index:torch.Tensor, rel_type:torch.Tensor, tail_index:torch.Tensor):\n",
        "\n",
        "    batch = pd.DataFrame(torch.transpose(torch.stack((head_index,rel_type,tail_index)),\n",
        "                                         0,1)\n",
        "                        )\n",
        "    \n",
        "    return torch.Tensor(batch.apply(lambda row : best_lim_sim_for_triple(head=row[0],\n",
        "                                                                         rel=row[1],\n",
        "                                                                         tail=row[2]),\n",
        "                                    axis=1))\n",
        "\n",
        "def lin_sims_for_batch(term1: torch.Tensor, term2: torch.Tensor)->torch.Tensor:\n",
        "\n",
        "    batch = pd.DataFrame(torch.transpose(torch.stack((term1, term2)),\n",
        "                                         0,1)\n",
        "                        )\n",
        "    \n",
        "    display(batch)\n",
        "    \n",
        "    return torch.Tensor(batch.apply(lambda row : best_lim_sim_for_triple(head=row[0],\n",
        "                                                                         rel=row[1],\n",
        "                                                                         tail=row[2]),\n",
        "                                    axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5],\n",
              "        [6, 7, 8, 9, 0, 1]])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def shuffle_tensor(t: torch.Tensor):\n",
        "    '''\n",
        "    Shuffles elments of a tensor.\n",
        "    WARNING :\n",
        "    shuffle_tensor(torch.tensor([[0,1,2,3,4,5],[6,7,8,9,0,1]]))\n",
        "    returns :\n",
        "    tensor([[0, 1, 2, 3, 4, 5],  OR tensor([[6, 7, 8, 9, 0, 1],\n",
        "            [6, 7, 8, 9, 0, 1]])            [0, 1, 2, 3, 4, 5]])\n",
        "    '''\n",
        "    idx = torch.randperm(t.shape[0])\n",
        "    return t[idx].view(t.size())\n",
        "\n",
        "shuffle_tensor(torch.tensor([[0,1,2,3,4,5],[6,7,8,9,0,1]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Heads are not touched :\n",
            "tensor([2227,  902, 3106,  ..., 2998,  543, 2821])\n",
            "tensor([2227,  902, 3106,  ..., 2998,  543, 2821])\n",
            "\n",
            "But tails are :\n",
            "tensor([2823, 1806, 2353,  ..., 3215, 2728, 1394])\n",
            "tensor([2524,  997,  160,  ...,  863, 1288, 2309])\n",
            "Shuffle ok : True\n"
          ]
        }
      ],
      "source": [
        "class tail_only_ComplEx(ComplEx):\n",
        "\n",
        "    '''\n",
        "    Overwritting random_sample() to make negative triples by setting a random tail to each triple,\n",
        "    instead of setting a random head or tail.\n",
        "    '''\n",
        "    @torch.no_grad()\n",
        "    def random_sample(\n",
        "        self,\n",
        "        head_index: torch.Tensor,\n",
        "        rel_type: torch.Tensor,\n",
        "        tail_index: torch.Tensor,\n",
        "        ) -> torch.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        Randomly samples negative triplets by replacing the tail.\n",
        "        Args:\n",
        "            head_index (torch.Tensor): The head indices.\n",
        "            rel_type (torch.Tensor): The relation type.\n",
        "            tail_index (torch.Tensor): The tail indices.\n",
        "        \"\"\"\n",
        "\n",
        "        tail_index = shuffle_tensor(tail_index.clone())\n",
        "\n",
        "        return head_index, rel_type, tail_index\n",
        "\n",
        "to = tail_only_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Heads are not touched :\")\n",
        "print(batchy[0])\n",
        "print(to.random_sample(*batchy)[0])\n",
        "\n",
        "print(\"\\nBut tails are :\")\n",
        "print(batchy[2])\n",
        "print(to.random_sample(*batchy)[2])\n",
        "\n",
        "def tens_to_set(t):\n",
        "    return set(t.tolist())\n",
        "\n",
        "t, f = batchy[2], to.random_sample(*batchy)[2]\n",
        "st, sf = tens_to_set(t), tens_to_set(f)\n",
        "print('Shuffle ok :', not bool(len(st - sf)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinSim_ComplEx(tail_only_ComplEx):\n",
        "  def loss(\n",
        "            self,\n",
        "            head_index: torch.Tensor,\n",
        "            rel_type: torch.Tensor,\n",
        "            tail_index: torch.Tensor,\n",
        "            ) -> torch.Tensor:\n",
        "            \n",
        "        '''\n",
        "        tail_only_ComplEx.loss() modified to account a LinSim term : one simply withdraw mean(similarities(batch)) to the loss.\n",
        "        '''\n",
        "\n",
        "        pos = head_index, rel_type, tail_index\n",
        "\n",
        "        false_head_index, false_rel_type, false_tail_index = self.random_sample(head_index, rel_type, tail_index)\n",
        "        neg = false_head_index, false_rel_type, false_tail_index\n",
        "\n",
        "        pos_score = self(*pos)\n",
        "        neg_score = self(*neg)\n",
        "        scores = torch.cat([pos_score, neg_score], dim=0)\n",
        "\n",
        "        pos_target = torch.ones_like(pos_score) \n",
        "        neg_target = torch.zeros_like(neg_score)\n",
        "        target = torch.cat([pos_target, neg_target], dim=0)\n",
        "\n",
        "        # stacking true and falses tails in df :\n",
        "        pos_and_neg_tails = pd.DataFrame(torch.stack((tail_index,false_tail_index)).transpose(0,1)).astype(\"int\")\n",
        "\n",
        "        # Calculating LinSim(positive_head, negative_head) : \n",
        "        similarities = torch.tensor(pos_and_neg_tails.apply(lambda row : lin_sim_on_mapped_terms(row[0], row[1]),\n",
        "                                                      axis = 1).values\n",
        "                                    )\n",
        "\n",
        "\n",
        "        return F.binary_cross_entropy_with_logits(scores, target) - torch.mean(similarities)\n",
        "  \n",
        "class best_LinSim_ComplEx(tail_only_ComplEx):\n",
        "  def loss(\n",
        "            self,\n",
        "            head_index: torch.Tensor,\n",
        "            rel_type: torch.Tensor,\n",
        "            tail_index: torch.Tensor,\n",
        "            ) -> torch.Tensor:\n",
        "            \n",
        "        '''\n",
        "        tail_only_ComplEx.loss() modified to account a LinSim term :\n",
        "        one withdraw the mean(bests similarities between each false tail of a triple to its possible tails) to the loss.\n",
        "        '''\n",
        "\n",
        "        pos = head_index, rel_type, tail_index\n",
        "\n",
        "        false_head_index, false_rel_type, false_tail_index = self.random_sample(head_index, rel_type, tail_index)\n",
        "        neg = false_head_index, false_rel_type, false_tail_index\n",
        "\n",
        "        pos_score = self(*pos)\n",
        "        neg_score = self(*neg)\n",
        "        scores = torch.cat([pos_score, neg_score], dim=0)\n",
        "\n",
        "        pos_target = torch.ones_like(pos_score) \n",
        "        neg_target = torch.zeros_like(neg_score)\n",
        "        target = torch.cat([pos_target, neg_target], dim=0)\n",
        "\n",
        "        # Calculating LinSim(positive_head, negative_head) : \n",
        "        similarities = best_lin_sims_for_batch(head_index, rel_type, false_tail_index)\n",
        "\n",
        "        return F.binary_cross_entropy_with_logits(scores, target) - torch.mean(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getBack(var_grad_fn):\n",
        "    print(var_grad_fn)\n",
        "    for n in var_grad_fn.next_functions:\n",
        "        if n[0]:\n",
        "            try:\n",
        "                tensor = getattr(n[0], 'variable')\n",
        "                print(n[0])\n",
        "                print('Tensor with grad found:\\n', tensor)\n",
        "                print(' - gradient:\\n', tensor.grad)\n",
        "                print()\n",
        "            except AttributeError as e:\n",
        "                getBack(n[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([2227,  902, 3106,  ..., 2998,  543, 2821]),\n",
              " tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
              " tensor([2823, 1806, 2353,  ..., 3215, 2728, 1394]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6931)\n",
            "Tracing back tensors:\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTracing back tensors:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m getBack(loss\u001b[38;5;241m.\u001b[39mgrad_fn)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "c = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "loss = c.loss(*batchy)\n",
        "print(loss)\n",
        "\n",
        "print('Tracing back tensors:')\n",
        "loss.backward()\n",
        "getBack(loss.grad_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.3401, grad_fn=<SubBackward0>)\n",
            "Tracing back tensors:\n",
            "<SubBackward0 object at 0x7f8a4c0cd070>\n",
            "<BinaryCrossEntropyWithLogitsBackward0 object at 0x7f8a8105cdf0>\n",
            "<CatBackward0 object at 0x7f8a82252eb0>\n",
            "<SubBackward0 object at 0x7f8aa7ae4880>\n",
            "<AddBackward0 object at 0x7f8b782bef70>\n",
            "<AddBackward0 object at 0x7f8b782be3d0>\n",
            "<SumBackward1 object at 0x7f8a35405790>\n",
            "<MulBackward0 object at 0x7f8a35405100>\n",
            "<MulBackward0 object at 0x7f8a354059d0>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405130>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405940>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0120, -0.0187, -0.0064, -0.0191, -0.0127],\n",
            "        [-0.0270, -0.0273, -0.0145, -0.0025, -0.0074],\n",
            "        [-0.0081,  0.0010, -0.0236,  0.0158, -0.0103],\n",
            "        ...,\n",
            "        [-0.0086, -0.0017,  0.0068,  0.0244,  0.0031],\n",
            "        [-0.0167, -0.0034, -0.0122,  0.0126, -0.0240],\n",
            "        [ 0.0266, -0.0127,  0.0065, -0.0262,  0.0264]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 5.5183e-06,  5.0796e-06,  3.2016e-06, -5.3822e-06,  1.2247e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405dc0>\n",
            "<AccumulateGrad object at 0x7f8a35405940>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a35405cd0>\n",
            "<MulBackward0 object at 0x7f8a35405100>\n",
            "<MulBackward0 object at 0x7f8a35405dc0>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405940>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a354053d0>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0120, -0.0187, -0.0064, -0.0191, -0.0127],\n",
            "        [-0.0270, -0.0273, -0.0145, -0.0025, -0.0074],\n",
            "        [-0.0081,  0.0010, -0.0236,  0.0158, -0.0103],\n",
            "        ...,\n",
            "        [-0.0086, -0.0017,  0.0068,  0.0244,  0.0031],\n",
            "        [-0.0167, -0.0034, -0.0122,  0.0126, -0.0240],\n",
            "        [ 0.0266, -0.0127,  0.0065, -0.0262,  0.0264]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 5.5183e-06,  5.0796e-06,  3.2016e-06, -5.3822e-06,  1.2247e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405880>\n",
            "<AccumulateGrad object at 0x7f8a354053d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a35405490>\n",
            "<MulBackward0 object at 0x7f8a35405cd0>\n",
            "<MulBackward0 object at 0x7f8a35405790>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405880>\n",
            "<AccumulateGrad object at 0x7f8a354053d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a354059d0>\n",
            "<AccumulateGrad object at 0x7f8a354053d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-1.1010e-02, -1.8848e-02, -2.0941e-02,  8.0526e-05, -1.6640e-02],\n",
            "        [-1.9588e-02, -2.4997e-02, -8.6210e-03, -2.2658e-02,  7.4522e-03],\n",
            "        [-1.8759e-02, -2.7154e-02,  1.8969e-02,  1.6867e-02, -4.2026e-03],\n",
            "        ...,\n",
            "        [-1.6773e-03,  4.3790e-04,  1.3864e-02, -2.0909e-02, -2.6024e-02],\n",
            "        [-1.7102e-02, -1.1849e-03,  1.1409e-02, -1.9107e-02,  2.3961e-02],\n",
            "        [-5.9466e-03, -1.2302e-02, -1.3358e-02, -1.0460e-02,  2.6607e-02]],\n",
            "       requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-2.9020e-06, -5.3758e-06,  1.5101e-06, -1.2572e-06, -6.2141e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405100>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8b782befd0>\n",
            "<MulBackward0 object at 0x7f8b782be3d0>\n",
            "<MulBackward0 object at 0x7f8a35405310>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405100>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405fa0>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-1.1010e-02, -1.8848e-02, -2.0941e-02,  8.0526e-05, -1.6640e-02],\n",
            "        [-1.9588e-02, -2.4997e-02, -8.6210e-03, -2.2658e-02,  7.4522e-03],\n",
            "        [-1.8759e-02, -2.7154e-02,  1.8969e-02,  1.6867e-02, -4.2026e-03],\n",
            "        ...,\n",
            "        [-1.6773e-03,  4.3790e-04,  1.3864e-02, -2.0909e-02, -2.6024e-02],\n",
            "        [-1.7102e-02, -1.1849e-03,  1.1409e-02, -1.9107e-02,  2.3961e-02],\n",
            "        [-5.9466e-03, -1.2302e-02, -1.3358e-02, -1.0460e-02,  2.6607e-02]],\n",
            "       requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-2.9020e-06, -5.3758e-06,  1.5101e-06, -1.2572e-06, -6.2141e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405cd0>\n",
            "<AccumulateGrad object at 0x7f8a35405fa0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<SubBackward0 object at 0x7f8b782beeb0>\n",
            "<AddBackward0 object at 0x7f8b782befd0>\n",
            "<AddBackward0 object at 0x7f8b782be3d0>\n",
            "<SumBackward1 object at 0x7f8a35405400>\n",
            "<MulBackward0 object at 0x7f8a35405790>\n",
            "<MulBackward0 object at 0x7f8a354059d0>\n",
            "<EmbeddingBackward0 object at 0x7f8a354053d0>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405940>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0120, -0.0187, -0.0064, -0.0191, -0.0127],\n",
            "        [-0.0270, -0.0273, -0.0145, -0.0025, -0.0074],\n",
            "        [-0.0081,  0.0010, -0.0236,  0.0158, -0.0103],\n",
            "        ...,\n",
            "        [-0.0086, -0.0017,  0.0068,  0.0244,  0.0031],\n",
            "        [-0.0167, -0.0034, -0.0122,  0.0126, -0.0240],\n",
            "        [ 0.0266, -0.0127,  0.0065, -0.0262,  0.0264]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 5.5183e-06,  5.0796e-06,  3.2016e-06, -5.3822e-06,  1.2247e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405dc0>\n",
            "<AccumulateGrad object at 0x7f8a35405940>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a35405310>\n",
            "<MulBackward0 object at 0x7f8a35405790>\n",
            "<MulBackward0 object at 0x7f8a35405dc0>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405940>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405130>\n",
            "<AccumulateGrad object at 0x7f8a39c24190>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0120, -0.0187, -0.0064, -0.0191, -0.0127],\n",
            "        [-0.0270, -0.0273, -0.0145, -0.0025, -0.0074],\n",
            "        [-0.0081,  0.0010, -0.0236,  0.0158, -0.0103],\n",
            "        ...,\n",
            "        [-0.0086, -0.0017,  0.0068,  0.0244,  0.0031],\n",
            "        [-0.0167, -0.0034, -0.0122,  0.0126, -0.0240],\n",
            "        [ 0.0266, -0.0127,  0.0065, -0.0262,  0.0264]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 5.5183e-06,  5.0796e-06,  3.2016e-06, -5.3822e-06,  1.2247e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405880>\n",
            "<AccumulateGrad object at 0x7f8a35405130>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a35405490>\n",
            "<MulBackward0 object at 0x7f8a35405310>\n",
            "<MulBackward0 object at 0x7f8a35405400>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405880>\n",
            "<AccumulateGrad object at 0x7f8a35405130>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a354059d0>\n",
            "<AccumulateGrad object at 0x7f8a35405130>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-1.1010e-02, -1.8848e-02, -2.0941e-02,  8.0526e-05, -1.6640e-02],\n",
            "        [-1.9588e-02, -2.4997e-02, -8.6210e-03, -2.2658e-02,  7.4522e-03],\n",
            "        [-1.8759e-02, -2.7154e-02,  1.8969e-02,  1.6867e-02, -4.2026e-03],\n",
            "        ...,\n",
            "        [-1.6773e-03,  4.3790e-04,  1.3864e-02, -2.0909e-02, -2.6024e-02],\n",
            "        [-1.7102e-02, -1.1849e-03,  1.1409e-02, -1.9107e-02,  2.3961e-02],\n",
            "        [-5.9466e-03, -1.2302e-02, -1.3358e-02, -1.0460e-02,  2.6607e-02]],\n",
            "       requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-2.9020e-06, -5.3758e-06,  1.5101e-06, -1.2572e-06, -6.2141e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405790>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8b782beca0>\n",
            "<MulBackward0 object at 0x7f8b782be3d0>\n",
            "<MulBackward0 object at 0x7f8a35405cd0>\n",
            "<EmbeddingBackward0 object at 0x7f8a35405790>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0378,  0.0046,  0.0326,  0.0086,  0.0012],\n",
            "        [ 0.0175,  0.0317,  0.0164, -0.0194, -0.0031],\n",
            "        [-0.0153, -0.0413, -0.0396, -0.0101,  0.0329],\n",
            "        ...,\n",
            "        [ 0.0052,  0.0156, -0.0249, -0.0352,  0.0039],\n",
            "        [-0.0024, -0.0192, -0.0100,  0.0288, -0.0158],\n",
            "        [ 0.0334,  0.0185, -0.0240, -0.0107, -0.0122]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.8109e-08, -6.5762e-08,  2.0529e-08,  3.1927e-08,  8.0659e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.6501e-08, -3.5314e-08,  2.3345e-07, -1.1869e-07,  5.3848e-08]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405100>\n",
            "<AccumulateGrad object at 0x7f8a354059d0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-1.1010e-02, -1.8848e-02, -2.0941e-02,  8.0526e-05, -1.6640e-02],\n",
            "        [-1.9588e-02, -2.4997e-02, -8.6210e-03, -2.2658e-02,  7.4522e-03],\n",
            "        [-1.8759e-02, -2.7154e-02,  1.8969e-02,  1.6867e-02, -4.2026e-03],\n",
            "        ...,\n",
            "        [-1.6773e-03,  4.3790e-04,  1.3864e-02, -2.0909e-02, -2.6024e-02],\n",
            "        [-1.7102e-02, -1.1849e-03,  1.1409e-02, -1.9107e-02,  2.3961e-02],\n",
            "        [-5.9466e-03, -1.2302e-02, -1.3358e-02, -1.0460e-02,  2.6607e-02]],\n",
            "       requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-2.9020e-06, -5.3758e-06,  1.5101e-06, -1.2572e-06, -6.2141e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a35405310>\n",
            "<AccumulateGrad object at 0x7f8a35405100>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0163, -0.0224, -0.0084, -0.0395,  0.0165],\n",
            "        [ 0.0298, -0.0268, -0.0156, -0.0270, -0.0048],\n",
            "        [ 0.0292,  0.0388, -0.0003,  0.0113,  0.0225],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0179,  0.0037, -0.0202,  0.0255],\n",
            "        [ 0.0111, -0.0351,  0.0092,  0.0308,  0.0270],\n",
            "        [-0.0116, -0.0379,  0.0056,  0.0030,  0.0159]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.7374e-08,  1.3321e-07, -4.7780e-08, -8.7954e-08, -4.5127e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0716e-07,  1.8001e-07,  7.6697e-08,  7.1309e-08,  2.2606e-07]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "blc = best_LinSim_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "loss = blc.loss(*batchy)\n",
        "print(loss)\n",
        "\n",
        "print('Tracing back tensors:')\n",
        "loss.backward()\n",
        "getBack(loss.grad_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.5985, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "Tracing back tensors:\n",
            "<SubBackward0 object at 0x7f8b7bb44340>\n",
            "<BinaryCrossEntropyWithLogitsBackward0 object at 0x7f8aa7ad0af0>\n",
            "<CatBackward0 object at 0x7f8aa7ad0a30>\n",
            "<SubBackward0 object at 0x7f8aa7ad0cd0>\n",
            "<AddBackward0 object at 0x7f8a39c246d0>\n",
            "<AddBackward0 object at 0x7f8a39c24550>\n",
            "<SumBackward1 object at 0x7f8a39c243d0>\n",
            "<MulBackward0 object at 0x7f8a39c245b0>\n",
            "<MulBackward0 object at 0x7f8a39c24610>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24730>\n",
            "<AccumulateGrad object at 0x7f8a39c24b20>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24760>\n",
            "<AccumulateGrad object at 0x7f8a39c24b20>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0100, -0.0191,  0.0218,  0.0036,  0.0243],\n",
            "        [ 0.0080,  0.0086, -0.0116, -0.0103, -0.0148],\n",
            "        [ 0.0262, -0.0135,  0.0117,  0.0120,  0.0222],\n",
            "        ...,\n",
            "        [ 0.0107, -0.0200,  0.0263,  0.0186, -0.0089],\n",
            "        [-0.0273, -0.0112, -0.0100,  0.0121, -0.0155],\n",
            "        [ 0.0141,  0.0217,  0.0241,  0.0006,  0.0195]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.4548e-06,  3.8433e-06, -1.9752e-06,  5.0051e-06,  2.1102e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24460>\n",
            "<AccumulateGrad object at 0x7f8a39c24760>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c243a0>\n",
            "<MulBackward0 object at 0x7f8a39c245b0>\n",
            "<MulBackward0 object at 0x7f8a39c24460>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24760>\n",
            "<AccumulateGrad object at 0x7f8a39c24b20>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24790>\n",
            "<AccumulateGrad object at 0x7f8a39c24b20>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0100, -0.0191,  0.0218,  0.0036,  0.0243],\n",
            "        [ 0.0080,  0.0086, -0.0116, -0.0103, -0.0148],\n",
            "        [ 0.0262, -0.0135,  0.0117,  0.0120,  0.0222],\n",
            "        ...,\n",
            "        [ 0.0107, -0.0200,  0.0263,  0.0186, -0.0089],\n",
            "        [-0.0273, -0.0112, -0.0100,  0.0121, -0.0155],\n",
            "        [ 0.0141,  0.0217,  0.0241,  0.0006,  0.0195]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.4548e-06,  3.8433e-06, -1.9752e-06,  5.0051e-06,  2.1102e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24580>\n",
            "<AccumulateGrad object at 0x7f8a39c24790>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c244c0>\n",
            "<MulBackward0 object at 0x7f8a39c243a0>\n",
            "<MulBackward0 object at 0x7f8a39c243d0>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24580>\n",
            "<AccumulateGrad object at 0x7f8a39c24790>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24610>\n",
            "<AccumulateGrad object at 0x7f8a39c24790>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0243,  0.0065, -0.0152, -0.0067,  0.0189],\n",
            "        [-0.0148,  0.0083,  0.0132,  0.0055,  0.0050],\n",
            "        [ 0.0149,  0.0003,  0.0115, -0.0182,  0.0219],\n",
            "        ...,\n",
            "        [-0.0262, -0.0093, -0.0219,  0.0246, -0.0145],\n",
            "        [ 0.0143, -0.0270,  0.0249,  0.0236, -0.0083],\n",
            "        [-0.0194, -0.0206,  0.0152,  0.0208, -0.0052]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.7870e-06,  2.5754e-06,  2.3927e-06,  7.6760e-06,  3.8672e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c245b0>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c246a0>\n",
            "<MulBackward0 object at 0x7f8a39c244c0>\n",
            "<MulBackward0 object at 0x7f8a39c24550>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c245b0>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24640>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0243,  0.0065, -0.0152, -0.0067,  0.0189],\n",
            "        [-0.0148,  0.0083,  0.0132,  0.0055,  0.0050],\n",
            "        [ 0.0149,  0.0003,  0.0115, -0.0182,  0.0219],\n",
            "        ...,\n",
            "        [-0.0262, -0.0093, -0.0219,  0.0246, -0.0145],\n",
            "        [ 0.0143, -0.0270,  0.0249,  0.0236, -0.0083],\n",
            "        [-0.0194, -0.0206,  0.0152,  0.0208, -0.0052]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.7870e-06,  2.5754e-06,  2.3927e-06,  7.6760e-06,  3.8672e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c243a0>\n",
            "<AccumulateGrad object at 0x7f8a39c24640>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<SubBackward0 object at 0x7f8aa7ad0940>\n",
            "<AddBackward0 object at 0x7f8a39c246a0>\n",
            "<AddBackward0 object at 0x7f8a39c244c0>\n",
            "<SumBackward1 object at 0x7f8a39c245e0>\n",
            "<MulBackward0 object at 0x7f8a39c243d0>\n",
            "<MulBackward0 object at 0x7f8a39c24610>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24790>\n",
            "<AccumulateGrad object at 0x7f8a39c24760>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24b20>\n",
            "<AccumulateGrad object at 0x7f8a39c24760>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0100, -0.0191,  0.0218,  0.0036,  0.0243],\n",
            "        [ 0.0080,  0.0086, -0.0116, -0.0103, -0.0148],\n",
            "        [ 0.0262, -0.0135,  0.0117,  0.0120,  0.0222],\n",
            "        ...,\n",
            "        [ 0.0107, -0.0200,  0.0263,  0.0186, -0.0089],\n",
            "        [-0.0273, -0.0112, -0.0100,  0.0121, -0.0155],\n",
            "        [ 0.0141,  0.0217,  0.0241,  0.0006,  0.0195]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.4548e-06,  3.8433e-06, -1.9752e-06,  5.0051e-06,  2.1102e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24460>\n",
            "<AccumulateGrad object at 0x7f8a39c24b20>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c24550>\n",
            "<MulBackward0 object at 0x7f8a39c243d0>\n",
            "<MulBackward0 object at 0x7f8a39c24460>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24b20>\n",
            "<AccumulateGrad object at 0x7f8a39c24760>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24bb0>\n",
            "<AccumulateGrad object at 0x7f8a39c24760>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0100, -0.0191,  0.0218,  0.0036,  0.0243],\n",
            "        [ 0.0080,  0.0086, -0.0116, -0.0103, -0.0148],\n",
            "        [ 0.0262, -0.0135,  0.0117,  0.0120,  0.0222],\n",
            "        ...,\n",
            "        [ 0.0107, -0.0200,  0.0263,  0.0186, -0.0089],\n",
            "        [-0.0273, -0.0112, -0.0100,  0.0121, -0.0155],\n",
            "        [ 0.0141,  0.0217,  0.0241,  0.0006,  0.0195]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.4548e-06,  3.8433e-06, -1.9752e-06,  5.0051e-06,  2.1102e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24580>\n",
            "<AccumulateGrad object at 0x7f8a39c24bb0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c24400>\n",
            "<MulBackward0 object at 0x7f8a39c24550>\n",
            "<MulBackward0 object at 0x7f8a39c245e0>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24580>\n",
            "<AccumulateGrad object at 0x7f8a39c24bb0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24610>\n",
            "<AccumulateGrad object at 0x7f8a39c24bb0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0243,  0.0065, -0.0152, -0.0067,  0.0189],\n",
            "        [-0.0148,  0.0083,  0.0132,  0.0055,  0.0050],\n",
            "        [ 0.0149,  0.0003,  0.0115, -0.0182,  0.0219],\n",
            "        ...,\n",
            "        [-0.0262, -0.0093, -0.0219,  0.0246, -0.0145],\n",
            "        [ 0.0143, -0.0270,  0.0249,  0.0236, -0.0083],\n",
            "        [-0.0194, -0.0206,  0.0152,  0.0208, -0.0052]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.7870e-06,  2.5754e-06,  2.3927e-06,  7.6760e-06,  3.8672e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c243d0>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<SumBackward1 object at 0x7f8a39c24670>\n",
            "<MulBackward0 object at 0x7f8a39c24400>\n",
            "<MulBackward0 object at 0x7f8a39c244c0>\n",
            "<EmbeddingBackward0 object at 0x7f8a39c243d0>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0091, -0.0172, -0.0129, -0.0319,  0.0385],\n",
            "        [-0.0213,  0.0040, -0.0029, -0.0149,  0.0278],\n",
            "        [-0.0034,  0.0179,  0.0190,  0.0140,  0.0063],\n",
            "        ...,\n",
            "        [ 0.0403, -0.0249, -0.0154, -0.0214,  0.0228],\n",
            "        [-0.0274, -0.0194,  0.0323, -0.0284, -0.0170],\n",
            "        [-0.0084,  0.0054, -0.0186, -0.0037, -0.0230]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 3.5252e-08, -2.4717e-08,  1.3003e-07,  9.1904e-09, -2.9464e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3908e-08,  1.4469e-08, -2.2340e-07, -3.6981e-08, -3.2992e-07]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c245b0>\n",
            "<AccumulateGrad object at 0x7f8a39c24610>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0243,  0.0065, -0.0152, -0.0067,  0.0189],\n",
            "        [-0.0148,  0.0083,  0.0132,  0.0055,  0.0050],\n",
            "        [ 0.0149,  0.0003,  0.0115, -0.0182,  0.0219],\n",
            "        ...,\n",
            "        [-0.0262, -0.0093, -0.0219,  0.0246, -0.0145],\n",
            "        [ 0.0143, -0.0270,  0.0249,  0.0236, -0.0083],\n",
            "        [-0.0194, -0.0206,  0.0152,  0.0208, -0.0052]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[-8.7870e-06,  2.5754e-06,  2.3927e-06,  7.6760e-06,  3.8672e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "\n",
            "<EmbeddingBackward0 object at 0x7f8a39c24550>\n",
            "<AccumulateGrad object at 0x7f8a39c245b0>\n",
            "Tensor with grad found:\n",
            " Parameter containing:\n",
            "tensor([[-0.0071,  0.0253,  0.0166,  0.0281,  0.0310],\n",
            "        [ 0.0306,  0.0367,  0.0304, -0.0285,  0.0047],\n",
            "        [ 0.0310,  0.0292,  0.0267,  0.0079, -0.0229],\n",
            "        ...,\n",
            "        [ 0.0230, -0.0340,  0.0070,  0.0414, -0.0046],\n",
            "        [ 0.0206,  0.0354, -0.0172, -0.0133,  0.0063],\n",
            "        [-0.0079,  0.0200, -0.0147,  0.0316,  0.0062]], requires_grad=True)\n",
            " - gradient:\n",
            " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.1921e-07, -3.0884e-08, -9.6474e-08,  1.3523e-08, -5.4313e-08],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.3594e-07, -2.1797e-07,  8.6514e-08, -1.4892e-07,  3.9426e-07]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lc = LinSim_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "loss = lc.loss(*batchy)\n",
        "print(loss)\n",
        "\n",
        "print('Tracing back tensors:')\n",
        "loss.backward()\n",
        "getBack(loss.grad_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(loader, model, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = total_examples = 0\n",
        "    for head_index, rel_type, tail_index in loader:\n",
        "\n",
        "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(head_index, rel_type, tail_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * head_index.numel()\n",
        "        total_examples += head_index.numel()\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data, model,device):\n",
        "    model.eval()\n",
        "    return model.test(\n",
        "        head_index=data.edge_index[0].to(device),\n",
        "        tail_index=data.edge_index[1].to(device),\n",
        "        rel_type=data.edge_attr.to(device),\n",
        "        batch_size=batch_size,\n",
        "        k=10, #The k in Hit@k\n",
        "    )\n",
        "\n",
        "def get_test_loss(loader, model, device):\n",
        "    model.eval()\n",
        "    total_loss = total_examples = 0\n",
        "    for head_index, rel_type, tail_index in loader:\n",
        "\n",
        "        head_index, rel_type, tail_index = head_index.to(device), rel_type.to(device), tail_index.to(device)\n",
        "        loss = model.loss(head_index, rel_type, tail_index)\n",
        "        total_loss += float(loss) * head_index.numel()\n",
        "        total_examples += head_index.numel()\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "\n",
        "def train_and_test_complex(\n",
        "                           model,\n",
        "                           train_data: torch_geometric.data.data.Data,\n",
        "                           test_data : torch_geometric.data.data.Data,\n",
        "                           xp_name = '',\n",
        "                           epochs: int = 1000, \n",
        "                           eval_period = 500,\n",
        "                           reset_parameters = False, save_params = True,\n",
        "                           use_wandb = False,\n",
        "                           params_save_path = '',\n",
        "                           device = 'cpu',\n",
        "                           dataset_name = 'iric'\n",
        "                           ):\n",
        "    \n",
        "    \n",
        "    # ----------------------------------------------- Reset parameters\n",
        "    if reset_parameters :\n",
        "        model.reset_parameters()\n",
        "\n",
        "    # ----------------------------------------------- Loader\n",
        "    print('Init loader...')\n",
        "    loader = model.loader(\n",
        "                          head_index = train_data.edge_index[0],\n",
        "                          tail_index = train_data.edge_index[1],\n",
        "                          rel_type   = train_data.edge_attr,\n",
        "                          batch_size = batch_size,\n",
        "                          shuffle    = True)\n",
        "    test_loader = model.loader(\n",
        "                          head_index = test_data.edge_index[0],\n",
        "                          tail_index = test_data.edge_index[1],\n",
        "                          rel_type   = test_data.edge_attr,\n",
        "                          batch_size = batch_size,\n",
        "                          shuffle    = True)\n",
        "\n",
        "    # ----------------------------------------------- Optimizer\n",
        "    print('Init optimizer...')\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # ----------------------------------------------- WandB\n",
        "    if use_wandb:\n",
        "        print('Init wandb...')\n",
        "        wandb.init(\n",
        "            settings=wandb.Settings(start_method=\"fork\"),\n",
        "            project=xp_name,\n",
        "            \n",
        "            config={\n",
        "            \"architecture\": str(type(model)),\n",
        "            \"dataset\": dataset_name,\n",
        "            \"epochs\": epochs,\n",
        "            'hidden_channels' : hidden_channels,\n",
        "            'batch_size' : batch_size\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # ----------------------------------------------- Train and eval\n",
        "    print('Train...')\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    for epoch in range(0, epochs+1):\n",
        "\n",
        "        loss = train(loader = loader,\n",
        "                     model  = model,\n",
        "                     optimizer = optimizer,\n",
        "                     device = device)\n",
        "        test_loss = get_test_loss(\n",
        "                            loader = test_loader,\n",
        "                            model  = model,\n",
        "                            device = device)\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "        test_losses.append(test_loss)\n",
        "        \n",
        "\n",
        "        if use_wandb : \n",
        "            wandb.log({\"loss\": loss, \"loss on test\": test_loss})\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Loss on test : {test_loss:.4f}')\n",
        "\n",
        "    # ----------------------------------------------- Periodic Evaluation\n",
        "        if eval_period:\n",
        "            if epoch%eval_period == 0:\n",
        "                print('Test...')\n",
        "                rank, mrr, hits = test(test_data, model=model, device=device)\n",
        "\n",
        "                print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}',\n",
        "                    f'Val MRR: {mrr:.4f}, Val Hits@10: {hits:.4f}')\n",
        "                if wandb:\n",
        "                    wandb.log({\"Val Mean Rank\" : rank,\n",
        "                            \"Val MRR\" : mrr,\n",
        "                            \"hits@10\": hits})\n",
        "\n",
        "    # ----------------------------------------------- End WandB\n",
        "    if use_wandb:\n",
        "        print('End wandb...')\n",
        "        wandb.finish()\n",
        "        print(\"WandB finished.\")\n",
        "\n",
        "    # ----------------------------------------------- Save model\n",
        "    print('Save parameters...')\n",
        "    if save_params:\n",
        "        torch.save(model.state_dict(), params_save_path)\n",
        "        print(\"Model saved at\", params_save_path)\n",
        "\n",
        "    print(\"End\")\n",
        "\n",
        "    return model, train_losses, train_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initiating models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device : cuda, batch size : 4096, epochs : 500, hidden channels : 5\n",
            "DataSets : \n",
            " Data(edge_index=[2, 8001], edge_attr=[8001], num_nodes=3343, edge_label=[8001], edge_label_index=[2, 8001]) \n",
            " Data(edge_index=[2, 9001], edge_attr=[9001], num_nodes=3343, edge_label=[2000], edge_label_index=[2, 2000]) \n",
            " Data(edge_index=[2, 8001], edge_attr=[8001], num_nodes=3343, edge_label=[2000], edge_label_index=[2, 2000])\n",
            "\n",
            "Model types :\n",
            "  -ComplEx ;\n",
            "  -Tail only Complex ;\n",
            "  -ComplEx with LinSim ;\n",
            "  -ComplEx with best possible LinSim\n"
          ]
        }
      ],
      "source": [
        "# TODO Envoyer le calcul sur GPU.\n",
        "# TODO Calculer sur le serveur.\n",
        "\n",
        "print(f\"Device : {device}, batch size : {batch_size}, epochs : {epochs}, hidden channels : {hidden_channels}\")\n",
        "print(\"DataSets :\", '\\n',train_data,'\\n', test_data,'\\n', val_data)\n",
        "\n",
        "print(\"\\nModel types :\\n  -ComplEx ;\\n  -Tail only Complex ;\\n  -ComplEx with LinSim ;\\n  -ComplEx with best possible LinSim\")\n",
        "\n",
        "\n",
        "usual_complex = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = 1,#train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ")\n",
        "\n",
        "to_ComplEx = tail_only_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ")\n",
        "\n",
        "lin_complex = LinSim_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ")\n",
        "\n",
        "best_lin_complex = best_LinSim_ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init loader...\n",
            "Init optimizer...\n",
            "Train...\n",
            "Epoch: 000, Loss: 0.6932, Loss on test : 0.6931\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:03<00:00, 2548.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Val Mean Rank: 1612.94 Val MRR: 0.0027, Val Hits@10: 0.0037\n",
            "Epoch: 001, Loss: 0.6931, Loss on test : 0.6931\n",
            "Epoch: 002, Loss: 0.6931, Loss on test : 0.6931\n",
            "Epoch: 003, Loss: 0.6931, Loss on test : 0.6930\n",
            "Epoch: 004, Loss: 0.6930, Loss on test : 0.6930\n",
            "Epoch: 005, Loss: 0.6930, Loss on test : 0.6930\n",
            "Epoch: 006, Loss: 0.6930, Loss on test : 0.6930\n",
            "Epoch: 007, Loss: 0.6929, Loss on test : 0.6929\n",
            "Epoch: 008, Loss: 0.6929, Loss on test : 0.6929\n",
            "Epoch: 009, Loss: 0.6929, Loss on test : 0.6928\n",
            "Epoch: 010, Loss: 0.6928, Loss on test : 0.6928\n",
            "Epoch: 011, Loss: 0.6928, Loss on test : 0.6928\n",
            "Epoch: 012, Loss: 0.6927, Loss on test : 0.6927\n",
            "Epoch: 013, Loss: 0.6927, Loss on test : 0.6927\n",
            "Epoch: 014, Loss: 0.6926, Loss on test : 0.6926\n",
            "Epoch: 015, Loss: 0.6925, Loss on test : 0.6925\n",
            "Epoch: 016, Loss: 0.6925, Loss on test : 0.6925\n",
            "Epoch: 017, Loss: 0.6924, Loss on test : 0.6924\n",
            "Epoch: 018, Loss: 0.6923, Loss on test : 0.6923\n",
            "Epoch: 019, Loss: 0.6922, Loss on test : 0.6922\n",
            "Epoch: 020, Loss: 0.6921, Loss on test : 0.6921\n",
            "Epoch: 021, Loss: 0.6920, Loss on test : 0.6920\n",
            "Epoch: 022, Loss: 0.6919, Loss on test : 0.6919\n",
            "Epoch: 023, Loss: 0.6917, Loss on test : 0.6918\n",
            "Epoch: 024, Loss: 0.6916, Loss on test : 0.6916\n",
            "Epoch: 025, Loss: 0.6914, Loss on test : 0.6915\n",
            "Epoch: 026, Loss: 0.6913, Loss on test : 0.6913\n",
            "Epoch: 027, Loss: 0.6911, Loss on test : 0.6911\n",
            "Epoch: 028, Loss: 0.6909, Loss on test : 0.6909\n",
            "Epoch: 029, Loss: 0.6907, Loss on test : 0.6907\n",
            "Epoch: 030, Loss: 0.6905, Loss on test : 0.6905\n",
            "Epoch: 031, Loss: 0.6903, Loss on test : 0.6903\n",
            "Epoch: 032, Loss: 0.6900, Loss on test : 0.6900\n",
            "Epoch: 033, Loss: 0.6897, Loss on test : 0.6898\n",
            "Epoch: 034, Loss: 0.6895, Loss on test : 0.6895\n",
            "Epoch: 035, Loss: 0.6892, Loss on test : 0.6892\n",
            "Epoch: 036, Loss: 0.6888, Loss on test : 0.6889\n",
            "Epoch: 037, Loss: 0.6885, Loss on test : 0.6886\n",
            "Epoch: 038, Loss: 0.6881, Loss on test : 0.6882\n",
            "Epoch: 039, Loss: 0.6877, Loss on test : 0.6878\n",
            "Epoch: 040, Loss: 0.6874, Loss on test : 0.6874\n",
            "Epoch: 041, Loss: 0.6870, Loss on test : 0.6870\n",
            "Epoch: 042, Loss: 0.6865, Loss on test : 0.6866\n",
            "Epoch: 043, Loss: 0.6861, Loss on test : 0.6862\n",
            "Epoch: 044, Loss: 0.6854, Loss on test : 0.6857\n",
            "Epoch: 045, Loss: 0.6850, Loss on test : 0.6851\n",
            "Epoch: 046, Loss: 0.6845, Loss on test : 0.6846\n",
            "Epoch: 047, Loss: 0.6840, Loss on test : 0.6841\n",
            "Epoch: 048, Loss: 0.6834, Loss on test : 0.6836\n",
            "Epoch: 049, Loss: 0.6828, Loss on test : 0.6829\n",
            "Epoch: 050, Loss: 0.6821, Loss on test : 0.6824\n",
            "Epoch: 051, Loss: 0.6814, Loss on test : 0.6816\n",
            "Epoch: 052, Loss: 0.6807, Loss on test : 0.6810\n",
            "Epoch: 053, Loss: 0.6800, Loss on test : 0.6802\n",
            "Epoch: 054, Loss: 0.6793, Loss on test : 0.6794\n",
            "Epoch: 055, Loss: 0.6784, Loss on test : 0.6788\n",
            "Epoch: 056, Loss: 0.6777, Loss on test : 0.6779\n",
            "Epoch: 057, Loss: 0.6768, Loss on test : 0.6772\n",
            "Epoch: 058, Loss: 0.6762, Loss on test : 0.6764\n",
            "Epoch: 059, Loss: 0.6750, Loss on test : 0.6754\n",
            "Epoch: 060, Loss: 0.6742, Loss on test : 0.6744\n",
            "Epoch: 061, Loss: 0.6731, Loss on test : 0.6735\n",
            "Epoch: 062, Loss: 0.6723, Loss on test : 0.6729\n",
            "Epoch: 063, Loss: 0.6710, Loss on test : 0.6717\n",
            "Epoch: 064, Loss: 0.6703, Loss on test : 0.6706\n",
            "Epoch: 065, Loss: 0.6690, Loss on test : 0.6695\n",
            "Epoch: 066, Loss: 0.6679, Loss on test : 0.6685\n",
            "Epoch: 067, Loss: 0.6668, Loss on test : 0.6673\n",
            "Epoch: 068, Loss: 0.6656, Loss on test : 0.6661\n",
            "Epoch: 069, Loss: 0.6642, Loss on test : 0.6651\n",
            "Epoch: 070, Loss: 0.6631, Loss on test : 0.6640\n",
            "Epoch: 071, Loss: 0.6619, Loss on test : 0.6629\n",
            "Epoch: 072, Loss: 0.6606, Loss on test : 0.6616\n",
            "Epoch: 073, Loss: 0.6594, Loss on test : 0.6601\n",
            "Epoch: 074, Loss: 0.6579, Loss on test : 0.6586\n",
            "Epoch: 075, Loss: 0.6564, Loss on test : 0.6575\n",
            "Epoch: 076, Loss: 0.6551, Loss on test : 0.6561\n",
            "Epoch: 077, Loss: 0.6537, Loss on test : 0.6546\n",
            "Epoch: 078, Loss: 0.6525, Loss on test : 0.6535\n",
            "Epoch: 079, Loss: 0.6507, Loss on test : 0.6516\n",
            "Epoch: 080, Loss: 0.6496, Loss on test : 0.6504\n",
            "Epoch: 081, Loss: 0.6473, Loss on test : 0.6491\n",
            "Epoch: 082, Loss: 0.6459, Loss on test : 0.6472\n",
            "Epoch: 083, Loss: 0.6443, Loss on test : 0.6460\n",
            "Epoch: 084, Loss: 0.6427, Loss on test : 0.6441\n",
            "Epoch: 085, Loss: 0.6414, Loss on test : 0.6427\n",
            "Epoch: 086, Loss: 0.6398, Loss on test : 0.6412\n",
            "Epoch: 087, Loss: 0.6379, Loss on test : 0.6394\n",
            "Epoch: 088, Loss: 0.6359, Loss on test : 0.6381\n",
            "Epoch: 089, Loss: 0.6345, Loss on test : 0.6357\n",
            "Epoch: 090, Loss: 0.6325, Loss on test : 0.6339\n",
            "Epoch: 091, Loss: 0.6311, Loss on test : 0.6328\n",
            "Epoch: 092, Loss: 0.6289, Loss on test : 0.6309\n",
            "Epoch: 093, Loss: 0.6271, Loss on test : 0.6284\n",
            "Epoch: 094, Loss: 0.6253, Loss on test : 0.6275\n",
            "Epoch: 095, Loss: 0.6235, Loss on test : 0.6253\n",
            "Epoch: 096, Loss: 0.6207, Loss on test : 0.6227\n",
            "Epoch: 097, Loss: 0.6198, Loss on test : 0.6217\n",
            "Epoch: 098, Loss: 0.6172, Loss on test : 0.6199\n",
            "Epoch: 099, Loss: 0.6160, Loss on test : 0.6178\n",
            "Epoch: 100, Loss: 0.6140, Loss on test : 0.6161\n",
            "Epoch: 101, Loss: 0.6115, Loss on test : 0.6141\n",
            "Epoch: 102, Loss: 0.6089, Loss on test : 0.6120\n",
            "Epoch: 103, Loss: 0.6073, Loss on test : 0.6107\n",
            "Epoch: 104, Loss: 0.6053, Loss on test : 0.6090\n",
            "Epoch: 105, Loss: 0.6032, Loss on test : 0.6067\n",
            "Epoch: 106, Loss: 0.6014, Loss on test : 0.6047\n",
            "Epoch: 107, Loss: 0.5996, Loss on test : 0.6028\n",
            "Epoch: 108, Loss: 0.5963, Loss on test : 0.6004\n",
            "Epoch: 109, Loss: 0.5952, Loss on test : 0.5993\n",
            "Epoch: 110, Loss: 0.5933, Loss on test : 0.5964\n",
            "Epoch: 111, Loss: 0.5904, Loss on test : 0.5952\n",
            "Epoch: 112, Loss: 0.5897, Loss on test : 0.5938\n",
            "Epoch: 113, Loss: 0.5868, Loss on test : 0.5901\n",
            "Epoch: 114, Loss: 0.5850, Loss on test : 0.5882\n",
            "Epoch: 115, Loss: 0.5824, Loss on test : 0.5867\n",
            "Epoch: 116, Loss: 0.5801, Loss on test : 0.5845\n",
            "Epoch: 117, Loss: 0.5787, Loss on test : 0.5828\n",
            "Epoch: 118, Loss: 0.5756, Loss on test : 0.5801\n",
            "Epoch: 119, Loss: 0.5750, Loss on test : 0.5781\n",
            "Epoch: 120, Loss: 0.5729, Loss on test : 0.5772\n",
            "Epoch: 121, Loss: 0.5693, Loss on test : 0.5736\n",
            "Epoch: 122, Loss: 0.5675, Loss on test : 0.5732\n",
            "Epoch: 123, Loss: 0.5656, Loss on test : 0.5713\n",
            "Epoch: 124, Loss: 0.5644, Loss on test : 0.5683\n",
            "Epoch: 125, Loss: 0.5612, Loss on test : 0.5665\n",
            "Epoch: 126, Loss: 0.5590, Loss on test : 0.5647\n",
            "Epoch: 127, Loss: 0.5588, Loss on test : 0.5613\n",
            "Epoch: 128, Loss: 0.5561, Loss on test : 0.5602\n",
            "Epoch: 129, Loss: 0.5541, Loss on test : 0.5595\n",
            "Epoch: 130, Loss: 0.5524, Loss on test : 0.5557\n",
            "Epoch: 131, Loss: 0.5493, Loss on test : 0.5551\n",
            "Epoch: 132, Loss: 0.5486, Loss on test : 0.5528\n",
            "Epoch: 133, Loss: 0.5462, Loss on test : 0.5521\n",
            "Epoch: 134, Loss: 0.5440, Loss on test : 0.5506\n",
            "Epoch: 135, Loss: 0.5415, Loss on test : 0.5475\n",
            "Epoch: 136, Loss: 0.5397, Loss on test : 0.5463\n",
            "Epoch: 137, Loss: 0.5370, Loss on test : 0.5447\n",
            "Epoch: 138, Loss: 0.5369, Loss on test : 0.5427\n",
            "Epoch: 139, Loss: 0.5351, Loss on test : 0.5398\n",
            "Epoch: 140, Loss: 0.5338, Loss on test : 0.5391\n",
            "Epoch: 141, Loss: 0.5306, Loss on test : 0.5361\n",
            "Epoch: 142, Loss: 0.5311, Loss on test : 0.5341\n",
            "Epoch: 143, Loss: 0.5266, Loss on test : 0.5340\n",
            "Epoch: 144, Loss: 0.5238, Loss on test : 0.5307\n",
            "Epoch: 145, Loss: 0.5246, Loss on test : 0.5317\n",
            "Epoch: 146, Loss: 0.5212, Loss on test : 0.5294\n",
            "Epoch: 147, Loss: 0.5187, Loss on test : 0.5266\n",
            "Epoch: 148, Loss: 0.5206, Loss on test : 0.5258\n",
            "Epoch: 149, Loss: 0.5162, Loss on test : 0.5238\n",
            "Epoch: 150, Loss: 0.5125, Loss on test : 0.5225\n",
            "Epoch: 151, Loss: 0.5136, Loss on test : 0.5204\n",
            "Epoch: 152, Loss: 0.5088, Loss on test : 0.5202\n",
            "Epoch: 153, Loss: 0.5092, Loss on test : 0.5182\n",
            "Epoch: 154, Loss: 0.5073, Loss on test : 0.5158\n",
            "Epoch: 155, Loss: 0.5058, Loss on test : 0.5130\n",
            "Epoch: 156, Loss: 0.5057, Loss on test : 0.5122\n",
            "Epoch: 157, Loss: 0.5051, Loss on test : 0.5106\n",
            "Epoch: 158, Loss: 0.5024, Loss on test : 0.5095\n",
            "Epoch: 159, Loss: 0.5006, Loss on test : 0.5082\n",
            "Epoch: 160, Loss: 0.4978, Loss on test : 0.5071\n",
            "Epoch: 161, Loss: 0.4961, Loss on test : 0.5038\n",
            "Epoch: 162, Loss: 0.4945, Loss on test : 0.5063\n",
            "Epoch: 163, Loss: 0.4952, Loss on test : 0.5035\n",
            "Epoch: 164, Loss: 0.4942, Loss on test : 0.5001\n",
            "Epoch: 165, Loss: 0.4914, Loss on test : 0.5010\n",
            "Epoch: 166, Loss: 0.4889, Loss on test : 0.4962\n",
            "Epoch: 167, Loss: 0.4885, Loss on test : 0.4968\n",
            "Epoch: 168, Loss: 0.4858, Loss on test : 0.4962\n",
            "Epoch: 169, Loss: 0.4845, Loss on test : 0.4948\n",
            "Epoch: 170, Loss: 0.4847, Loss on test : 0.4943\n",
            "Epoch: 171, Loss: 0.4845, Loss on test : 0.4922\n",
            "Epoch: 172, Loss: 0.4833, Loss on test : 0.4910\n",
            "Epoch: 173, Loss: 0.4819, Loss on test : 0.4913\n",
            "Epoch: 174, Loss: 0.4811, Loss on test : 0.4889\n",
            "Epoch: 175, Loss: 0.4744, Loss on test : 0.4880\n",
            "Epoch: 176, Loss: 0.4789, Loss on test : 0.4862\n",
            "Epoch: 177, Loss: 0.4763, Loss on test : 0.4855\n",
            "Epoch: 178, Loss: 0.4744, Loss on test : 0.4820\n",
            "Epoch: 179, Loss: 0.4758, Loss on test : 0.4817\n",
            "Epoch: 180, Loss: 0.4713, Loss on test : 0.4816\n",
            "Epoch: 181, Loss: 0.4693, Loss on test : 0.4768\n",
            "Epoch: 182, Loss: 0.4664, Loss on test : 0.4763\n",
            "Epoch: 183, Loss: 0.4674, Loss on test : 0.4782\n",
            "Epoch: 184, Loss: 0.4677, Loss on test : 0.4791\n",
            "Epoch: 185, Loss: 0.4684, Loss on test : 0.4772\n",
            "Epoch: 186, Loss: 0.4662, Loss on test : 0.4759\n",
            "Epoch: 187, Loss: 0.4647, Loss on test : 0.4754\n",
            "Epoch: 188, Loss: 0.4639, Loss on test : 0.4751\n",
            "Epoch: 189, Loss: 0.4627, Loss on test : 0.4731\n",
            "Epoch: 190, Loss: 0.4628, Loss on test : 0.4712\n",
            "Epoch: 191, Loss: 0.4623, Loss on test : 0.4710\n",
            "Epoch: 192, Loss: 0.4590, Loss on test : 0.4684\n",
            "Epoch: 193, Loss: 0.4589, Loss on test : 0.4675\n",
            "Epoch: 194, Loss: 0.4619, Loss on test : 0.4714\n",
            "Epoch: 195, Loss: 0.4563, Loss on test : 0.4669\n",
            "Epoch: 196, Loss: 0.4532, Loss on test : 0.4653\n",
            "Epoch: 197, Loss: 0.4540, Loss on test : 0.4676\n",
            "Epoch: 198, Loss: 0.4543, Loss on test : 0.4637\n",
            "Epoch: 199, Loss: 0.4549, Loss on test : 0.4672\n",
            "Epoch: 200, Loss: 0.4549, Loss on test : 0.4624\n",
            "Epoch: 201, Loss: 0.4471, Loss on test : 0.4629\n",
            "Epoch: 202, Loss: 0.4502, Loss on test : 0.4594\n",
            "Epoch: 203, Loss: 0.4493, Loss on test : 0.4567\n",
            "Epoch: 204, Loss: 0.4488, Loss on test : 0.4578\n",
            "Epoch: 205, Loss: 0.4509, Loss on test : 0.4589\n",
            "Epoch: 206, Loss: 0.4508, Loss on test : 0.4588\n",
            "Epoch: 207, Loss: 0.4438, Loss on test : 0.4564\n",
            "Epoch: 208, Loss: 0.4443, Loss on test : 0.4587\n",
            "Epoch: 209, Loss: 0.4465, Loss on test : 0.4566\n",
            "Epoch: 210, Loss: 0.4475, Loss on test : 0.4569\n",
            "Epoch: 211, Loss: 0.4393, Loss on test : 0.4550\n",
            "Epoch: 212, Loss: 0.4459, Loss on test : 0.4552\n",
            "Epoch: 213, Loss: 0.4399, Loss on test : 0.4521\n",
            "Epoch: 214, Loss: 0.4397, Loss on test : 0.4532\n",
            "Epoch: 215, Loss: 0.4411, Loss on test : 0.4510\n",
            "Epoch: 216, Loss: 0.4392, Loss on test : 0.4518\n",
            "Epoch: 217, Loss: 0.4369, Loss on test : 0.4514\n",
            "Epoch: 218, Loss: 0.4371, Loss on test : 0.4517\n",
            "Epoch: 219, Loss: 0.4397, Loss on test : 0.4501\n",
            "Epoch: 220, Loss: 0.4352, Loss on test : 0.4524\n",
            "Epoch: 221, Loss: 0.4358, Loss on test : 0.4504\n",
            "Epoch: 222, Loss: 0.4344, Loss on test : 0.4484\n",
            "Epoch: 223, Loss: 0.4337, Loss on test : 0.4445\n",
            "Epoch: 224, Loss: 0.4377, Loss on test : 0.4479\n",
            "Epoch: 225, Loss: 0.4257, Loss on test : 0.4490\n",
            "Epoch: 226, Loss: 0.4297, Loss on test : 0.4442\n",
            "Epoch: 227, Loss: 0.4310, Loss on test : 0.4470\n",
            "Epoch: 228, Loss: 0.4300, Loss on test : 0.4400\n",
            "Epoch: 229, Loss: 0.4282, Loss on test : 0.4407\n",
            "Epoch: 230, Loss: 0.4304, Loss on test : 0.4393\n",
            "Epoch: 231, Loss: 0.4276, Loss on test : 0.4421\n",
            "Epoch: 232, Loss: 0.4291, Loss on test : 0.4397\n",
            "Epoch: 233, Loss: 0.4293, Loss on test : 0.4415\n",
            "Epoch: 234, Loss: 0.4269, Loss on test : 0.4425\n",
            "Epoch: 235, Loss: 0.4271, Loss on test : 0.4379\n",
            "Epoch: 236, Loss: 0.4224, Loss on test : 0.4385\n",
            "Epoch: 237, Loss: 0.4264, Loss on test : 0.4363\n",
            "Epoch: 238, Loss: 0.4236, Loss on test : 0.4358\n",
            "Epoch: 239, Loss: 0.4219, Loss on test : 0.4333\n",
            "Epoch: 240, Loss: 0.4244, Loss on test : 0.4346\n",
            "Epoch: 241, Loss: 0.4245, Loss on test : 0.4350\n",
            "Epoch: 242, Loss: 0.4244, Loss on test : 0.4365\n",
            "Epoch: 243, Loss: 0.4186, Loss on test : 0.4323\n",
            "Epoch: 244, Loss: 0.4170, Loss on test : 0.4310\n",
            "Epoch: 245, Loss: 0.4149, Loss on test : 0.4325\n",
            "Epoch: 246, Loss: 0.4199, Loss on test : 0.4319\n",
            "Epoch: 247, Loss: 0.4177, Loss on test : 0.4266\n",
            "Epoch: 248, Loss: 0.4148, Loss on test : 0.4285\n",
            "Epoch: 249, Loss: 0.4177, Loss on test : 0.4282\n",
            "Epoch: 250, Loss: 0.4178, Loss on test : 0.4269\n",
            "Epoch: 251, Loss: 0.4159, Loss on test : 0.4245\n",
            "Epoch: 252, Loss: 0.4149, Loss on test : 0.4281\n",
            "Epoch: 253, Loss: 0.4142, Loss on test : 0.4298\n",
            "Epoch: 254, Loss: 0.4129, Loss on test : 0.4287\n",
            "Epoch: 255, Loss: 0.4129, Loss on test : 0.4250\n",
            "Epoch: 256, Loss: 0.4109, Loss on test : 0.4207\n",
            "Epoch: 257, Loss: 0.4127, Loss on test : 0.4240\n",
            "Epoch: 258, Loss: 0.4090, Loss on test : 0.4280\n",
            "Epoch: 259, Loss: 0.4088, Loss on test : 0.4234\n",
            "Epoch: 260, Loss: 0.4105, Loss on test : 0.4212\n",
            "Epoch: 261, Loss: 0.4087, Loss on test : 0.4231\n",
            "Epoch: 262, Loss: 0.4105, Loss on test : 0.4224\n",
            "Epoch: 263, Loss: 0.4156, Loss on test : 0.4221\n",
            "Epoch: 264, Loss: 0.4054, Loss on test : 0.4206\n",
            "Epoch: 265, Loss: 0.4046, Loss on test : 0.4186\n",
            "Epoch: 266, Loss: 0.4058, Loss on test : 0.4212\n",
            "Epoch: 267, Loss: 0.4064, Loss on test : 0.4210\n",
            "Epoch: 268, Loss: 0.4058, Loss on test : 0.4155\n",
            "Epoch: 269, Loss: 0.4039, Loss on test : 0.4166\n",
            "Epoch: 270, Loss: 0.3996, Loss on test : 0.4159\n",
            "Epoch: 271, Loss: 0.3996, Loss on test : 0.4157\n",
            "Epoch: 272, Loss: 0.4020, Loss on test : 0.4188\n",
            "Epoch: 273, Loss: 0.4037, Loss on test : 0.4146\n",
            "Epoch: 274, Loss: 0.4003, Loss on test : 0.4105\n",
            "Epoch: 275, Loss: 0.3980, Loss on test : 0.4079\n",
            "Epoch: 276, Loss: 0.3942, Loss on test : 0.4117\n",
            "Epoch: 277, Loss: 0.3989, Loss on test : 0.4137\n",
            "Epoch: 278, Loss: 0.3975, Loss on test : 0.4112\n",
            "Epoch: 279, Loss: 0.3940, Loss on test : 0.4128\n",
            "Epoch: 280, Loss: 0.3983, Loss on test : 0.4083\n",
            "Epoch: 281, Loss: 0.3985, Loss on test : 0.4088\n",
            "Epoch: 282, Loss: 0.3943, Loss on test : 0.4096\n",
            "Epoch: 283, Loss: 0.3905, Loss on test : 0.4073\n",
            "Epoch: 284, Loss: 0.3931, Loss on test : 0.4110\n",
            "Epoch: 285, Loss: 0.3920, Loss on test : 0.4072\n",
            "Epoch: 286, Loss: 0.3894, Loss on test : 0.4074\n",
            "Epoch: 287, Loss: 0.3838, Loss on test : 0.4069\n",
            "Epoch: 288, Loss: 0.3900, Loss on test : 0.4102\n",
            "Epoch: 289, Loss: 0.3851, Loss on test : 0.4056\n",
            "Epoch: 290, Loss: 0.3880, Loss on test : 0.4024\n",
            "Epoch: 291, Loss: 0.3883, Loss on test : 0.4036\n",
            "Epoch: 292, Loss: 0.3846, Loss on test : 0.4023\n",
            "Epoch: 293, Loss: 0.3835, Loss on test : 0.4011\n",
            "Epoch: 294, Loss: 0.3821, Loss on test : 0.4033\n",
            "Epoch: 295, Loss: 0.3800, Loss on test : 0.4019\n",
            "Epoch: 296, Loss: 0.3807, Loss on test : 0.3966\n",
            "Epoch: 297, Loss: 0.3842, Loss on test : 0.4012\n",
            "Epoch: 298, Loss: 0.3787, Loss on test : 0.3995\n",
            "Epoch: 299, Loss: 0.3831, Loss on test : 0.3973\n",
            "Epoch: 300, Loss: 0.3833, Loss on test : 0.3972\n",
            "Epoch: 301, Loss: 0.3812, Loss on test : 0.3974\n",
            "Epoch: 302, Loss: 0.3822, Loss on test : 0.3980\n",
            "Epoch: 303, Loss: 0.3784, Loss on test : 0.3972\n",
            "Epoch: 304, Loss: 0.3790, Loss on test : 0.3913\n",
            "Epoch: 305, Loss: 0.3744, Loss on test : 0.3958\n",
            "Epoch: 306, Loss: 0.3752, Loss on test : 0.3910\n",
            "Epoch: 307, Loss: 0.3827, Loss on test : 0.3964\n",
            "Epoch: 308, Loss: 0.3763, Loss on test : 0.3956\n",
            "Epoch: 309, Loss: 0.3767, Loss on test : 0.3921\n",
            "Epoch: 310, Loss: 0.3770, Loss on test : 0.3885\n",
            "Epoch: 311, Loss: 0.3716, Loss on test : 0.3864\n",
            "Epoch: 312, Loss: 0.3735, Loss on test : 0.3908\n",
            "Epoch: 313, Loss: 0.3703, Loss on test : 0.3876\n",
            "Epoch: 314, Loss: 0.3679, Loss on test : 0.3907\n",
            "Epoch: 315, Loss: 0.3728, Loss on test : 0.3868\n",
            "Epoch: 316, Loss: 0.3739, Loss on test : 0.3878\n",
            "Epoch: 317, Loss: 0.3672, Loss on test : 0.3863\n",
            "Epoch: 318, Loss: 0.3668, Loss on test : 0.3873\n",
            "Epoch: 319, Loss: 0.3626, Loss on test : 0.3836\n",
            "Epoch: 320, Loss: 0.3611, Loss on test : 0.3797\n",
            "Epoch: 321, Loss: 0.3656, Loss on test : 0.3816\n",
            "Epoch: 322, Loss: 0.3691, Loss on test : 0.3846\n",
            "Epoch: 323, Loss: 0.3615, Loss on test : 0.3828\n",
            "Epoch: 324, Loss: 0.3615, Loss on test : 0.3858\n",
            "Epoch: 325, Loss: 0.3601, Loss on test : 0.3852\n",
            "Epoch: 326, Loss: 0.3632, Loss on test : 0.3806\n",
            "Epoch: 327, Loss: 0.3616, Loss on test : 0.3816\n",
            "Epoch: 328, Loss: 0.3616, Loss on test : 0.3765\n",
            "Epoch: 329, Loss: 0.3603, Loss on test : 0.3763\n",
            "Epoch: 330, Loss: 0.3597, Loss on test : 0.3780\n",
            "Epoch: 331, Loss: 0.3539, Loss on test : 0.3750\n",
            "Epoch: 332, Loss: 0.3576, Loss on test : 0.3710\n",
            "Epoch: 333, Loss: 0.3563, Loss on test : 0.3769\n",
            "Epoch: 334, Loss: 0.3600, Loss on test : 0.3735\n",
            "Epoch: 335, Loss: 0.3523, Loss on test : 0.3719\n",
            "Epoch: 336, Loss: 0.3500, Loss on test : 0.3771\n",
            "Epoch: 337, Loss: 0.3514, Loss on test : 0.3752\n",
            "Epoch: 338, Loss: 0.3506, Loss on test : 0.3720\n",
            "Epoch: 339, Loss: 0.3509, Loss on test : 0.3694\n",
            "Epoch: 340, Loss: 0.3478, Loss on test : 0.3702\n",
            "Epoch: 341, Loss: 0.3511, Loss on test : 0.3706\n",
            "Epoch: 342, Loss: 0.3529, Loss on test : 0.3678\n",
            "Epoch: 343, Loss: 0.3581, Loss on test : 0.3667\n",
            "Epoch: 344, Loss: 0.3523, Loss on test : 0.3648\n",
            "Epoch: 345, Loss: 0.3445, Loss on test : 0.3668\n",
            "Epoch: 346, Loss: 0.3406, Loss on test : 0.3626\n",
            "Epoch: 347, Loss: 0.3430, Loss on test : 0.3648\n",
            "Epoch: 348, Loss: 0.3434, Loss on test : 0.3628\n",
            "Epoch: 349, Loss: 0.3450, Loss on test : 0.3680\n",
            "Epoch: 350, Loss: 0.3389, Loss on test : 0.3625\n",
            "Epoch: 351, Loss: 0.3449, Loss on test : 0.3688\n",
            "Epoch: 352, Loss: 0.3367, Loss on test : 0.3554\n",
            "Epoch: 353, Loss: 0.3419, Loss on test : 0.3628\n",
            "Epoch: 354, Loss: 0.3382, Loss on test : 0.3596\n",
            "Epoch: 355, Loss: 0.3378, Loss on test : 0.3655\n",
            "Epoch: 356, Loss: 0.3390, Loss on test : 0.3596\n",
            "Epoch: 357, Loss: 0.3326, Loss on test : 0.3638\n",
            "Epoch: 358, Loss: 0.3401, Loss on test : 0.3593\n",
            "Epoch: 359, Loss: 0.3361, Loss on test : 0.3573\n",
            "Epoch: 360, Loss: 0.3314, Loss on test : 0.3597\n",
            "Epoch: 361, Loss: 0.3329, Loss on test : 0.3499\n",
            "Epoch: 362, Loss: 0.3339, Loss on test : 0.3570\n",
            "Epoch: 363, Loss: 0.3315, Loss on test : 0.3566\n",
            "Epoch: 364, Loss: 0.3287, Loss on test : 0.3536\n",
            "Epoch: 365, Loss: 0.3281, Loss on test : 0.3545\n",
            "Epoch: 366, Loss: 0.3298, Loss on test : 0.3497\n",
            "Epoch: 367, Loss: 0.3292, Loss on test : 0.3490\n",
            "Epoch: 368, Loss: 0.3269, Loss on test : 0.3499\n",
            "Epoch: 369, Loss: 0.3257, Loss on test : 0.3498\n",
            "Epoch: 370, Loss: 0.3265, Loss on test : 0.3517\n",
            "Epoch: 371, Loss: 0.3256, Loss on test : 0.3465\n",
            "Epoch: 372, Loss: 0.3259, Loss on test : 0.3414\n",
            "Epoch: 373, Loss: 0.3230, Loss on test : 0.3505\n",
            "Epoch: 374, Loss: 0.3174, Loss on test : 0.3469\n",
            "Epoch: 375, Loss: 0.3167, Loss on test : 0.3465\n",
            "Epoch: 376, Loss: 0.3217, Loss on test : 0.3456\n",
            "Epoch: 377, Loss: 0.3173, Loss on test : 0.3452\n",
            "Epoch: 378, Loss: 0.3203, Loss on test : 0.3425\n",
            "Epoch: 379, Loss: 0.3221, Loss on test : 0.3437\n",
            "Epoch: 380, Loss: 0.3116, Loss on test : 0.3387\n",
            "Epoch: 381, Loss: 0.3127, Loss on test : 0.3435\n",
            "Epoch: 382, Loss: 0.3095, Loss on test : 0.3404\n",
            "Epoch: 383, Loss: 0.3184, Loss on test : 0.3397\n",
            "Epoch: 384, Loss: 0.3162, Loss on test : 0.3352\n",
            "Epoch: 385, Loss: 0.3118, Loss on test : 0.3389\n",
            "Epoch: 386, Loss: 0.3056, Loss on test : 0.3402\n",
            "Epoch: 387, Loss: 0.3110, Loss on test : 0.3387\n",
            "Epoch: 388, Loss: 0.3172, Loss on test : 0.3381\n",
            "Epoch: 389, Loss: 0.3091, Loss on test : 0.3411\n",
            "Epoch: 390, Loss: 0.3049, Loss on test : 0.3353\n",
            "Epoch: 391, Loss: 0.3068, Loss on test : 0.3320\n",
            "Epoch: 392, Loss: 0.3130, Loss on test : 0.3364\n",
            "Epoch: 393, Loss: 0.3076, Loss on test : 0.3361\n",
            "Epoch: 394, Loss: 0.3058, Loss on test : 0.3307\n",
            "Epoch: 395, Loss: 0.3052, Loss on test : 0.3346\n",
            "Epoch: 396, Loss: 0.3047, Loss on test : 0.3347\n",
            "Epoch: 397, Loss: 0.3014, Loss on test : 0.3303\n",
            "Epoch: 398, Loss: 0.3009, Loss on test : 0.3301\n",
            "Epoch: 399, Loss: 0.3022, Loss on test : 0.3285\n",
            "Epoch: 400, Loss: 0.3016, Loss on test : 0.3290\n",
            "Epoch: 401, Loss: 0.2996, Loss on test : 0.3291\n",
            "Epoch: 402, Loss: 0.2990, Loss on test : 0.3234\n",
            "Epoch: 403, Loss: 0.3028, Loss on test : 0.3259\n",
            "Epoch: 404, Loss: 0.3023, Loss on test : 0.3280\n",
            "Epoch: 405, Loss: 0.2944, Loss on test : 0.3268\n",
            "Epoch: 406, Loss: 0.2974, Loss on test : 0.3283\n",
            "Epoch: 407, Loss: 0.2958, Loss on test : 0.3261\n",
            "Epoch: 408, Loss: 0.2966, Loss on test : 0.3225\n",
            "Epoch: 409, Loss: 0.2933, Loss on test : 0.3182\n",
            "Epoch: 410, Loss: 0.2882, Loss on test : 0.3232\n",
            "Epoch: 411, Loss: 0.2963, Loss on test : 0.3240\n",
            "Epoch: 412, Loss: 0.2906, Loss on test : 0.3149\n",
            "Epoch: 413, Loss: 0.2941, Loss on test : 0.3175\n",
            "Epoch: 414, Loss: 0.2890, Loss on test : 0.3187\n",
            "Epoch: 415, Loss: 0.2881, Loss on test : 0.3114\n",
            "Epoch: 416, Loss: 0.2870, Loss on test : 0.3178\n",
            "Epoch: 417, Loss: 0.2895, Loss on test : 0.3190\n",
            "Epoch: 418, Loss: 0.2858, Loss on test : 0.3188\n",
            "Epoch: 419, Loss: 0.2914, Loss on test : 0.3116\n",
            "Epoch: 420, Loss: 0.2891, Loss on test : 0.3151\n",
            "Epoch: 421, Loss: 0.2891, Loss on test : 0.3129\n",
            "Epoch: 422, Loss: 0.2782, Loss on test : 0.3164\n",
            "Epoch: 423, Loss: 0.2865, Loss on test : 0.3151\n",
            "Epoch: 424, Loss: 0.2843, Loss on test : 0.3170\n",
            "Epoch: 425, Loss: 0.2773, Loss on test : 0.3115\n",
            "Epoch: 426, Loss: 0.2849, Loss on test : 0.3149\n",
            "Epoch: 427, Loss: 0.2790, Loss on test : 0.3116\n",
            "Epoch: 428, Loss: 0.2785, Loss on test : 0.3124\n",
            "Epoch: 429, Loss: 0.2755, Loss on test : 0.3115\n",
            "Epoch: 430, Loss: 0.2758, Loss on test : 0.3095\n",
            "Epoch: 431, Loss: 0.2823, Loss on test : 0.3073\n",
            "Epoch: 432, Loss: 0.2795, Loss on test : 0.3088\n",
            "Epoch: 433, Loss: 0.2781, Loss on test : 0.3080\n",
            "Epoch: 434, Loss: 0.2771, Loss on test : 0.3091\n",
            "Epoch: 435, Loss: 0.2735, Loss on test : 0.3098\n",
            "Epoch: 436, Loss: 0.2764, Loss on test : 0.3041\n",
            "Epoch: 437, Loss: 0.2714, Loss on test : 0.3022\n",
            "Epoch: 438, Loss: 0.2700, Loss on test : 0.3060\n",
            "Epoch: 439, Loss: 0.2742, Loss on test : 0.3028\n",
            "Epoch: 440, Loss: 0.2694, Loss on test : 0.3046\n",
            "Epoch: 441, Loss: 0.2729, Loss on test : 0.2996\n",
            "Epoch: 442, Loss: 0.2732, Loss on test : 0.3058\n",
            "Epoch: 443, Loss: 0.2687, Loss on test : 0.2967\n",
            "Epoch: 444, Loss: 0.2681, Loss on test : 0.2974\n",
            "Epoch: 445, Loss: 0.2717, Loss on test : 0.2961\n",
            "Epoch: 446, Loss: 0.2715, Loss on test : 0.3028\n",
            "Epoch: 447, Loss: 0.2707, Loss on test : 0.3000\n",
            "Epoch: 448, Loss: 0.2677, Loss on test : 0.3009\n",
            "Epoch: 449, Loss: 0.2625, Loss on test : 0.2939\n",
            "Epoch: 450, Loss: 0.2617, Loss on test : 0.2967\n",
            "Epoch: 451, Loss: 0.2602, Loss on test : 0.2965\n",
            "Epoch: 452, Loss: 0.2564, Loss on test : 0.2885\n",
            "Epoch: 453, Loss: 0.2609, Loss on test : 0.2976\n",
            "Epoch: 454, Loss: 0.2599, Loss on test : 0.2976\n",
            "Epoch: 455, Loss: 0.2601, Loss on test : 0.2924\n",
            "Epoch: 456, Loss: 0.2587, Loss on test : 0.2948\n",
            "Epoch: 457, Loss: 0.2563, Loss on test : 0.2926\n",
            "Epoch: 458, Loss: 0.2558, Loss on test : 0.2933\n",
            "Epoch: 459, Loss: 0.2625, Loss on test : 0.2982\n",
            "Epoch: 460, Loss: 0.2596, Loss on test : 0.2914\n",
            "Epoch: 461, Loss: 0.2567, Loss on test : 0.2954\n",
            "Epoch: 462, Loss: 0.2535, Loss on test : 0.2971\n",
            "Epoch: 463, Loss: 0.2525, Loss on test : 0.2870\n",
            "Epoch: 464, Loss: 0.2592, Loss on test : 0.2881\n",
            "Epoch: 465, Loss: 0.2482, Loss on test : 0.2925\n",
            "Epoch: 466, Loss: 0.2522, Loss on test : 0.2876\n",
            "Epoch: 467, Loss: 0.2522, Loss on test : 0.2917\n",
            "Epoch: 468, Loss: 0.2525, Loss on test : 0.2899\n",
            "Epoch: 469, Loss: 0.2514, Loss on test : 0.2861\n",
            "Epoch: 470, Loss: 0.2493, Loss on test : 0.2853\n",
            "Epoch: 471, Loss: 0.2505, Loss on test : 0.2916\n",
            "Epoch: 472, Loss: 0.2492, Loss on test : 0.2824\n",
            "Epoch: 473, Loss: 0.2448, Loss on test : 0.2827\n",
            "Epoch: 474, Loss: 0.2478, Loss on test : 0.2852\n",
            "Epoch: 475, Loss: 0.2461, Loss on test : 0.2846\n",
            "Epoch: 476, Loss: 0.2485, Loss on test : 0.2858\n",
            "Epoch: 477, Loss: 0.2443, Loss on test : 0.2843\n",
            "Epoch: 478, Loss: 0.2441, Loss on test : 0.2838\n",
            "Epoch: 479, Loss: 0.2410, Loss on test : 0.2885\n",
            "Epoch: 480, Loss: 0.2526, Loss on test : 0.2790\n",
            "Epoch: 481, Loss: 0.2416, Loss on test : 0.2798\n",
            "Epoch: 482, Loss: 0.2419, Loss on test : 0.2832\n",
            "Epoch: 483, Loss: 0.2417, Loss on test : 0.2849\n",
            "Epoch: 484, Loss: 0.2394, Loss on test : 0.2749\n",
            "Epoch: 485, Loss: 0.2392, Loss on test : 0.2783\n",
            "Epoch: 486, Loss: 0.2358, Loss on test : 0.2844\n",
            "Epoch: 487, Loss: 0.2429, Loss on test : 0.2775\n",
            "Epoch: 488, Loss: 0.2361, Loss on test : 0.2767\n",
            "Epoch: 489, Loss: 0.2402, Loss on test : 0.2771\n",
            "Epoch: 490, Loss: 0.2391, Loss on test : 0.2758\n",
            "Epoch: 491, Loss: 0.2459, Loss on test : 0.2795\n",
            "Epoch: 492, Loss: 0.2398, Loss on test : 0.2767\n",
            "Epoch: 493, Loss: 0.2389, Loss on test : 0.2785\n",
            "Epoch: 494, Loss: 0.2323, Loss on test : 0.2760\n",
            "Epoch: 495, Loss: 0.2293, Loss on test : 0.2809\n",
            "Epoch: 496, Loss: 0.2383, Loss on test : 0.2745\n",
            "Epoch: 497, Loss: 0.2363, Loss on test : 0.2737\n",
            "Epoch: 498, Loss: 0.2322, Loss on test : 0.2718\n",
            "Epoch: 499, Loss: 0.2300, Loss on test : 0.2780\n",
            "Epoch: 500, Loss: 0.2290, Loss on test : 0.2699\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:03<00:00, 2542.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 500, Val Mean Rank: 112.27 Val MRR: 0.1456, Val Hits@10: 0.3683\n",
            "Epoch: 501, Loss: 0.2329, Loss on test : 0.2745\n",
            "Epoch: 502, Loss: 0.2355, Loss on test : 0.2706\n",
            "Epoch: 503, Loss: 0.2289, Loss on test : 0.2728\n",
            "Epoch: 504, Loss: 0.2302, Loss on test : 0.2745\n",
            "Epoch: 505, Loss: 0.2295, Loss on test : 0.2704\n",
            "Epoch: 506, Loss: 0.2326, Loss on test : 0.2765\n",
            "Epoch: 507, Loss: 0.2278, Loss on test : 0.2693\n",
            "Epoch: 508, Loss: 0.2323, Loss on test : 0.2744\n",
            "Epoch: 509, Loss: 0.2268, Loss on test : 0.2752\n",
            "Epoch: 510, Loss: 0.2271, Loss on test : 0.2731\n",
            "Epoch: 511, Loss: 0.2267, Loss on test : 0.2691\n",
            "Epoch: 512, Loss: 0.2203, Loss on test : 0.2705\n",
            "Epoch: 513, Loss: 0.2256, Loss on test : 0.2680\n",
            "Epoch: 514, Loss: 0.2202, Loss on test : 0.2672\n",
            "Epoch: 515, Loss: 0.2235, Loss on test : 0.2652\n",
            "Epoch: 516, Loss: 0.2236, Loss on test : 0.2714\n",
            "Epoch: 517, Loss: 0.2190, Loss on test : 0.2670\n",
            "Epoch: 518, Loss: 0.2278, Loss on test : 0.2656\n",
            "Epoch: 519, Loss: 0.2205, Loss on test : 0.2674\n",
            "Epoch: 520, Loss: 0.2221, Loss on test : 0.2672\n",
            "Epoch: 521, Loss: 0.2227, Loss on test : 0.2625\n",
            "Epoch: 522, Loss: 0.2184, Loss on test : 0.2680\n",
            "Epoch: 523, Loss: 0.2144, Loss on test : 0.2627\n",
            "Epoch: 524, Loss: 0.2249, Loss on test : 0.2613\n",
            "Epoch: 525, Loss: 0.2225, Loss on test : 0.2635\n",
            "Epoch: 526, Loss: 0.2188, Loss on test : 0.2618\n",
            "Epoch: 527, Loss: 0.2157, Loss on test : 0.2635\n",
            "Epoch: 528, Loss: 0.2183, Loss on test : 0.2587\n",
            "Epoch: 529, Loss: 0.2156, Loss on test : 0.2584\n",
            "Epoch: 530, Loss: 0.2146, Loss on test : 0.2631\n",
            "Epoch: 531, Loss: 0.2161, Loss on test : 0.2624\n",
            "Epoch: 532, Loss: 0.2148, Loss on test : 0.2608\n",
            "Epoch: 533, Loss: 0.2161, Loss on test : 0.2555\n",
            "Epoch: 534, Loss: 0.2140, Loss on test : 0.2633\n",
            "Epoch: 535, Loss: 0.2170, Loss on test : 0.2657\n",
            "Epoch: 536, Loss: 0.2072, Loss on test : 0.2575\n",
            "Epoch: 537, Loss: 0.2118, Loss on test : 0.2594\n",
            "Epoch: 538, Loss: 0.2123, Loss on test : 0.2637\n",
            "Epoch: 539, Loss: 0.2151, Loss on test : 0.2549\n",
            "Epoch: 540, Loss: 0.2089, Loss on test : 0.2553\n",
            "Epoch: 541, Loss: 0.2120, Loss on test : 0.2570\n",
            "Epoch: 542, Loss: 0.2065, Loss on test : 0.2558\n",
            "Epoch: 543, Loss: 0.2120, Loss on test : 0.2545\n",
            "Epoch: 544, Loss: 0.2080, Loss on test : 0.2564\n",
            "Epoch: 545, Loss: 0.2087, Loss on test : 0.2477\n",
            "Epoch: 546, Loss: 0.2095, Loss on test : 0.2573\n",
            "Epoch: 547, Loss: 0.2048, Loss on test : 0.2578\n",
            "Epoch: 548, Loss: 0.2083, Loss on test : 0.2575\n",
            "Epoch: 549, Loss: 0.2020, Loss on test : 0.2541\n",
            "Epoch: 550, Loss: 0.2090, Loss on test : 0.2543\n",
            "Epoch: 551, Loss: 0.2056, Loss on test : 0.2559\n",
            "Epoch: 552, Loss: 0.2117, Loss on test : 0.2498\n",
            "Epoch: 553, Loss: 0.2050, Loss on test : 0.2520\n",
            "Epoch: 554, Loss: 0.2049, Loss on test : 0.2559\n",
            "Epoch: 555, Loss: 0.2043, Loss on test : 0.2512\n",
            "Epoch: 556, Loss: 0.2034, Loss on test : 0.2547\n",
            "Epoch: 557, Loss: 0.2014, Loss on test : 0.2527\n",
            "Epoch: 558, Loss: 0.2063, Loss on test : 0.2525\n",
            "Epoch: 559, Loss: 0.2046, Loss on test : 0.2489\n",
            "Epoch: 560, Loss: 0.2079, Loss on test : 0.2494\n",
            "Epoch: 561, Loss: 0.2092, Loss on test : 0.2487\n",
            "Epoch: 562, Loss: 0.1999, Loss on test : 0.2502\n",
            "Epoch: 563, Loss: 0.2028, Loss on test : 0.2515\n",
            "Epoch: 564, Loss: 0.1976, Loss on test : 0.2461\n",
            "Epoch: 565, Loss: 0.2018, Loss on test : 0.2559\n",
            "Epoch: 566, Loss: 0.1954, Loss on test : 0.2485\n",
            "Epoch: 567, Loss: 0.2029, Loss on test : 0.2503\n",
            "Epoch: 568, Loss: 0.2039, Loss on test : 0.2451\n",
            "Epoch: 569, Loss: 0.1938, Loss on test : 0.2518\n",
            "Epoch: 570, Loss: 0.2016, Loss on test : 0.2466\n",
            "Epoch: 571, Loss: 0.1956, Loss on test : 0.2491\n",
            "Epoch: 572, Loss: 0.2032, Loss on test : 0.2525\n",
            "Epoch: 573, Loss: 0.1965, Loss on test : 0.2413\n",
            "Epoch: 574, Loss: 0.1876, Loss on test : 0.2493\n",
            "Epoch: 575, Loss: 0.1988, Loss on test : 0.2497\n",
            "Epoch: 576, Loss: 0.1943, Loss on test : 0.2508\n",
            "Epoch: 577, Loss: 0.2009, Loss on test : 0.2465\n",
            "Epoch: 578, Loss: 0.1911, Loss on test : 0.2488\n",
            "Epoch: 579, Loss: 0.1932, Loss on test : 0.2483\n",
            "Epoch: 580, Loss: 0.1892, Loss on test : 0.2487\n",
            "Epoch: 581, Loss: 0.1950, Loss on test : 0.2435\n",
            "Epoch: 582, Loss: 0.2002, Loss on test : 0.2429\n",
            "Epoch: 583, Loss: 0.1890, Loss on test : 0.2500\n",
            "Epoch: 584, Loss: 0.1938, Loss on test : 0.2450\n",
            "Epoch: 585, Loss: 0.1917, Loss on test : 0.2480\n",
            "Epoch: 586, Loss: 0.1953, Loss on test : 0.2447\n",
            "Epoch: 587, Loss: 0.1937, Loss on test : 0.2392\n",
            "Epoch: 588, Loss: 0.1928, Loss on test : 0.2481\n",
            "Epoch: 589, Loss: 0.1869, Loss on test : 0.2429\n",
            "Epoch: 590, Loss: 0.1941, Loss on test : 0.2414\n",
            "Epoch: 591, Loss: 0.1893, Loss on test : 0.2402\n",
            "Epoch: 592, Loss: 0.1861, Loss on test : 0.2500\n",
            "Epoch: 593, Loss: 0.1866, Loss on test : 0.2452\n",
            "Epoch: 594, Loss: 0.1911, Loss on test : 0.2463\n",
            "Epoch: 595, Loss: 0.1886, Loss on test : 0.2461\n",
            "Epoch: 596, Loss: 0.1893, Loss on test : 0.2400\n",
            "Epoch: 597, Loss: 0.1871, Loss on test : 0.2427\n",
            "Epoch: 598, Loss: 0.1823, Loss on test : 0.2400\n",
            "Epoch: 599, Loss: 0.1932, Loss on test : 0.2371\n",
            "Epoch: 600, Loss: 0.1789, Loss on test : 0.2376\n",
            "Epoch: 601, Loss: 0.1887, Loss on test : 0.2439\n",
            "Epoch: 602, Loss: 0.1887, Loss on test : 0.2382\n",
            "Epoch: 603, Loss: 0.1824, Loss on test : 0.2417\n",
            "Epoch: 604, Loss: 0.1912, Loss on test : 0.2377\n",
            "Epoch: 605, Loss: 0.1837, Loss on test : 0.2377\n",
            "Epoch: 606, Loss: 0.1858, Loss on test : 0.2368\n",
            "Epoch: 607, Loss: 0.1832, Loss on test : 0.2358\n",
            "Epoch: 608, Loss: 0.1826, Loss on test : 0.2359\n",
            "Epoch: 609, Loss: 0.1813, Loss on test : 0.2406\n",
            "Epoch: 610, Loss: 0.1813, Loss on test : 0.2383\n",
            "Epoch: 611, Loss: 0.1819, Loss on test : 0.2374\n",
            "Epoch: 612, Loss: 0.1810, Loss on test : 0.2331\n",
            "Epoch: 613, Loss: 0.1822, Loss on test : 0.2425\n",
            "Epoch: 614, Loss: 0.1824, Loss on test : 0.2389\n",
            "Epoch: 615, Loss: 0.1858, Loss on test : 0.2381\n",
            "Epoch: 616, Loss: 0.1816, Loss on test : 0.2428\n",
            "Epoch: 617, Loss: 0.1795, Loss on test : 0.2383\n",
            "Epoch: 618, Loss: 0.1821, Loss on test : 0.2341\n",
            "Epoch: 619, Loss: 0.1762, Loss on test : 0.2348\n",
            "Epoch: 620, Loss: 0.1791, Loss on test : 0.2347\n",
            "Epoch: 621, Loss: 0.1773, Loss on test : 0.2309\n",
            "Epoch: 622, Loss: 0.1807, Loss on test : 0.2362\n",
            "Epoch: 623, Loss: 0.1796, Loss on test : 0.2371\n",
            "Epoch: 624, Loss: 0.1758, Loss on test : 0.2332\n",
            "Epoch: 625, Loss: 0.1743, Loss on test : 0.2324\n",
            "Epoch: 626, Loss: 0.1748, Loss on test : 0.2381\n",
            "Epoch: 627, Loss: 0.1775, Loss on test : 0.2349\n",
            "Epoch: 628, Loss: 0.1719, Loss on test : 0.2333\n",
            "Epoch: 629, Loss: 0.1766, Loss on test : 0.2337\n",
            "Epoch: 630, Loss: 0.1696, Loss on test : 0.2303\n",
            "Epoch: 631, Loss: 0.1776, Loss on test : 0.2290\n",
            "Epoch: 632, Loss: 0.1738, Loss on test : 0.2299\n",
            "Epoch: 633, Loss: 0.1704, Loss on test : 0.2359\n",
            "Epoch: 634, Loss: 0.1748, Loss on test : 0.2336\n",
            "Epoch: 635, Loss: 0.1762, Loss on test : 0.2345\n",
            "Epoch: 636, Loss: 0.1721, Loss on test : 0.2318\n",
            "Epoch: 637, Loss: 0.1753, Loss on test : 0.2362\n",
            "Epoch: 638, Loss: 0.1696, Loss on test : 0.2297\n",
            "Epoch: 639, Loss: 0.1724, Loss on test : 0.2293\n",
            "Epoch: 640, Loss: 0.1708, Loss on test : 0.2354\n",
            "Epoch: 641, Loss: 0.1748, Loss on test : 0.2315\n",
            "Epoch: 642, Loss: 0.1716, Loss on test : 0.2310\n",
            "Epoch: 643, Loss: 0.1731, Loss on test : 0.2272\n",
            "Epoch: 644, Loss: 0.1742, Loss on test : 0.2344\n",
            "Epoch: 645, Loss: 0.1711, Loss on test : 0.2316\n",
            "Epoch: 646, Loss: 0.1693, Loss on test : 0.2276\n",
            "Epoch: 647, Loss: 0.1744, Loss on test : 0.2339\n",
            "Epoch: 648, Loss: 0.1695, Loss on test : 0.2269\n",
            "Epoch: 649, Loss: 0.1681, Loss on test : 0.2348\n",
            "Epoch: 650, Loss: 0.1712, Loss on test : 0.2334\n",
            "Epoch: 651, Loss: 0.1679, Loss on test : 0.2347\n",
            "Epoch: 652, Loss: 0.1682, Loss on test : 0.2295\n",
            "Epoch: 653, Loss: 0.1700, Loss on test : 0.2320\n",
            "Epoch: 654, Loss: 0.1668, Loss on test : 0.2244\n",
            "Epoch: 655, Loss: 0.1694, Loss on test : 0.2340\n",
            "Epoch: 656, Loss: 0.1669, Loss on test : 0.2291\n",
            "Epoch: 657, Loss: 0.1658, Loss on test : 0.2255\n",
            "Epoch: 658, Loss: 0.1681, Loss on test : 0.2286\n",
            "Epoch: 659, Loss: 0.1639, Loss on test : 0.2299\n",
            "Epoch: 660, Loss: 0.1635, Loss on test : 0.2341\n",
            "Epoch: 661, Loss: 0.1680, Loss on test : 0.2256\n",
            "Epoch: 662, Loss: 0.1663, Loss on test : 0.2300\n",
            "Epoch: 663, Loss: 0.1652, Loss on test : 0.2265\n",
            "Epoch: 664, Loss: 0.1612, Loss on test : 0.2288\n",
            "Epoch: 665, Loss: 0.1671, Loss on test : 0.2237\n",
            "Epoch: 666, Loss: 0.1680, Loss on test : 0.2254\n",
            "Epoch: 667, Loss: 0.1631, Loss on test : 0.2270\n",
            "Epoch: 668, Loss: 0.1660, Loss on test : 0.2231\n",
            "Epoch: 669, Loss: 0.1616, Loss on test : 0.2305\n",
            "Epoch: 670, Loss: 0.1640, Loss on test : 0.2234\n",
            "Epoch: 671, Loss: 0.1644, Loss on test : 0.2328\n",
            "Epoch: 672, Loss: 0.1653, Loss on test : 0.2272\n",
            "Epoch: 673, Loss: 0.1658, Loss on test : 0.2292\n",
            "Epoch: 674, Loss: 0.1550, Loss on test : 0.2243\n",
            "Epoch: 675, Loss: 0.1640, Loss on test : 0.2276\n",
            "Epoch: 676, Loss: 0.1670, Loss on test : 0.2229\n",
            "Epoch: 677, Loss: 0.1612, Loss on test : 0.2289\n",
            "Epoch: 678, Loss: 0.1607, Loss on test : 0.2294\n",
            "Epoch: 679, Loss: 0.1603, Loss on test : 0.2245\n",
            "Epoch: 680, Loss: 0.1592, Loss on test : 0.2252\n",
            "Epoch: 681, Loss: 0.1647, Loss on test : 0.2311\n",
            "Epoch: 682, Loss: 0.1614, Loss on test : 0.2234\n",
            "Epoch: 683, Loss: 0.1556, Loss on test : 0.2251\n",
            "Epoch: 684, Loss: 0.1539, Loss on test : 0.2278\n",
            "Epoch: 685, Loss: 0.1635, Loss on test : 0.2255\n",
            "Epoch: 686, Loss: 0.1636, Loss on test : 0.2278\n",
            "Epoch: 687, Loss: 0.1569, Loss on test : 0.2265\n",
            "Epoch: 688, Loss: 0.1576, Loss on test : 0.2265\n",
            "Epoch: 689, Loss: 0.1560, Loss on test : 0.2269\n",
            "Epoch: 690, Loss: 0.1630, Loss on test : 0.2316\n",
            "Epoch: 691, Loss: 0.1536, Loss on test : 0.2269\n",
            "Epoch: 692, Loss: 0.1572, Loss on test : 0.2266\n",
            "Epoch: 693, Loss: 0.1617, Loss on test : 0.2216\n",
            "Epoch: 694, Loss: 0.1529, Loss on test : 0.2240\n",
            "Epoch: 695, Loss: 0.1586, Loss on test : 0.2237\n",
            "Epoch: 696, Loss: 0.1605, Loss on test : 0.2228\n",
            "Epoch: 697, Loss: 0.1604, Loss on test : 0.2250\n",
            "Epoch: 698, Loss: 0.1531, Loss on test : 0.2226\n",
            "Epoch: 699, Loss: 0.1552, Loss on test : 0.2201\n",
            "Epoch: 700, Loss: 0.1519, Loss on test : 0.2207\n",
            "Epoch: 701, Loss: 0.1590, Loss on test : 0.2199\n",
            "Epoch: 702, Loss: 0.1559, Loss on test : 0.2249\n",
            "Epoch: 703, Loss: 0.1565, Loss on test : 0.2193\n",
            "Epoch: 704, Loss: 0.1543, Loss on test : 0.2192\n",
            "Epoch: 705, Loss: 0.1563, Loss on test : 0.2198\n",
            "Epoch: 706, Loss: 0.1533, Loss on test : 0.2184\n",
            "Epoch: 707, Loss: 0.1538, Loss on test : 0.2239\n",
            "Epoch: 708, Loss: 0.1570, Loss on test : 0.2208\n",
            "Epoch: 709, Loss: 0.1510, Loss on test : 0.2202\n",
            "Epoch: 710, Loss: 0.1561, Loss on test : 0.2224\n",
            "Epoch: 711, Loss: 0.1528, Loss on test : 0.2201\n",
            "Epoch: 712, Loss: 0.1546, Loss on test : 0.2216\n",
            "Epoch: 713, Loss: 0.1546, Loss on test : 0.2194\n",
            "Epoch: 714, Loss: 0.1538, Loss on test : 0.2201\n",
            "Epoch: 715, Loss: 0.1526, Loss on test : 0.2228\n",
            "Epoch: 716, Loss: 0.1560, Loss on test : 0.2186\n",
            "Epoch: 717, Loss: 0.1551, Loss on test : 0.2219\n",
            "Epoch: 718, Loss: 0.1542, Loss on test : 0.2225\n",
            "Epoch: 719, Loss: 0.1543, Loss on test : 0.2223\n",
            "Epoch: 720, Loss: 0.1547, Loss on test : 0.2186\n",
            "Epoch: 721, Loss: 0.1501, Loss on test : 0.2222\n",
            "Epoch: 722, Loss: 0.1536, Loss on test : 0.2139\n",
            "Epoch: 723, Loss: 0.1499, Loss on test : 0.2185\n",
            "Epoch: 724, Loss: 0.1483, Loss on test : 0.2240\n",
            "Epoch: 725, Loss: 0.1472, Loss on test : 0.2272\n",
            "Epoch: 726, Loss: 0.1491, Loss on test : 0.2163\n",
            "Epoch: 727, Loss: 0.1469, Loss on test : 0.2214\n",
            "Epoch: 728, Loss: 0.1466, Loss on test : 0.2206\n",
            "Epoch: 729, Loss: 0.1475, Loss on test : 0.2138\n",
            "Epoch: 730, Loss: 0.1439, Loss on test : 0.2191\n",
            "Epoch: 731, Loss: 0.1479, Loss on test : 0.2158\n",
            "Epoch: 732, Loss: 0.1506, Loss on test : 0.2259\n",
            "Epoch: 733, Loss: 0.1472, Loss on test : 0.2192\n",
            "Epoch: 734, Loss: 0.1479, Loss on test : 0.2207\n",
            "Epoch: 735, Loss: 0.1469, Loss on test : 0.2203\n",
            "Epoch: 736, Loss: 0.1467, Loss on test : 0.2206\n",
            "Epoch: 737, Loss: 0.1452, Loss on test : 0.2206\n",
            "Epoch: 738, Loss: 0.1479, Loss on test : 0.2189\n",
            "Epoch: 739, Loss: 0.1427, Loss on test : 0.2155\n",
            "Epoch: 740, Loss: 0.1454, Loss on test : 0.2182\n",
            "Epoch: 741, Loss: 0.1444, Loss on test : 0.2179\n",
            "Epoch: 742, Loss: 0.1381, Loss on test : 0.2180\n",
            "Epoch: 743, Loss: 0.1462, Loss on test : 0.2187\n",
            "Epoch: 744, Loss: 0.1464, Loss on test : 0.2148\n",
            "Epoch: 745, Loss: 0.1460, Loss on test : 0.2163\n",
            "Epoch: 746, Loss: 0.1477, Loss on test : 0.2207\n",
            "Epoch: 747, Loss: 0.1438, Loss on test : 0.2158\n",
            "Epoch: 748, Loss: 0.1467, Loss on test : 0.2167\n",
            "Epoch: 749, Loss: 0.1431, Loss on test : 0.2151\n",
            "Epoch: 750, Loss: 0.1411, Loss on test : 0.2154\n",
            "Epoch: 751, Loss: 0.1470, Loss on test : 0.2224\n",
            "Epoch: 752, Loss: 0.1478, Loss on test : 0.2144\n",
            "Epoch: 753, Loss: 0.1466, Loss on test : 0.2148\n",
            "Epoch: 754, Loss: 0.1497, Loss on test : 0.2197\n",
            "Epoch: 755, Loss: 0.1428, Loss on test : 0.2201\n",
            "Epoch: 756, Loss: 0.1444, Loss on test : 0.2144\n",
            "Epoch: 757, Loss: 0.1432, Loss on test : 0.2182\n",
            "Epoch: 758, Loss: 0.1400, Loss on test : 0.2142\n",
            "Epoch: 759, Loss: 0.1455, Loss on test : 0.2153\n",
            "Epoch: 760, Loss: 0.1388, Loss on test : 0.2139\n",
            "Epoch: 761, Loss: 0.1442, Loss on test : 0.2203\n",
            "Epoch: 762, Loss: 0.1371, Loss on test : 0.2190\n",
            "Epoch: 763, Loss: 0.1389, Loss on test : 0.2169\n",
            "Epoch: 764, Loss: 0.1439, Loss on test : 0.2172\n",
            "Epoch: 765, Loss: 0.1378, Loss on test : 0.2163\n",
            "Epoch: 766, Loss: 0.1422, Loss on test : 0.2160\n",
            "Epoch: 767, Loss: 0.1348, Loss on test : 0.2226\n",
            "Epoch: 768, Loss: 0.1423, Loss on test : 0.2079\n",
            "Epoch: 769, Loss: 0.1431, Loss on test : 0.2167\n",
            "Epoch: 770, Loss: 0.1361, Loss on test : 0.2153\n",
            "Epoch: 771, Loss: 0.1372, Loss on test : 0.2186\n",
            "Epoch: 772, Loss: 0.1407, Loss on test : 0.2163\n",
            "Epoch: 773, Loss: 0.1413, Loss on test : 0.2170\n",
            "Epoch: 774, Loss: 0.1401, Loss on test : 0.2103\n",
            "Epoch: 775, Loss: 0.1410, Loss on test : 0.2153\n",
            "Epoch: 776, Loss: 0.1430, Loss on test : 0.2106\n",
            "Epoch: 777, Loss: 0.1411, Loss on test : 0.2165\n",
            "Epoch: 778, Loss: 0.1366, Loss on test : 0.2196\n",
            "Epoch: 779, Loss: 0.1407, Loss on test : 0.2180\n",
            "Epoch: 780, Loss: 0.1362, Loss on test : 0.2119\n",
            "Epoch: 781, Loss: 0.1362, Loss on test : 0.2114\n",
            "Epoch: 782, Loss: 0.1451, Loss on test : 0.2149\n",
            "Epoch: 783, Loss: 0.1392, Loss on test : 0.2102\n",
            "Epoch: 784, Loss: 0.1377, Loss on test : 0.2132\n",
            "Epoch: 785, Loss: 0.1383, Loss on test : 0.2151\n",
            "Epoch: 786, Loss: 0.1361, Loss on test : 0.2107\n",
            "Epoch: 787, Loss: 0.1377, Loss on test : 0.2145\n",
            "Epoch: 788, Loss: 0.1352, Loss on test : 0.2187\n",
            "Epoch: 789, Loss: 0.1386, Loss on test : 0.2143\n",
            "Epoch: 790, Loss: 0.1391, Loss on test : 0.2095\n",
            "Epoch: 791, Loss: 0.1361, Loss on test : 0.2130\n",
            "Epoch: 792, Loss: 0.1352, Loss on test : 0.2145\n",
            "Epoch: 793, Loss: 0.1397, Loss on test : 0.2102\n",
            "Epoch: 794, Loss: 0.1370, Loss on test : 0.2120\n",
            "Epoch: 795, Loss: 0.1351, Loss on test : 0.2189\n",
            "Epoch: 796, Loss: 0.1360, Loss on test : 0.2159\n",
            "Epoch: 797, Loss: 0.1372, Loss on test : 0.2152\n",
            "Epoch: 798, Loss: 0.1298, Loss on test : 0.2088\n",
            "Epoch: 799, Loss: 0.1349, Loss on test : 0.2169\n",
            "Epoch: 800, Loss: 0.1379, Loss on test : 0.2158\n",
            "Epoch: 801, Loss: 0.1346, Loss on test : 0.2107\n",
            "Epoch: 802, Loss: 0.1332, Loss on test : 0.2079\n",
            "Epoch: 803, Loss: 0.1300, Loss on test : 0.2148\n",
            "Epoch: 804, Loss: 0.1285, Loss on test : 0.2145\n",
            "Epoch: 805, Loss: 0.1369, Loss on test : 0.2146\n",
            "Epoch: 806, Loss: 0.1372, Loss on test : 0.2117\n",
            "Epoch: 807, Loss: 0.1370, Loss on test : 0.2130\n",
            "Epoch: 808, Loss: 0.1358, Loss on test : 0.2130\n",
            "Epoch: 809, Loss: 0.1302, Loss on test : 0.2132\n",
            "Epoch: 810, Loss: 0.1401, Loss on test : 0.2107\n",
            "Epoch: 811, Loss: 0.1288, Loss on test : 0.2117\n",
            "Epoch: 812, Loss: 0.1321, Loss on test : 0.2106\n",
            "Epoch: 813, Loss: 0.1315, Loss on test : 0.2159\n",
            "Epoch: 814, Loss: 0.1321, Loss on test : 0.2122\n",
            "Epoch: 815, Loss: 0.1339, Loss on test : 0.2120\n",
            "Epoch: 816, Loss: 0.1303, Loss on test : 0.2114\n",
            "Epoch: 817, Loss: 0.1309, Loss on test : 0.2136\n",
            "Epoch: 818, Loss: 0.1285, Loss on test : 0.2081\n",
            "Epoch: 819, Loss: 0.1315, Loss on test : 0.2146\n",
            "Epoch: 820, Loss: 0.1330, Loss on test : 0.2135\n",
            "Epoch: 821, Loss: 0.1308, Loss on test : 0.2148\n",
            "Epoch: 822, Loss: 0.1277, Loss on test : 0.2128\n",
            "Epoch: 823, Loss: 0.1285, Loss on test : 0.2043\n",
            "Epoch: 824, Loss: 0.1313, Loss on test : 0.2139\n",
            "Epoch: 825, Loss: 0.1310, Loss on test : 0.2174\n",
            "Epoch: 826, Loss: 0.1298, Loss on test : 0.2125\n",
            "Epoch: 827, Loss: 0.1350, Loss on test : 0.2121\n",
            "Epoch: 828, Loss: 0.1273, Loss on test : 0.2161\n",
            "Epoch: 829, Loss: 0.1330, Loss on test : 0.2092\n",
            "Epoch: 830, Loss: 0.1325, Loss on test : 0.2118\n",
            "Epoch: 831, Loss: 0.1311, Loss on test : 0.2100\n",
            "Epoch: 832, Loss: 0.1296, Loss on test : 0.2105\n",
            "Epoch: 833, Loss: 0.1323, Loss on test : 0.2137\n",
            "Epoch: 834, Loss: 0.1291, Loss on test : 0.2155\n",
            "Epoch: 835, Loss: 0.1325, Loss on test : 0.2088\n",
            "Epoch: 836, Loss: 0.1330, Loss on test : 0.2121\n",
            "Epoch: 837, Loss: 0.1284, Loss on test : 0.2104\n",
            "Epoch: 838, Loss: 0.1224, Loss on test : 0.2030\n",
            "Epoch: 839, Loss: 0.1312, Loss on test : 0.2121\n",
            "Epoch: 840, Loss: 0.1251, Loss on test : 0.2057\n",
            "Epoch: 841, Loss: 0.1271, Loss on test : 0.2102\n",
            "Epoch: 842, Loss: 0.1302, Loss on test : 0.2145\n",
            "Epoch: 843, Loss: 0.1285, Loss on test : 0.2118\n",
            "Epoch: 844, Loss: 0.1286, Loss on test : 0.2131\n",
            "Epoch: 845, Loss: 0.1297, Loss on test : 0.2148\n",
            "Epoch: 846, Loss: 0.1238, Loss on test : 0.2086\n",
            "Epoch: 847, Loss: 0.1269, Loss on test : 0.2134\n",
            "Epoch: 848, Loss: 0.1289, Loss on test : 0.2089\n",
            "Epoch: 849, Loss: 0.1246, Loss on test : 0.2087\n",
            "Epoch: 850, Loss: 0.1231, Loss on test : 0.2085\n",
            "Epoch: 851, Loss: 0.1250, Loss on test : 0.2100\n",
            "Epoch: 852, Loss: 0.1241, Loss on test : 0.2079\n",
            "Epoch: 853, Loss: 0.1264, Loss on test : 0.2096\n",
            "Epoch: 854, Loss: 0.1244, Loss on test : 0.2066\n",
            "Epoch: 855, Loss: 0.1356, Loss on test : 0.2137\n",
            "Epoch: 856, Loss: 0.1243, Loss on test : 0.2112\n",
            "Epoch: 857, Loss: 0.1198, Loss on test : 0.2141\n",
            "Epoch: 858, Loss: 0.1282, Loss on test : 0.2092\n",
            "Epoch: 859, Loss: 0.1215, Loss on test : 0.2074\n",
            "Epoch: 860, Loss: 0.1224, Loss on test : 0.2149\n",
            "Epoch: 861, Loss: 0.1298, Loss on test : 0.2097\n",
            "Epoch: 862, Loss: 0.1203, Loss on test : 0.2065\n",
            "Epoch: 863, Loss: 0.1258, Loss on test : 0.2065\n",
            "Epoch: 864, Loss: 0.1265, Loss on test : 0.2075\n",
            "Epoch: 865, Loss: 0.1249, Loss on test : 0.2068\n",
            "Epoch: 866, Loss: 0.1211, Loss on test : 0.2114\n",
            "Epoch: 867, Loss: 0.1210, Loss on test : 0.2089\n",
            "Epoch: 868, Loss: 0.1227, Loss on test : 0.2112\n",
            "Epoch: 869, Loss: 0.1249, Loss on test : 0.2085\n",
            "Epoch: 870, Loss: 0.1255, Loss on test : 0.2099\n",
            "Epoch: 871, Loss: 0.1209, Loss on test : 0.2102\n",
            "Epoch: 872, Loss: 0.1213, Loss on test : 0.2133\n",
            "Epoch: 873, Loss: 0.1188, Loss on test : 0.2076\n",
            "Epoch: 874, Loss: 0.1225, Loss on test : 0.2056\n",
            "Epoch: 875, Loss: 0.1238, Loss on test : 0.2138\n",
            "Epoch: 876, Loss: 0.1197, Loss on test : 0.2145\n",
            "Epoch: 877, Loss: 0.1271, Loss on test : 0.2062\n",
            "Epoch: 878, Loss: 0.1245, Loss on test : 0.2107\n",
            "Epoch: 879, Loss: 0.1272, Loss on test : 0.2135\n",
            "Epoch: 880, Loss: 0.1237, Loss on test : 0.2116\n",
            "Epoch: 881, Loss: 0.1241, Loss on test : 0.2093\n",
            "Epoch: 882, Loss: 0.1205, Loss on test : 0.2061\n",
            "Epoch: 883, Loss: 0.1250, Loss on test : 0.2095\n",
            "Epoch: 884, Loss: 0.1214, Loss on test : 0.2145\n",
            "Epoch: 885, Loss: 0.1214, Loss on test : 0.2105\n",
            "Epoch: 886, Loss: 0.1208, Loss on test : 0.2069\n",
            "Epoch: 887, Loss: 0.1194, Loss on test : 0.2060\n",
            "Epoch: 888, Loss: 0.1230, Loss on test : 0.2083\n",
            "Epoch: 889, Loss: 0.1218, Loss on test : 0.2114\n",
            "Epoch: 890, Loss: 0.1208, Loss on test : 0.2084\n",
            "Epoch: 891, Loss: 0.1217, Loss on test : 0.2075\n",
            "Epoch: 892, Loss: 0.1179, Loss on test : 0.2046\n",
            "Epoch: 893, Loss: 0.1219, Loss on test : 0.2108\n",
            "Epoch: 894, Loss: 0.1222, Loss on test : 0.2121\n",
            "Epoch: 895, Loss: 0.1164, Loss on test : 0.2102\n",
            "Epoch: 896, Loss: 0.1198, Loss on test : 0.2121\n",
            "Epoch: 897, Loss: 0.1191, Loss on test : 0.2100\n",
            "Epoch: 898, Loss: 0.1143, Loss on test : 0.2127\n",
            "Epoch: 899, Loss: 0.1189, Loss on test : 0.2055\n",
            "Epoch: 900, Loss: 0.1170, Loss on test : 0.2074\n",
            "Epoch: 901, Loss: 0.1163, Loss on test : 0.2110\n",
            "Epoch: 902, Loss: 0.1187, Loss on test : 0.2073\n",
            "Epoch: 903, Loss: 0.1186, Loss on test : 0.2112\n",
            "Epoch: 904, Loss: 0.1192, Loss on test : 0.2096\n",
            "Epoch: 905, Loss: 0.1203, Loss on test : 0.2056\n",
            "Epoch: 906, Loss: 0.1125, Loss on test : 0.2115\n",
            "Epoch: 907, Loss: 0.1147, Loss on test : 0.2094\n",
            "Epoch: 908, Loss: 0.1197, Loss on test : 0.2078\n",
            "Epoch: 909, Loss: 0.1212, Loss on test : 0.2115\n",
            "Epoch: 910, Loss: 0.1171, Loss on test : 0.2096\n",
            "Epoch: 911, Loss: 0.1172, Loss on test : 0.2104\n",
            "Epoch: 912, Loss: 0.1184, Loss on test : 0.2171\n",
            "Epoch: 913, Loss: 0.1217, Loss on test : 0.2105\n",
            "Epoch: 914, Loss: 0.1180, Loss on test : 0.2098\n",
            "Epoch: 915, Loss: 0.1180, Loss on test : 0.2011\n",
            "Epoch: 916, Loss: 0.1143, Loss on test : 0.2085\n",
            "Epoch: 917, Loss: 0.1188, Loss on test : 0.2116\n",
            "Epoch: 918, Loss: 0.1148, Loss on test : 0.2112\n",
            "Epoch: 919, Loss: 0.1208, Loss on test : 0.2056\n",
            "Epoch: 920, Loss: 0.1157, Loss on test : 0.2098\n",
            "Epoch: 921, Loss: 0.1157, Loss on test : 0.2067\n",
            "Epoch: 922, Loss: 0.1177, Loss on test : 0.2094\n",
            "Epoch: 923, Loss: 0.1236, Loss on test : 0.2116\n",
            "Epoch: 924, Loss: 0.1183, Loss on test : 0.2089\n",
            "Epoch: 925, Loss: 0.1213, Loss on test : 0.2133\n",
            "Epoch: 926, Loss: 0.1202, Loss on test : 0.2043\n",
            "Epoch: 927, Loss: 0.1177, Loss on test : 0.2085\n",
            "Epoch: 928, Loss: 0.1159, Loss on test : 0.2000\n",
            "Epoch: 929, Loss: 0.1162, Loss on test : 0.2074\n",
            "Epoch: 930, Loss: 0.1172, Loss on test : 0.2084\n",
            "Epoch: 931, Loss: 0.1168, Loss on test : 0.2074\n",
            "Epoch: 932, Loss: 0.1157, Loss on test : 0.2066\n",
            "Epoch: 933, Loss: 0.1151, Loss on test : 0.2093\n",
            "Epoch: 934, Loss: 0.1157, Loss on test : 0.2104\n",
            "Epoch: 935, Loss: 0.1136, Loss on test : 0.2148\n",
            "Epoch: 936, Loss: 0.1180, Loss on test : 0.2114\n",
            "Epoch: 937, Loss: 0.1136, Loss on test : 0.2078\n",
            "Epoch: 938, Loss: 0.1155, Loss on test : 0.2076\n",
            "Epoch: 939, Loss: 0.1117, Loss on test : 0.2098\n",
            "Epoch: 940, Loss: 0.1132, Loss on test : 0.2102\n",
            "Epoch: 941, Loss: 0.1154, Loss on test : 0.2113\n",
            "Epoch: 942, Loss: 0.1090, Loss on test : 0.2073\n",
            "Epoch: 943, Loss: 0.1148, Loss on test : 0.2057\n",
            "Epoch: 944, Loss: 0.1106, Loss on test : 0.2064\n",
            "Epoch: 945, Loss: 0.1153, Loss on test : 0.2136\n",
            "Epoch: 946, Loss: 0.1144, Loss on test : 0.2043\n",
            "Epoch: 947, Loss: 0.1130, Loss on test : 0.2088\n",
            "Epoch: 948, Loss: 0.1129, Loss on test : 0.2089\n",
            "Epoch: 949, Loss: 0.1158, Loss on test : 0.2088\n",
            "Epoch: 950, Loss: 0.1159, Loss on test : 0.2101\n",
            "Epoch: 951, Loss: 0.1138, Loss on test : 0.2111\n",
            "Epoch: 952, Loss: 0.1139, Loss on test : 0.2075\n",
            "Epoch: 953, Loss: 0.1095, Loss on test : 0.2059\n",
            "Epoch: 954, Loss: 0.1120, Loss on test : 0.2073\n",
            "Epoch: 955, Loss: 0.1152, Loss on test : 0.2029\n",
            "Epoch: 956, Loss: 0.1186, Loss on test : 0.2022\n",
            "Epoch: 957, Loss: 0.1150, Loss on test : 0.2046\n",
            "Epoch: 958, Loss: 0.1155, Loss on test : 0.2039\n",
            "Epoch: 959, Loss: 0.1150, Loss on test : 0.2082\n",
            "Epoch: 960, Loss: 0.1147, Loss on test : 0.2061\n",
            "Epoch: 961, Loss: 0.1113, Loss on test : 0.2097\n",
            "Epoch: 962, Loss: 0.1135, Loss on test : 0.2075\n",
            "Epoch: 963, Loss: 0.1147, Loss on test : 0.2072\n",
            "Epoch: 964, Loss: 0.1120, Loss on test : 0.2063\n",
            "Epoch: 965, Loss: 0.1123, Loss on test : 0.2087\n",
            "Epoch: 966, Loss: 0.1154, Loss on test : 0.2137\n",
            "Epoch: 967, Loss: 0.1090, Loss on test : 0.2062\n",
            "Epoch: 968, Loss: 0.1088, Loss on test : 0.2117\n",
            "Epoch: 969, Loss: 0.1079, Loss on test : 0.2085\n",
            "Epoch: 970, Loss: 0.1141, Loss on test : 0.2099\n",
            "Epoch: 971, Loss: 0.1115, Loss on test : 0.2123\n",
            "Epoch: 972, Loss: 0.1136, Loss on test : 0.2073\n",
            "Epoch: 973, Loss: 0.1054, Loss on test : 0.2034\n",
            "Epoch: 974, Loss: 0.1075, Loss on test : 0.2088\n",
            "Epoch: 975, Loss: 0.1082, Loss on test : 0.2052\n",
            "Epoch: 976, Loss: 0.1134, Loss on test : 0.2072\n",
            "Epoch: 977, Loss: 0.1111, Loss on test : 0.2038\n",
            "Epoch: 978, Loss: 0.1178, Loss on test : 0.2082\n",
            "Epoch: 979, Loss: 0.1083, Loss on test : 0.2078\n",
            "Epoch: 980, Loss: 0.1142, Loss on test : 0.2087\n",
            "Epoch: 981, Loss: 0.1044, Loss on test : 0.2059\n",
            "Epoch: 982, Loss: 0.1070, Loss on test : 0.2139\n",
            "Epoch: 983, Loss: 0.1128, Loss on test : 0.2087\n",
            "Epoch: 984, Loss: 0.1083, Loss on test : 0.2076\n",
            "Epoch: 985, Loss: 0.1112, Loss on test : 0.2067\n",
            "Epoch: 986, Loss: 0.1091, Loss on test : 0.2098\n",
            "Epoch: 987, Loss: 0.1038, Loss on test : 0.2068\n",
            "Epoch: 988, Loss: 0.1070, Loss on test : 0.2078\n",
            "Epoch: 989, Loss: 0.1146, Loss on test : 0.2082\n",
            "Epoch: 990, Loss: 0.1137, Loss on test : 0.2109\n",
            "Epoch: 991, Loss: 0.1096, Loss on test : 0.2049\n",
            "Epoch: 992, Loss: 0.1024, Loss on test : 0.2071\n",
            "Epoch: 993, Loss: 0.1038, Loss on test : 0.2093\n",
            "Epoch: 994, Loss: 0.1100, Loss on test : 0.2054\n",
            "Epoch: 995, Loss: 0.1051, Loss on test : 0.2077\n",
            "Epoch: 996, Loss: 0.1047, Loss on test : 0.2096\n",
            "Epoch: 997, Loss: 0.1099, Loss on test : 0.2026\n",
            "Epoch: 998, Loss: 0.1064, Loss on test : 0.2035\n",
            "Epoch: 999, Loss: 0.1031, Loss on test : 0.2084\n",
            "Epoch: 1000, Loss: 0.1089, Loss on test : 0.2101\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:03<00:00, 2449.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1000, Val Mean Rank: 85.98 Val MRR: 0.1841, Val Hits@10: 0.4478\n",
            "Save parameters...\n",
            "End\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m usual_complex, usual_complex_losses \u001b[38;5;241m=\u001b[39m train_and_test_complex(model\u001b[38;5;241m=\u001b[39musual_complex,\n\u001b[1;32m      2\u001b[0m                                              train_data\u001b[38;5;241m=\u001b[39mtrain_data, test_data\u001b[38;5;241m=\u001b[39mtest_data,\n\u001b[1;32m      3\u001b[0m                                              xp_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirst try pipeline with tail only complex\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                              epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                              eval_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                              reset_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                                              save_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                                              device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m      9\u001b[0m                                              use_wandb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                                              )\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "usual_complex, usual_complex_losses = train_and_test_complex(model=usual_complex,\n",
        "                                             train_data=train_data, test_data=test_data,\n",
        "                                             xp_name='First try pipeline with tail only complex',\n",
        "                                             epochs=1000,\n",
        "                                             eval_period=500,\n",
        "                                             reset_parameters=True,\n",
        "                                             save_params=False,\n",
        "                                             device=device,\n",
        "                                             use_wandb = False\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init loader...\n",
            "Init optimizer...\n",
            "Train...\n",
            "Epoch: 000, Loss: 0.6931\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:34<00:00, 263.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Val Mean Rank: 1607.70 Val MRR: 0.0033, Val Hits@10: 0.0040\n",
            "Epoch: 001, Loss: 0.6931\n",
            "Epoch: 002, Loss: 0.6931\n",
            "Epoch: 003, Loss: 0.6931\n",
            "Epoch: 004, Loss: 0.6931\n",
            "Epoch: 005, Loss: 0.6931\n",
            "Epoch: 006, Loss: 0.6931\n",
            "Epoch: 007, Loss: 0.6931\n",
            "Epoch: 008, Loss: 0.6931\n",
            "Epoch: 009, Loss: 0.6931\n",
            "Epoch: 010, Loss: 0.6931\n",
            "Epoch: 011, Loss: 0.6931\n",
            "Epoch: 012, Loss: 0.6931\n",
            "Epoch: 013, Loss: 0.6931\n",
            "Epoch: 014, Loss: 0.6931\n",
            "Epoch: 015, Loss: 0.6931\n",
            "Epoch: 016, Loss: 0.6931\n",
            "Epoch: 017, Loss: 0.6931\n",
            "Epoch: 018, Loss: 0.6931\n",
            "Epoch: 019, Loss: 0.6931\n",
            "Epoch: 020, Loss: 0.6930\n",
            "Epoch: 021, Loss: 0.6930\n",
            "Epoch: 022, Loss: 0.6930\n",
            "Epoch: 023, Loss: 0.6930\n",
            "Epoch: 024, Loss: 0.6930\n",
            "Epoch: 025, Loss: 0.6929\n",
            "Epoch: 026, Loss: 0.6929\n",
            "Epoch: 027, Loss: 0.6929\n",
            "Epoch: 028, Loss: 0.6928\n",
            "Epoch: 029, Loss: 0.6928\n",
            "Epoch: 030, Loss: 0.6927\n",
            "Epoch: 031, Loss: 0.6927\n",
            "Epoch: 032, Loss: 0.6926\n",
            "Epoch: 033, Loss: 0.6926\n",
            "Epoch: 034, Loss: 0.6925\n",
            "Epoch: 035, Loss: 0.6924\n",
            "Epoch: 036, Loss: 0.6923\n",
            "Epoch: 037, Loss: 0.6923\n",
            "Epoch: 038, Loss: 0.6922\n",
            "Epoch: 039, Loss: 0.6921\n",
            "Epoch: 040, Loss: 0.6919\n",
            "Epoch: 041, Loss: 0.6918\n",
            "Epoch: 042, Loss: 0.6917\n",
            "Epoch: 043, Loss: 0.6916\n",
            "Epoch: 044, Loss: 0.6914\n",
            "Epoch: 045, Loss: 0.6913\n",
            "Epoch: 046, Loss: 0.6911\n",
            "Epoch: 047, Loss: 0.6909\n",
            "Epoch: 048, Loss: 0.6907\n",
            "Epoch: 049, Loss: 0.6905\n",
            "Epoch: 050, Loss: 0.6903\n",
            "Epoch: 051, Loss: 0.6901\n",
            "Epoch: 052, Loss: 0.6898\n",
            "Epoch: 053, Loss: 0.6895\n",
            "Epoch: 054, Loss: 0.6892\n",
            "Epoch: 055, Loss: 0.6890\n",
            "Epoch: 056, Loss: 0.6886\n",
            "Epoch: 057, Loss: 0.6884\n",
            "Epoch: 058, Loss: 0.6879\n",
            "Epoch: 059, Loss: 0.6877\n",
            "Epoch: 060, Loss: 0.6873\n",
            "Epoch: 061, Loss: 0.6869\n",
            "Epoch: 062, Loss: 0.6864\n",
            "Epoch: 063, Loss: 0.6861\n",
            "Epoch: 064, Loss: 0.6855\n",
            "Epoch: 065, Loss: 0.6852\n",
            "Epoch: 066, Loss: 0.6846\n",
            "Epoch: 067, Loss: 0.6841\n",
            "Epoch: 068, Loss: 0.6836\n",
            "Epoch: 069, Loss: 0.6833\n",
            "Epoch: 070, Loss: 0.6825\n",
            "Epoch: 071, Loss: 0.6819\n",
            "Epoch: 072, Loss: 0.6813\n",
            "Epoch: 073, Loss: 0.6807\n",
            "Epoch: 074, Loss: 0.6801\n",
            "Epoch: 075, Loss: 0.6794\n",
            "Epoch: 076, Loss: 0.6787\n",
            "Epoch: 077, Loss: 0.6780\n",
            "Epoch: 078, Loss: 0.6774\n",
            "Epoch: 079, Loss: 0.6765\n",
            "Epoch: 080, Loss: 0.6757\n",
            "Epoch: 081, Loss: 0.6748\n",
            "Epoch: 082, Loss: 0.6740\n",
            "Epoch: 083, Loss: 0.6731\n",
            "Epoch: 084, Loss: 0.6724\n",
            "Epoch: 085, Loss: 0.6715\n",
            "Epoch: 086, Loss: 0.6705\n",
            "Epoch: 087, Loss: 0.6693\n",
            "Epoch: 088, Loss: 0.6688\n",
            "Epoch: 089, Loss: 0.6678\n",
            "Epoch: 090, Loss: 0.6665\n",
            "Epoch: 091, Loss: 0.6655\n",
            "Epoch: 092, Loss: 0.6645\n",
            "Epoch: 093, Loss: 0.6631\n",
            "Epoch: 094, Loss: 0.6628\n",
            "Epoch: 095, Loss: 0.6608\n",
            "Epoch: 096, Loss: 0.6605\n",
            "Epoch: 097, Loss: 0.6583\n",
            "Epoch: 098, Loss: 0.6572\n",
            "Epoch: 099, Loss: 0.6562\n",
            "Epoch: 100, Loss: 0.6552\n",
            "Epoch: 101, Loss: 0.6542\n",
            "Epoch: 102, Loss: 0.6522\n",
            "Epoch: 103, Loss: 0.6511\n",
            "Epoch: 104, Loss: 0.6493\n",
            "Epoch: 105, Loss: 0.6488\n",
            "Epoch: 106, Loss: 0.6468\n",
            "Epoch: 107, Loss: 0.6453\n",
            "Epoch: 108, Loss: 0.6446\n",
            "Epoch: 109, Loss: 0.6424\n",
            "Epoch: 110, Loss: 0.6421\n",
            "Epoch: 111, Loss: 0.6394\n",
            "Epoch: 112, Loss: 0.6374\n",
            "Epoch: 113, Loss: 0.6367\n",
            "Epoch: 114, Loss: 0.6347\n",
            "Epoch: 115, Loss: 0.6336\n",
            "Epoch: 116, Loss: 0.6328\n",
            "Epoch: 117, Loss: 0.6305\n",
            "Epoch: 118, Loss: 0.6298\n",
            "Epoch: 119, Loss: 0.6271\n",
            "Epoch: 120, Loss: 0.6265\n",
            "Epoch: 121, Loss: 0.6246\n",
            "Epoch: 122, Loss: 0.6224\n",
            "Epoch: 123, Loss: 0.6212\n",
            "Epoch: 124, Loss: 0.6182\n",
            "Epoch: 125, Loss: 0.6175\n",
            "Epoch: 126, Loss: 0.6164\n",
            "Epoch: 127, Loss: 0.6150\n",
            "Epoch: 128, Loss: 0.6129\n",
            "Epoch: 129, Loss: 0.6107\n",
            "Epoch: 130, Loss: 0.6099\n",
            "Epoch: 131, Loss: 0.6076\n",
            "Epoch: 132, Loss: 0.6067\n",
            "Epoch: 133, Loss: 0.6049\n",
            "Epoch: 134, Loss: 0.6036\n",
            "Epoch: 135, Loss: 0.6020\n",
            "Epoch: 136, Loss: 0.6001\n",
            "Epoch: 137, Loss: 0.5986\n",
            "Epoch: 138, Loss: 0.5945\n",
            "Epoch: 139, Loss: 0.5944\n",
            "Epoch: 140, Loss: 0.5936\n",
            "Epoch: 141, Loss: 0.5905\n",
            "Epoch: 142, Loss: 0.5895\n",
            "Epoch: 143, Loss: 0.5888\n",
            "Epoch: 144, Loss: 0.5840\n",
            "Epoch: 145, Loss: 0.5861\n",
            "Epoch: 146, Loss: 0.5834\n",
            "Epoch: 147, Loss: 0.5809\n",
            "Epoch: 148, Loss: 0.5804\n",
            "Epoch: 149, Loss: 0.5788\n",
            "Epoch: 150, Loss: 0.5761\n",
            "Epoch: 151, Loss: 0.5748\n",
            "Epoch: 152, Loss: 0.5736\n",
            "Epoch: 153, Loss: 0.5723\n",
            "Epoch: 154, Loss: 0.5699\n",
            "Epoch: 155, Loss: 0.5666\n",
            "Epoch: 156, Loss: 0.5671\n",
            "Epoch: 157, Loss: 0.5665\n",
            "Epoch: 158, Loss: 0.5653\n",
            "Epoch: 159, Loss: 0.5622\n",
            "Epoch: 160, Loss: 0.5613\n",
            "Epoch: 161, Loss: 0.5606\n",
            "Epoch: 162, Loss: 0.5591\n",
            "Epoch: 163, Loss: 0.5563\n",
            "Epoch: 164, Loss: 0.5530\n",
            "Epoch: 165, Loss: 0.5564\n",
            "Epoch: 166, Loss: 0.5512\n",
            "Epoch: 167, Loss: 0.5485\n",
            "Epoch: 168, Loss: 0.5494\n",
            "Epoch: 169, Loss: 0.5469\n",
            "Epoch: 170, Loss: 0.5493\n",
            "Epoch: 171, Loss: 0.5476\n",
            "Epoch: 172, Loss: 0.5443\n",
            "Epoch: 173, Loss: 0.5432\n",
            "Epoch: 174, Loss: 0.5446\n",
            "Epoch: 175, Loss: 0.5416\n",
            "Epoch: 176, Loss: 0.5385\n",
            "Epoch: 177, Loss: 0.5378\n",
            "Epoch: 178, Loss: 0.5391\n",
            "Epoch: 179, Loss: 0.5372\n",
            "Epoch: 180, Loss: 0.5384\n",
            "Epoch: 181, Loss: 0.5360\n",
            "Epoch: 182, Loss: 0.5321\n",
            "Epoch: 183, Loss: 0.5331\n",
            "Epoch: 184, Loss: 0.5301\n",
            "Epoch: 185, Loss: 0.5287\n",
            "Epoch: 186, Loss: 0.5264\n",
            "Epoch: 187, Loss: 0.5286\n",
            "Epoch: 188, Loss: 0.5264\n",
            "Epoch: 189, Loss: 0.5253\n",
            "Epoch: 190, Loss: 0.5242\n",
            "Epoch: 191, Loss: 0.5234\n",
            "Epoch: 192, Loss: 0.5257\n",
            "Epoch: 193, Loss: 0.5165\n",
            "Epoch: 194, Loss: 0.5190\n",
            "Epoch: 195, Loss: 0.5185\n",
            "Epoch: 196, Loss: 0.5189\n",
            "Epoch: 197, Loss: 0.5206\n",
            "Epoch: 198, Loss: 0.5197\n",
            "Epoch: 199, Loss: 0.5178\n",
            "Epoch: 200, Loss: 0.5148\n",
            "Epoch: 201, Loss: 0.5140\n",
            "Epoch: 202, Loss: 0.5184\n",
            "Epoch: 203, Loss: 0.5122\n",
            "Epoch: 204, Loss: 0.5127\n",
            "Epoch: 205, Loss: 0.5143\n",
            "Epoch: 206, Loss: 0.5092\n",
            "Epoch: 207, Loss: 0.5107\n",
            "Epoch: 208, Loss: 0.5088\n",
            "Epoch: 209, Loss: 0.5116\n",
            "Epoch: 210, Loss: 0.5075\n",
            "Epoch: 211, Loss: 0.5040\n",
            "Epoch: 212, Loss: 0.5078\n",
            "Epoch: 213, Loss: 0.5098\n",
            "Epoch: 214, Loss: 0.5089\n",
            "Epoch: 215, Loss: 0.5032\n",
            "Epoch: 216, Loss: 0.5053\n",
            "Epoch: 217, Loss: 0.5058\n",
            "Epoch: 218, Loss: 0.5049\n",
            "Epoch: 219, Loss: 0.5010\n",
            "Epoch: 220, Loss: 0.5062\n",
            "Epoch: 221, Loss: 0.5030\n",
            "Epoch: 222, Loss: 0.4993\n",
            "Epoch: 223, Loss: 0.5001\n",
            "Epoch: 224, Loss: 0.5027\n",
            "Epoch: 225, Loss: 0.5022\n",
            "Epoch: 226, Loss: 0.4988\n",
            "Epoch: 227, Loss: 0.4965\n",
            "Epoch: 228, Loss: 0.4972\n",
            "Epoch: 229, Loss: 0.5013\n",
            "Epoch: 230, Loss: 0.4946\n",
            "Epoch: 231, Loss: 0.4959\n",
            "Epoch: 232, Loss: 0.4962\n",
            "Epoch: 233, Loss: 0.4971\n",
            "Epoch: 234, Loss: 0.4931\n",
            "Epoch: 235, Loss: 0.4963\n",
            "Epoch: 236, Loss: 0.4944\n",
            "Epoch: 237, Loss: 0.4944\n",
            "Epoch: 238, Loss: 0.4917\n",
            "Epoch: 239, Loss: 0.4921\n",
            "Epoch: 240, Loss: 0.4939\n",
            "Epoch: 241, Loss: 0.4925\n",
            "Epoch: 242, Loss: 0.4919\n",
            "Epoch: 243, Loss: 0.4922\n",
            "Epoch: 244, Loss: 0.4906\n",
            "Epoch: 245, Loss: 0.4915\n",
            "Epoch: 246, Loss: 0.4890\n",
            "Epoch: 247, Loss: 0.4908\n",
            "Epoch: 248, Loss: 0.4919\n",
            "Epoch: 249, Loss: 0.4889\n",
            "Epoch: 250, Loss: 0.4891\n",
            "Epoch: 251, Loss: 0.4885\n",
            "Epoch: 252, Loss: 0.4906\n",
            "Epoch: 253, Loss: 0.4810\n",
            "Epoch: 254, Loss: 0.4881\n",
            "Epoch: 255, Loss: 0.4926\n",
            "Epoch: 256, Loss: 0.4847\n",
            "Epoch: 257, Loss: 0.4891\n",
            "Epoch: 258, Loss: 0.4892\n",
            "Epoch: 259, Loss: 0.4883\n",
            "Epoch: 260, Loss: 0.4832\n",
            "Epoch: 261, Loss: 0.4846\n",
            "Epoch: 262, Loss: 0.4855\n",
            "Epoch: 263, Loss: 0.4859\n",
            "Epoch: 264, Loss: 0.4861\n",
            "Epoch: 265, Loss: 0.4871\n",
            "Epoch: 266, Loss: 0.4877\n",
            "Epoch: 267, Loss: 0.4860\n",
            "Epoch: 268, Loss: 0.4862\n",
            "Epoch: 269, Loss: 0.4828\n",
            "Epoch: 270, Loss: 0.4806\n",
            "Epoch: 271, Loss: 0.4856\n",
            "Epoch: 272, Loss: 0.4809\n",
            "Epoch: 273, Loss: 0.4791\n",
            "Epoch: 274, Loss: 0.4813\n",
            "Epoch: 275, Loss: 0.4820\n",
            "Epoch: 276, Loss: 0.4782\n",
            "Epoch: 277, Loss: 0.4779\n",
            "Epoch: 278, Loss: 0.4833\n",
            "Epoch: 279, Loss: 0.4804\n",
            "Epoch: 280, Loss: 0.4811\n",
            "Epoch: 281, Loss: 0.4787\n",
            "Epoch: 282, Loss: 0.4785\n",
            "Epoch: 283, Loss: 0.4768\n",
            "Epoch: 284, Loss: 0.4779\n",
            "Epoch: 285, Loss: 0.4786\n",
            "Epoch: 286, Loss: 0.4829\n",
            "Epoch: 287, Loss: 0.4748\n",
            "Epoch: 288, Loss: 0.4750\n",
            "Epoch: 289, Loss: 0.4786\n",
            "Epoch: 290, Loss: 0.4805\n",
            "Epoch: 291, Loss: 0.4741\n",
            "Epoch: 292, Loss: 0.4773\n",
            "Epoch: 293, Loss: 0.4788\n",
            "Epoch: 294, Loss: 0.4727\n",
            "Epoch: 295, Loss: 0.4717\n",
            "Epoch: 296, Loss: 0.4755\n",
            "Epoch: 297, Loss: 0.4761\n",
            "Epoch: 298, Loss: 0.4742\n",
            "Epoch: 299, Loss: 0.4719\n",
            "Epoch: 300, Loss: 0.4759\n",
            "Epoch: 301, Loss: 0.4691\n",
            "Epoch: 302, Loss: 0.4720\n",
            "Epoch: 303, Loss: 0.4758\n",
            "Epoch: 304, Loss: 0.4685\n",
            "Epoch: 305, Loss: 0.4759\n",
            "Epoch: 306, Loss: 0.4683\n",
            "Epoch: 307, Loss: 0.4766\n",
            "Epoch: 308, Loss: 0.4769\n",
            "Epoch: 309, Loss: 0.4723\n",
            "Epoch: 310, Loss: 0.4724\n",
            "Epoch: 311, Loss: 0.4745\n",
            "Epoch: 312, Loss: 0.4786\n",
            "Epoch: 313, Loss: 0.4753\n",
            "Epoch: 314, Loss: 0.4741\n",
            "Epoch: 315, Loss: 0.4723\n",
            "Epoch: 316, Loss: 0.4713\n",
            "Epoch: 317, Loss: 0.4721\n",
            "Epoch: 318, Loss: 0.4664\n",
            "Epoch: 319, Loss: 0.4709\n",
            "Epoch: 320, Loss: 0.4700\n",
            "Epoch: 321, Loss: 0.4722\n",
            "Epoch: 322, Loss: 0.4663\n",
            "Epoch: 323, Loss: 0.4732\n",
            "Epoch: 324, Loss: 0.4730\n",
            "Epoch: 325, Loss: 0.4714\n",
            "Epoch: 326, Loss: 0.4717\n",
            "Epoch: 327, Loss: 0.4687\n",
            "Epoch: 328, Loss: 0.4715\n",
            "Epoch: 329, Loss: 0.4688\n",
            "Epoch: 330, Loss: 0.4678\n",
            "Epoch: 331, Loss: 0.4687\n",
            "Epoch: 332, Loss: 0.4669\n",
            "Epoch: 333, Loss: 0.4649\n",
            "Epoch: 334, Loss: 0.4674\n",
            "Epoch: 335, Loss: 0.4703\n",
            "Epoch: 336, Loss: 0.4670\n",
            "Epoch: 337, Loss: 0.4624\n",
            "Epoch: 338, Loss: 0.4664\n",
            "Epoch: 339, Loss: 0.4645\n",
            "Epoch: 340, Loss: 0.4617\n",
            "Epoch: 341, Loss: 0.4636\n",
            "Epoch: 342, Loss: 0.4615\n",
            "Epoch: 343, Loss: 0.4597\n",
            "Epoch: 344, Loss: 0.4680\n",
            "Epoch: 345, Loss: 0.4692\n",
            "Epoch: 346, Loss: 0.4680\n",
            "Epoch: 347, Loss: 0.4670\n",
            "Epoch: 348, Loss: 0.4639\n",
            "Epoch: 349, Loss: 0.4634\n",
            "Epoch: 350, Loss: 0.4671\n",
            "Epoch: 351, Loss: 0.4599\n",
            "Epoch: 352, Loss: 0.4649\n",
            "Epoch: 353, Loss: 0.4626\n",
            "Epoch: 354, Loss: 0.4647\n",
            "Epoch: 355, Loss: 0.4687\n",
            "Epoch: 356, Loss: 0.4658\n",
            "Epoch: 357, Loss: 0.4582\n",
            "Epoch: 358, Loss: 0.4644\n",
            "Epoch: 359, Loss: 0.4641\n",
            "Epoch: 360, Loss: 0.4674\n",
            "Epoch: 361, Loss: 0.4616\n",
            "Epoch: 362, Loss: 0.4602\n",
            "Epoch: 363, Loss: 0.4617\n",
            "Epoch: 364, Loss: 0.4600\n",
            "Epoch: 365, Loss: 0.4574\n",
            "Epoch: 366, Loss: 0.4590\n",
            "Epoch: 367, Loss: 0.4598\n",
            "Epoch: 368, Loss: 0.4559\n",
            "Epoch: 369, Loss: 0.4623\n",
            "Epoch: 370, Loss: 0.4579\n",
            "Epoch: 371, Loss: 0.4605\n",
            "Epoch: 372, Loss: 0.4625\n",
            "Epoch: 373, Loss: 0.4583\n",
            "Epoch: 374, Loss: 0.4624\n",
            "Epoch: 375, Loss: 0.4564\n",
            "Epoch: 376, Loss: 0.4548\n",
            "Epoch: 377, Loss: 0.4601\n",
            "Epoch: 378, Loss: 0.4550\n",
            "Epoch: 379, Loss: 0.4578\n",
            "Epoch: 380, Loss: 0.4555\n",
            "Epoch: 381, Loss: 0.4576\n",
            "Epoch: 382, Loss: 0.4545\n",
            "Epoch: 383, Loss: 0.4515\n",
            "Epoch: 384, Loss: 0.4604\n",
            "Epoch: 385, Loss: 0.4558\n",
            "Epoch: 386, Loss: 0.4530\n",
            "Epoch: 387, Loss: 0.4565\n",
            "Epoch: 388, Loss: 0.4514\n",
            "Epoch: 389, Loss: 0.4555\n",
            "Epoch: 390, Loss: 0.4557\n",
            "Epoch: 391, Loss: 0.4577\n",
            "Epoch: 392, Loss: 0.4523\n",
            "Epoch: 393, Loss: 0.4571\n",
            "Epoch: 394, Loss: 0.4546\n",
            "Epoch: 395, Loss: 0.4512\n",
            "Epoch: 396, Loss: 0.4488\n",
            "Epoch: 397, Loss: 0.4520\n",
            "Epoch: 398, Loss: 0.4575\n",
            "Epoch: 399, Loss: 0.4525\n",
            "Epoch: 400, Loss: 0.4486\n",
            "Epoch: 401, Loss: 0.4502\n",
            "Epoch: 402, Loss: 0.4505\n",
            "Epoch: 403, Loss: 0.4507\n",
            "Epoch: 404, Loss: 0.4471\n",
            "Epoch: 405, Loss: 0.4499\n",
            "Epoch: 406, Loss: 0.4482\n",
            "Epoch: 407, Loss: 0.4468\n",
            "Epoch: 408, Loss: 0.4463\n",
            "Epoch: 409, Loss: 0.4446\n",
            "Epoch: 410, Loss: 0.4534\n",
            "Epoch: 411, Loss: 0.4525\n",
            "Epoch: 412, Loss: 0.4532\n",
            "Epoch: 413, Loss: 0.4467\n",
            "Epoch: 414, Loss: 0.4463\n",
            "Epoch: 415, Loss: 0.4448\n",
            "Epoch: 416, Loss: 0.4519\n",
            "Epoch: 417, Loss: 0.4450\n",
            "Epoch: 418, Loss: 0.4521\n",
            "Epoch: 419, Loss: 0.4511\n",
            "Epoch: 420, Loss: 0.4460\n",
            "Epoch: 421, Loss: 0.4456\n",
            "Epoch: 422, Loss: 0.4438\n",
            "Epoch: 423, Loss: 0.4470\n",
            "Epoch: 424, Loss: 0.4385\n",
            "Epoch: 425, Loss: 0.4491\n",
            "Epoch: 426, Loss: 0.4410\n",
            "Epoch: 427, Loss: 0.4430\n",
            "Epoch: 428, Loss: 0.4378\n",
            "Epoch: 429, Loss: 0.4444\n",
            "Epoch: 430, Loss: 0.4427\n",
            "Epoch: 431, Loss: 0.4378\n",
            "Epoch: 432, Loss: 0.4390\n",
            "Epoch: 433, Loss: 0.4424\n",
            "Epoch: 434, Loss: 0.4419\n",
            "Epoch: 435, Loss: 0.4434\n",
            "Epoch: 436, Loss: 0.4389\n",
            "Epoch: 437, Loss: 0.4357\n",
            "Epoch: 438, Loss: 0.4415\n",
            "Epoch: 439, Loss: 0.4365\n",
            "Epoch: 440, Loss: 0.4438\n",
            "Epoch: 441, Loss: 0.4429\n",
            "Epoch: 442, Loss: 0.4408\n",
            "Epoch: 443, Loss: 0.4358\n",
            "Epoch: 444, Loss: 0.4416\n",
            "Epoch: 445, Loss: 0.4398\n",
            "Epoch: 446, Loss: 0.4342\n",
            "Epoch: 447, Loss: 0.4341\n",
            "Epoch: 448, Loss: 0.4337\n",
            "Epoch: 449, Loss: 0.4361\n",
            "Epoch: 450, Loss: 0.4365\n",
            "Epoch: 451, Loss: 0.4333\n",
            "Epoch: 452, Loss: 0.4349\n",
            "Epoch: 453, Loss: 0.4349\n",
            "Epoch: 454, Loss: 0.4284\n",
            "Epoch: 455, Loss: 0.4365\n",
            "Epoch: 456, Loss: 0.4293\n",
            "Epoch: 457, Loss: 0.4297\n",
            "Epoch: 458, Loss: 0.4341\n",
            "Epoch: 459, Loss: 0.4325\n",
            "Epoch: 460, Loss: 0.4252\n",
            "Epoch: 461, Loss: 0.4305\n",
            "Epoch: 462, Loss: 0.4344\n",
            "Epoch: 463, Loss: 0.4311\n",
            "Epoch: 464, Loss: 0.4289\n",
            "Epoch: 465, Loss: 0.4303\n",
            "Epoch: 466, Loss: 0.4294\n",
            "Epoch: 467, Loss: 0.4230\n",
            "Epoch: 468, Loss: 0.4275\n",
            "Epoch: 469, Loss: 0.4284\n",
            "Epoch: 470, Loss: 0.4242\n",
            "Epoch: 471, Loss: 0.4241\n",
            "Epoch: 472, Loss: 0.4275\n",
            "Epoch: 473, Loss: 0.4244\n",
            "Epoch: 474, Loss: 0.4304\n",
            "Epoch: 475, Loss: 0.4258\n",
            "Epoch: 476, Loss: 0.4217\n",
            "Epoch: 477, Loss: 0.4230\n",
            "Epoch: 478, Loss: 0.4170\n",
            "Epoch: 479, Loss: 0.4217\n",
            "Epoch: 480, Loss: 0.4241\n",
            "Epoch: 481, Loss: 0.4239\n",
            "Epoch: 482, Loss: 0.4172\n",
            "Epoch: 483, Loss: 0.4158\n",
            "Epoch: 484, Loss: 0.4204\n",
            "Epoch: 485, Loss: 0.4178\n",
            "Epoch: 486, Loss: 0.4183\n",
            "Epoch: 487, Loss: 0.4204\n",
            "Epoch: 488, Loss: 0.4237\n",
            "Epoch: 489, Loss: 0.4156\n",
            "Epoch: 490, Loss: 0.4167\n",
            "Epoch: 491, Loss: 0.4166\n",
            "Epoch: 492, Loss: 0.4154\n",
            "Epoch: 493, Loss: 0.4210\n",
            "Epoch: 494, Loss: 0.4211\n",
            "Epoch: 495, Loss: 0.4164\n",
            "Epoch: 496, Loss: 0.4181\n",
            "Epoch: 497, Loss: 0.4138\n",
            "Epoch: 498, Loss: 0.4128\n",
            "Epoch: 499, Loss: 0.4114\n",
            "Epoch: 500, Loss: 0.4120\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:34<00:00, 264.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 500, Val Mean Rank: 237.64 Val MRR: 0.1484, Val Hits@10: 0.3301\n",
            "Epoch: 501, Loss: 0.4124\n",
            "Epoch: 502, Loss: 0.4090\n",
            "Epoch: 503, Loss: 0.4060\n",
            "Epoch: 504, Loss: 0.4127\n",
            "Epoch: 505, Loss: 0.4097\n",
            "Epoch: 506, Loss: 0.4074\n",
            "Epoch: 507, Loss: 0.4107\n",
            "Epoch: 508, Loss: 0.4070\n",
            "Epoch: 509, Loss: 0.4096\n",
            "Epoch: 510, Loss: 0.4067\n",
            "Epoch: 511, Loss: 0.4076\n",
            "Epoch: 512, Loss: 0.4009\n",
            "Epoch: 513, Loss: 0.4013\n",
            "Epoch: 514, Loss: 0.4078\n",
            "Epoch: 515, Loss: 0.3988\n",
            "Epoch: 516, Loss: 0.4064\n",
            "Epoch: 517, Loss: 0.4047\n",
            "Epoch: 518, Loss: 0.4056\n",
            "Epoch: 519, Loss: 0.4017\n",
            "Epoch: 520, Loss: 0.4022\n",
            "Epoch: 521, Loss: 0.4005\n",
            "Epoch: 522, Loss: 0.4017\n",
            "Epoch: 523, Loss: 0.3995\n",
            "Epoch: 524, Loss: 0.3987\n",
            "Epoch: 525, Loss: 0.3998\n",
            "Epoch: 526, Loss: 0.4005\n",
            "Epoch: 527, Loss: 0.3946\n",
            "Epoch: 528, Loss: 0.3980\n",
            "Epoch: 529, Loss: 0.3959\n",
            "Epoch: 530, Loss: 0.3966\n",
            "Epoch: 531, Loss: 0.3957\n",
            "Epoch: 532, Loss: 0.3940\n",
            "Epoch: 533, Loss: 0.3979\n",
            "Epoch: 534, Loss: 0.3979\n",
            "Epoch: 535, Loss: 0.3871\n",
            "Epoch: 536, Loss: 0.3904\n",
            "Epoch: 537, Loss: 0.3970\n",
            "Epoch: 538, Loss: 0.3922\n",
            "Epoch: 539, Loss: 0.3927\n",
            "Epoch: 540, Loss: 0.3904\n",
            "Epoch: 541, Loss: 0.3945\n",
            "Epoch: 542, Loss: 0.3910\n",
            "Epoch: 543, Loss: 0.3880\n",
            "Epoch: 544, Loss: 0.3853\n",
            "Epoch: 545, Loss: 0.3875\n",
            "Epoch: 546, Loss: 0.3862\n",
            "Epoch: 547, Loss: 0.3905\n",
            "Epoch: 548, Loss: 0.3906\n",
            "Epoch: 549, Loss: 0.3824\n",
            "Epoch: 550, Loss: 0.3910\n",
            "Epoch: 551, Loss: 0.3809\n",
            "Epoch: 552, Loss: 0.3816\n",
            "Epoch: 553, Loss: 0.3840\n",
            "Epoch: 554, Loss: 0.3821\n",
            "Epoch: 555, Loss: 0.3881\n",
            "Epoch: 556, Loss: 0.3798\n",
            "Epoch: 557, Loss: 0.3809\n",
            "Epoch: 558, Loss: 0.3812\n",
            "Epoch: 559, Loss: 0.3795\n",
            "Epoch: 560, Loss: 0.3789\n",
            "Epoch: 561, Loss: 0.3842\n",
            "Epoch: 562, Loss: 0.3778\n",
            "Epoch: 563, Loss: 0.3738\n",
            "Epoch: 564, Loss: 0.3794\n",
            "Epoch: 565, Loss: 0.3751\n",
            "Epoch: 566, Loss: 0.3717\n",
            "Epoch: 567, Loss: 0.3772\n",
            "Epoch: 568, Loss: 0.3778\n",
            "Epoch: 569, Loss: 0.3734\n",
            "Epoch: 570, Loss: 0.3723\n",
            "Epoch: 571, Loss: 0.3771\n",
            "Epoch: 572, Loss: 0.3737\n",
            "Epoch: 573, Loss: 0.3692\n",
            "Epoch: 574, Loss: 0.3692\n",
            "Epoch: 575, Loss: 0.3671\n",
            "Epoch: 576, Loss: 0.3661\n",
            "Epoch: 577, Loss: 0.3688\n",
            "Epoch: 578, Loss: 0.3679\n",
            "Epoch: 579, Loss: 0.3668\n",
            "Epoch: 580, Loss: 0.3668\n",
            "Epoch: 581, Loss: 0.3649\n",
            "Epoch: 582, Loss: 0.3642\n",
            "Epoch: 583, Loss: 0.3690\n",
            "Epoch: 584, Loss: 0.3681\n",
            "Epoch: 585, Loss: 0.3600\n",
            "Epoch: 586, Loss: 0.3626\n",
            "Epoch: 587, Loss: 0.3632\n",
            "Epoch: 588, Loss: 0.3662\n",
            "Epoch: 589, Loss: 0.3668\n",
            "Epoch: 590, Loss: 0.3637\n",
            "Epoch: 591, Loss: 0.3564\n",
            "Epoch: 592, Loss: 0.3595\n",
            "Epoch: 593, Loss: 0.3586\n",
            "Epoch: 594, Loss: 0.3555\n",
            "Epoch: 595, Loss: 0.3676\n",
            "Epoch: 596, Loss: 0.3569\n",
            "Epoch: 597, Loss: 0.3568\n",
            "Epoch: 598, Loss: 0.3604\n",
            "Epoch: 599, Loss: 0.3578\n",
            "Epoch: 600, Loss: 0.3538\n",
            "Epoch: 601, Loss: 0.3548\n",
            "Epoch: 602, Loss: 0.3519\n",
            "Epoch: 603, Loss: 0.3526\n",
            "Epoch: 604, Loss: 0.3572\n",
            "Epoch: 605, Loss: 0.3524\n",
            "Epoch: 606, Loss: 0.3520\n",
            "Epoch: 607, Loss: 0.3462\n",
            "Epoch: 608, Loss: 0.3534\n",
            "Epoch: 609, Loss: 0.3517\n",
            "Epoch: 610, Loss: 0.3550\n",
            "Epoch: 611, Loss: 0.3512\n",
            "Epoch: 612, Loss: 0.3508\n",
            "Epoch: 613, Loss: 0.3477\n",
            "Epoch: 614, Loss: 0.3447\n",
            "Epoch: 615, Loss: 0.3439\n",
            "Epoch: 616, Loss: 0.3492\n",
            "Epoch: 617, Loss: 0.3442\n",
            "Epoch: 618, Loss: 0.3426\n",
            "Epoch: 619, Loss: 0.3418\n",
            "Epoch: 620, Loss: 0.3451\n",
            "Epoch: 621, Loss: 0.3423\n",
            "Epoch: 622, Loss: 0.3393\n",
            "Epoch: 623, Loss: 0.3484\n",
            "Epoch: 624, Loss: 0.3349\n",
            "Epoch: 625, Loss: 0.3399\n",
            "Epoch: 626, Loss: 0.3435\n",
            "Epoch: 627, Loss: 0.3384\n",
            "Epoch: 628, Loss: 0.3357\n",
            "Epoch: 629, Loss: 0.3465\n",
            "Epoch: 630, Loss: 0.3372\n",
            "Epoch: 631, Loss: 0.3370\n",
            "Epoch: 632, Loss: 0.3346\n",
            "Epoch: 633, Loss: 0.3317\n",
            "Epoch: 634, Loss: 0.3314\n",
            "Epoch: 635, Loss: 0.3382\n",
            "Epoch: 636, Loss: 0.3394\n",
            "Epoch: 637, Loss: 0.3314\n",
            "Epoch: 638, Loss: 0.3386\n",
            "Epoch: 639, Loss: 0.3368\n",
            "Epoch: 640, Loss: 0.3350\n",
            "Epoch: 641, Loss: 0.3332\n",
            "Epoch: 642, Loss: 0.3331\n",
            "Epoch: 643, Loss: 0.3285\n",
            "Epoch: 644, Loss: 0.3251\n",
            "Epoch: 645, Loss: 0.3317\n",
            "Epoch: 646, Loss: 0.3262\n",
            "Epoch: 647, Loss: 0.3332\n",
            "Epoch: 648, Loss: 0.3322\n",
            "Epoch: 649, Loss: 0.3322\n",
            "Epoch: 650, Loss: 0.3327\n",
            "Epoch: 651, Loss: 0.3276\n",
            "Epoch: 652, Loss: 0.3273\n",
            "Epoch: 653, Loss: 0.3349\n",
            "Epoch: 654, Loss: 0.3257\n",
            "Epoch: 655, Loss: 0.3259\n",
            "Epoch: 656, Loss: 0.3295\n",
            "Epoch: 657, Loss: 0.3195\n",
            "Epoch: 658, Loss: 0.3252\n",
            "Epoch: 659, Loss: 0.3226\n",
            "Epoch: 660, Loss: 0.3226\n",
            "Epoch: 661, Loss: 0.3241\n",
            "Epoch: 662, Loss: 0.3212\n",
            "Epoch: 663, Loss: 0.3212\n",
            "Epoch: 664, Loss: 0.3185\n",
            "Epoch: 665, Loss: 0.3245\n",
            "Epoch: 666, Loss: 0.3168\n",
            "Epoch: 667, Loss: 0.3224\n",
            "Epoch: 668, Loss: 0.3212\n",
            "Epoch: 669, Loss: 0.3202\n",
            "Epoch: 670, Loss: 0.3202\n",
            "Epoch: 671, Loss: 0.3181\n",
            "Epoch: 672, Loss: 0.3174\n",
            "Epoch: 673, Loss: 0.3148\n",
            "Epoch: 674, Loss: 0.3151\n",
            "Epoch: 675, Loss: 0.3158\n",
            "Epoch: 676, Loss: 0.3143\n",
            "Epoch: 677, Loss: 0.3157\n",
            "Epoch: 678, Loss: 0.3142\n",
            "Epoch: 679, Loss: 0.3132\n",
            "Epoch: 680, Loss: 0.3130\n",
            "Epoch: 681, Loss: 0.3135\n",
            "Epoch: 682, Loss: 0.3118\n",
            "Epoch: 683, Loss: 0.3091\n",
            "Epoch: 684, Loss: 0.3078\n",
            "Epoch: 685, Loss: 0.3114\n",
            "Epoch: 686, Loss: 0.3062\n",
            "Epoch: 687, Loss: 0.3080\n",
            "Epoch: 688, Loss: 0.3116\n",
            "Epoch: 689, Loss: 0.3054\n",
            "Epoch: 690, Loss: 0.3061\n",
            "Epoch: 691, Loss: 0.3037\n",
            "Epoch: 692, Loss: 0.3084\n",
            "Epoch: 693, Loss: 0.3076\n",
            "Epoch: 694, Loss: 0.3047\n",
            "Epoch: 695, Loss: 0.3003\n",
            "Epoch: 696, Loss: 0.3047\n",
            "Epoch: 697, Loss: 0.3037\n",
            "Epoch: 698, Loss: 0.3052\n",
            "Epoch: 699, Loss: 0.3072\n",
            "Epoch: 700, Loss: 0.3033\n",
            "Epoch: 701, Loss: 0.3013\n",
            "Epoch: 702, Loss: 0.3061\n",
            "Epoch: 703, Loss: 0.3034\n",
            "Epoch: 704, Loss: 0.3012\n",
            "Epoch: 705, Loss: 0.3010\n",
            "Epoch: 706, Loss: 0.2996\n",
            "Epoch: 707, Loss: 0.3002\n",
            "Epoch: 708, Loss: 0.3040\n",
            "Epoch: 709, Loss: 0.3004\n",
            "Epoch: 710, Loss: 0.2916\n",
            "Epoch: 711, Loss: 0.2944\n",
            "Epoch: 712, Loss: 0.3017\n",
            "Epoch: 713, Loss: 0.2996\n",
            "Epoch: 714, Loss: 0.2930\n",
            "Epoch: 715, Loss: 0.2982\n",
            "Epoch: 716, Loss: 0.2941\n",
            "Epoch: 717, Loss: 0.2948\n",
            "Epoch: 718, Loss: 0.2880\n",
            "Epoch: 719, Loss: 0.2908\n",
            "Epoch: 720, Loss: 0.2967\n",
            "Epoch: 721, Loss: 0.2973\n",
            "Epoch: 722, Loss: 0.2942\n",
            "Epoch: 723, Loss: 0.2885\n",
            "Epoch: 724, Loss: 0.2920\n",
            "Epoch: 725, Loss: 0.2894\n",
            "Epoch: 726, Loss: 0.2987\n",
            "Epoch: 727, Loss: 0.2885\n",
            "Epoch: 728, Loss: 0.2902\n",
            "Epoch: 729, Loss: 0.2915\n",
            "Epoch: 730, Loss: 0.2919\n",
            "Epoch: 731, Loss: 0.2847\n",
            "Epoch: 732, Loss: 0.2857\n",
            "Epoch: 733, Loss: 0.2885\n",
            "Epoch: 734, Loss: 0.2840\n",
            "Epoch: 735, Loss: 0.2891\n",
            "Epoch: 736, Loss: 0.2868\n",
            "Epoch: 737, Loss: 0.2883\n",
            "Epoch: 738, Loss: 0.2832\n",
            "Epoch: 739, Loss: 0.2849\n",
            "Epoch: 740, Loss: 0.2823\n",
            "Epoch: 741, Loss: 0.2882\n",
            "Epoch: 742, Loss: 0.2851\n",
            "Epoch: 743, Loss: 0.2824\n",
            "Epoch: 744, Loss: 0.2852\n",
            "Epoch: 745, Loss: 0.2816\n",
            "Epoch: 746, Loss: 0.2841\n",
            "Epoch: 747, Loss: 0.2861\n",
            "Epoch: 748, Loss: 0.2836\n",
            "Epoch: 749, Loss: 0.2807\n",
            "Epoch: 750, Loss: 0.2796\n",
            "Epoch: 751, Loss: 0.2842\n",
            "Epoch: 752, Loss: 0.2799\n",
            "Epoch: 753, Loss: 0.2820\n",
            "Epoch: 754, Loss: 0.2799\n",
            "Epoch: 755, Loss: 0.2766\n",
            "Epoch: 756, Loss: 0.2767\n",
            "Epoch: 757, Loss: 0.2793\n",
            "Epoch: 758, Loss: 0.2701\n",
            "Epoch: 759, Loss: 0.2791\n",
            "Epoch: 760, Loss: 0.2747\n",
            "Epoch: 761, Loss: 0.2708\n",
            "Epoch: 762, Loss: 0.2748\n",
            "Epoch: 763, Loss: 0.2752\n",
            "Epoch: 764, Loss: 0.2710\n",
            "Epoch: 765, Loss: 0.2748\n",
            "Epoch: 766, Loss: 0.2746\n",
            "Epoch: 767, Loss: 0.2801\n",
            "Epoch: 768, Loss: 0.2732\n",
            "Epoch: 769, Loss: 0.2756\n",
            "Epoch: 770, Loss: 0.2750\n",
            "Epoch: 771, Loss: 0.2716\n",
            "Epoch: 772, Loss: 0.2712\n",
            "Epoch: 773, Loss: 0.2696\n",
            "Epoch: 774, Loss: 0.2694\n",
            "Epoch: 775, Loss: 0.2716\n",
            "Epoch: 776, Loss: 0.2715\n",
            "Epoch: 777, Loss: 0.2635\n",
            "Epoch: 778, Loss: 0.2697\n",
            "Epoch: 779, Loss: 0.2684\n",
            "Epoch: 780, Loss: 0.2717\n",
            "Epoch: 781, Loss: 0.2661\n",
            "Epoch: 782, Loss: 0.2641\n",
            "Epoch: 783, Loss: 0.2679\n",
            "Epoch: 784, Loss: 0.2683\n",
            "Epoch: 785, Loss: 0.2638\n",
            "Epoch: 786, Loss: 0.2713\n",
            "Epoch: 787, Loss: 0.2655\n",
            "Epoch: 788, Loss: 0.2662\n",
            "Epoch: 789, Loss: 0.2653\n",
            "Epoch: 790, Loss: 0.2596\n",
            "Epoch: 791, Loss: 0.2603\n",
            "Epoch: 792, Loss: 0.2621\n",
            "Epoch: 793, Loss: 0.2680\n",
            "Epoch: 794, Loss: 0.2599\n",
            "Epoch: 795, Loss: 0.2615\n",
            "Epoch: 796, Loss: 0.2634\n",
            "Epoch: 797, Loss: 0.2649\n",
            "Epoch: 798, Loss: 0.2612\n",
            "Epoch: 799, Loss: 0.2625\n",
            "Epoch: 800, Loss: 0.2687\n",
            "Epoch: 801, Loss: 0.2590\n",
            "Epoch: 802, Loss: 0.2610\n",
            "Epoch: 803, Loss: 0.2605\n",
            "Epoch: 804, Loss: 0.2624\n",
            "Epoch: 805, Loss: 0.2629\n",
            "Epoch: 806, Loss: 0.2611\n",
            "Epoch: 807, Loss: 0.2592\n",
            "Epoch: 808, Loss: 0.2527\n",
            "Epoch: 809, Loss: 0.2549\n",
            "Epoch: 810, Loss: 0.2556\n",
            "Epoch: 811, Loss: 0.2583\n",
            "Epoch: 812, Loss: 0.2615\n",
            "Epoch: 813, Loss: 0.2577\n",
            "Epoch: 814, Loss: 0.2595\n",
            "Epoch: 815, Loss: 0.2590\n",
            "Epoch: 816, Loss: 0.2576\n",
            "Epoch: 817, Loss: 0.2492\n",
            "Epoch: 818, Loss: 0.2597\n",
            "Epoch: 819, Loss: 0.2611\n",
            "Epoch: 820, Loss: 0.2513\n",
            "Epoch: 821, Loss: 0.2541\n",
            "Epoch: 822, Loss: 0.2538\n",
            "Epoch: 823, Loss: 0.2522\n",
            "Epoch: 824, Loss: 0.2497\n",
            "Epoch: 825, Loss: 0.2510\n",
            "Epoch: 826, Loss: 0.2536\n",
            "Epoch: 827, Loss: 0.2489\n",
            "Epoch: 828, Loss: 0.2489\n",
            "Epoch: 829, Loss: 0.2548\n",
            "Epoch: 830, Loss: 0.2506\n",
            "Epoch: 831, Loss: 0.2485\n",
            "Epoch: 832, Loss: 0.2538\n",
            "Epoch: 833, Loss: 0.2549\n",
            "Epoch: 834, Loss: 0.2499\n",
            "Epoch: 835, Loss: 0.2500\n",
            "Epoch: 836, Loss: 0.2543\n",
            "Epoch: 837, Loss: 0.2488\n",
            "Epoch: 838, Loss: 0.2546\n",
            "Epoch: 839, Loss: 0.2552\n",
            "Epoch: 840, Loss: 0.2443\n",
            "Epoch: 841, Loss: 0.2493\n",
            "Epoch: 842, Loss: 0.2452\n",
            "Epoch: 843, Loss: 0.2501\n",
            "Epoch: 844, Loss: 0.2484\n",
            "Epoch: 845, Loss: 0.2510\n",
            "Epoch: 846, Loss: 0.2468\n",
            "Epoch: 847, Loss: 0.2506\n",
            "Epoch: 848, Loss: 0.2407\n",
            "Epoch: 849, Loss: 0.2402\n",
            "Epoch: 850, Loss: 0.2462\n",
            "Epoch: 851, Loss: 0.2494\n",
            "Epoch: 852, Loss: 0.2461\n",
            "Epoch: 853, Loss: 0.2456\n",
            "Epoch: 854, Loss: 0.2472\n",
            "Epoch: 855, Loss: 0.2418\n",
            "Epoch: 856, Loss: 0.2481\n",
            "Epoch: 857, Loss: 0.2438\n",
            "Epoch: 858, Loss: 0.2430\n",
            "Epoch: 859, Loss: 0.2422\n",
            "Epoch: 860, Loss: 0.2400\n",
            "Epoch: 861, Loss: 0.2419\n",
            "Epoch: 862, Loss: 0.2420\n",
            "Epoch: 863, Loss: 0.2426\n",
            "Epoch: 864, Loss: 0.2392\n",
            "Epoch: 865, Loss: 0.2425\n",
            "Epoch: 866, Loss: 0.2391\n",
            "Epoch: 867, Loss: 0.2417\n",
            "Epoch: 868, Loss: 0.2404\n",
            "Epoch: 869, Loss: 0.2388\n",
            "Epoch: 870, Loss: 0.2365\n",
            "Epoch: 871, Loss: 0.2435\n",
            "Epoch: 872, Loss: 0.2373\n",
            "Epoch: 873, Loss: 0.2462\n",
            "Epoch: 874, Loss: 0.2396\n",
            "Epoch: 875, Loss: 0.2436\n",
            "Epoch: 876, Loss: 0.2357\n",
            "Epoch: 877, Loss: 0.2434\n",
            "Epoch: 878, Loss: 0.2342\n",
            "Epoch: 879, Loss: 0.2396\n",
            "Epoch: 880, Loss: 0.2422\n",
            "Epoch: 881, Loss: 0.2407\n",
            "Epoch: 882, Loss: 0.2413\n",
            "Epoch: 883, Loss: 0.2404\n",
            "Epoch: 884, Loss: 0.2341\n",
            "Epoch: 885, Loss: 0.2337\n",
            "Epoch: 886, Loss: 0.2435\n",
            "Epoch: 887, Loss: 0.2350\n",
            "Epoch: 888, Loss: 0.2364\n",
            "Epoch: 889, Loss: 0.2378\n",
            "Epoch: 890, Loss: 0.2362\n",
            "Epoch: 891, Loss: 0.2400\n",
            "Epoch: 892, Loss: 0.2355\n",
            "Epoch: 893, Loss: 0.2312\n",
            "Epoch: 894, Loss: 0.2375\n",
            "Epoch: 895, Loss: 0.2350\n",
            "Epoch: 896, Loss: 0.2321\n",
            "Epoch: 897, Loss: 0.2398\n",
            "Epoch: 898, Loss: 0.2313\n",
            "Epoch: 899, Loss: 0.2323\n",
            "Epoch: 900, Loss: 0.2355\n",
            "Epoch: 901, Loss: 0.2327\n",
            "Epoch: 902, Loss: 0.2256\n",
            "Epoch: 903, Loss: 0.2326\n",
            "Epoch: 904, Loss: 0.2282\n",
            "Epoch: 905, Loss: 0.2297\n",
            "Epoch: 906, Loss: 0.2284\n",
            "Epoch: 907, Loss: 0.2349\n",
            "Epoch: 908, Loss: 0.2365\n",
            "Epoch: 909, Loss: 0.2301\n",
            "Epoch: 910, Loss: 0.2287\n",
            "Epoch: 911, Loss: 0.2235\n",
            "Epoch: 912, Loss: 0.2348\n",
            "Epoch: 913, Loss: 0.2269\n",
            "Epoch: 914, Loss: 0.2358\n",
            "Epoch: 915, Loss: 0.2340\n",
            "Epoch: 916, Loss: 0.2281\n",
            "Epoch: 917, Loss: 0.2250\n",
            "Epoch: 918, Loss: 0.2296\n",
            "Epoch: 919, Loss: 0.2271\n",
            "Epoch: 920, Loss: 0.2270\n",
            "Epoch: 921, Loss: 0.2244\n",
            "Epoch: 922, Loss: 0.2318\n",
            "Epoch: 923, Loss: 0.2286\n",
            "Epoch: 924, Loss: 0.2240\n",
            "Epoch: 925, Loss: 0.2312\n",
            "Epoch: 926, Loss: 0.2246\n",
            "Epoch: 927, Loss: 0.2296\n",
            "Epoch: 928, Loss: 0.2279\n",
            "Epoch: 929, Loss: 0.2226\n",
            "Epoch: 930, Loss: 0.2241\n",
            "Epoch: 931, Loss: 0.2259\n",
            "Epoch: 932, Loss: 0.2219\n",
            "Epoch: 933, Loss: 0.2227\n",
            "Epoch: 934, Loss: 0.2200\n",
            "Epoch: 935, Loss: 0.2211\n",
            "Epoch: 936, Loss: 0.2266\n",
            "Epoch: 937, Loss: 0.2203\n",
            "Epoch: 938, Loss: 0.2212\n",
            "Epoch: 939, Loss: 0.2262\n",
            "Epoch: 940, Loss: 0.2203\n",
            "Epoch: 941, Loss: 0.2268\n",
            "Epoch: 942, Loss: 0.2213\n",
            "Epoch: 943, Loss: 0.2213\n",
            "Epoch: 944, Loss: 0.2195\n",
            "Epoch: 945, Loss: 0.2204\n",
            "Epoch: 946, Loss: 0.2184\n",
            "Epoch: 947, Loss: 0.2237\n",
            "Epoch: 948, Loss: 0.2229\n",
            "Epoch: 949, Loss: 0.2177\n",
            "Epoch: 950, Loss: 0.2172\n",
            "Epoch: 951, Loss: 0.2261\n",
            "Epoch: 952, Loss: 0.2238\n",
            "Epoch: 953, Loss: 0.2238\n",
            "Epoch: 954, Loss: 0.2254\n",
            "Epoch: 955, Loss: 0.2112\n",
            "Epoch: 956, Loss: 0.2273\n",
            "Epoch: 957, Loss: 0.2243\n",
            "Epoch: 958, Loss: 0.2149\n",
            "Epoch: 959, Loss: 0.2238\n",
            "Epoch: 960, Loss: 0.2179\n",
            "Epoch: 961, Loss: 0.2204\n",
            "Epoch: 962, Loss: 0.2161\n",
            "Epoch: 963, Loss: 0.2177\n",
            "Epoch: 964, Loss: 0.2165\n",
            "Epoch: 965, Loss: 0.2211\n",
            "Epoch: 966, Loss: 0.2184\n",
            "Epoch: 967, Loss: 0.2183\n",
            "Epoch: 968, Loss: 0.2159\n",
            "Epoch: 969, Loss: 0.2184\n",
            "Epoch: 970, Loss: 0.2134\n",
            "Epoch: 971, Loss: 0.2124\n",
            "Epoch: 972, Loss: 0.2173\n",
            "Epoch: 973, Loss: 0.2133\n",
            "Epoch: 974, Loss: 0.2169\n",
            "Epoch: 975, Loss: 0.2198\n",
            "Epoch: 976, Loss: 0.2140\n",
            "Epoch: 977, Loss: 0.2127\n",
            "Epoch: 978, Loss: 0.2141\n",
            "Epoch: 979, Loss: 0.2149\n",
            "Epoch: 980, Loss: 0.2146\n",
            "Epoch: 981, Loss: 0.2176\n",
            "Epoch: 982, Loss: 0.2181\n",
            "Epoch: 983, Loss: 0.2180\n",
            "Epoch: 984, Loss: 0.2164\n",
            "Epoch: 985, Loss: 0.2133\n",
            "Epoch: 986, Loss: 0.2144\n",
            "Epoch: 987, Loss: 0.2131\n",
            "Epoch: 988, Loss: 0.2177\n",
            "Epoch: 989, Loss: 0.2167\n",
            "Epoch: 990, Loss: 0.2155\n",
            "Epoch: 991, Loss: 0.2169\n",
            "Epoch: 992, Loss: 0.2153\n",
            "Epoch: 993, Loss: 0.2130\n",
            "Epoch: 994, Loss: 0.2152\n",
            "Epoch: 995, Loss: 0.2093\n",
            "Epoch: 996, Loss: 0.2083\n",
            "Epoch: 997, Loss: 0.2125\n",
            "Epoch: 998, Loss: 0.2140\n",
            "Epoch: 999, Loss: 0.2139\n",
            "Epoch: 1000, Loss: 0.2114\n",
            "Test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9001/9001 [00:33<00:00, 264.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1000, Val Mean Rank: 337.21 Val MRR: 0.0318, Val Hits@10: 0.0678\n",
            "Save parameters...\n",
            "End\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "To, To_losses = train_and_test_complex(model=to_ComplEx,\n",
        "                                             train_data=train_data, test_data=test_data,\n",
        "                                             xp_name='First try pipeline with tail only complex',\n",
        "                                             epochs=1000,\n",
        "                                             eval_period=500,\n",
        "                                             reset_parameters=True,\n",
        "                                             save_params=False,\n",
        "                                             device=device,\n",
        "                                             use_wandb = False\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Init loader...\n",
            "Init optimizer...\n",
            "Train...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m blc, losses_blc\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_lin_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mxp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFirst try pipeline with best lin sim complex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43meval_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mreset_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43msave_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[66], line 103\u001b[0m, in \u001b[0;36mtrain_and_test_complex\u001b[0;34m(model, train_data, test_data, xp_name, epochs, eval_period, reset_parameters, save_params, use_wandb, params_save_path, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m get_test_loss(\n\u001b[1;32m    108\u001b[0m                         loader \u001b[38;5;241m=\u001b[39m test_loader,\n\u001b[1;32m    109\u001b[0m                         model  \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m    110\u001b[0m                         device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    112\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
            "Cell \u001b[0;32mIn[66], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, model, optimizer, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m head_index, rel_type, tail_index \u001b[38;5;241m=\u001b[39m head_index\u001b[38;5;241m.\u001b[39mto(device), rel_type\u001b[38;5;241m.\u001b[39mto(device), tail_index\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "Cell \u001b[0;32mIn[44], line 64\u001b[0m, in \u001b[0;36mbest_LinSim_ComplEx.loss\u001b[0;34m(self, head_index, rel_type, tail_index)\u001b[0m\n\u001b[1;32m     61\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pos_target, neg_target], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Calculating LinSim(positive_head, negative_head) : \u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mbest_lin_sims_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_tail_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(scores, target) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(similarities)\n",
            "Cell \u001b[0;32mIn[41], line 28\u001b[0m, in \u001b[0;36mbest_lin_sims_for_batch\u001b[0;34m(head_index, rel_type, tail_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_lin_sims_for_batch\u001b[39m(head_index:torch\u001b[38;5;241m.\u001b[39mTensor, rel_type:torch\u001b[38;5;241m.\u001b[39mTensor, tail_index:torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 28\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtail_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(batch\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row : best_lim_sim_for_triple(head\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     33\u001b[0m                                                                          rel\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     34\u001b[0m                                                                          tail\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     35\u001b[0m                                     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:772\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, abc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;66;03m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[0;32m--> 772\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "blc, losses_blc= train_and_test_complex(model=best_lin_complex,\n",
        "                                             train_data=train_data, test_data=test_data,\n",
        "                                             xp_name='First try pipeline with best lin sim complex',\n",
        "                                             epochs=1000,\n",
        "                                             eval_period=500,\n",
        "                                             reset_parameters=True,\n",
        "                                             save_params=False,\n",
        "                                             device=device,\n",
        "                                             use_wandb = False\n",
        "                                             )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
