{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQDup52LepGf"
      },
      "source": [
        "# Imports, install and mount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIJjGMol68d"
      },
      "source": [
        "<!--  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IdMkskBXNLUF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (5.2.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "# ! pip install cuda\n",
        "# ! pip install torch_geometric\n",
        "# ! pip install nxontology\n",
        "# ! pip install tensordict\n",
        "# ! pip install pandas\n",
        "# ! pip install tensorflow\n",
        "# ! pip install scipy\n",
        "# ! pip install matplotlib\n",
        "\n",
        "# ! pip3 install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import ComplEx\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zsSHTMaQjRm"
      },
      "source": [
        "# Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZUA9xMQQjRm",
        "outputId": "074d9313-168f-444d-c817-4bb87dc9d0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/elliot/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        }
      ],
      "source": [
        "# ComplEx embeddings :\n",
        "\n",
        "hidden_channels = 220\n",
        "batch_size = 4096\n",
        "epochs = 1000\n",
        "\n",
        "params_save_name = f\"PARAMS_ComplEx_HC_6_times_{hidden_channels}_on_full_Os_GO\"\n",
        "params_save_path = \"/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
        "\n",
        "#Â Datas\n",
        "mapped_iric_path = '/home/elliot/Documents/ESL2024/data/mapped_Os_to_GO_iric.tsv'\n",
        "datasets_save_path = '/home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/dataset_'\n",
        "val_path = datasets_save_path + 'VAL' +  '.pickle'\n",
        "test_path = datasets_save_path + 'TEST' +  '.pickle'\n",
        "train_path = datasets_save_path + 'TRAIN' +  '.pickle'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "print(device)\n",
        "\n",
        "# wandb.init(\n",
        "#     settings=wandb.Settings(start_method=\"fork\"),\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"ComplEx on Os_to_GO_iric\",\n",
        "    \n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"architecture\": \"ComplEx\",\n",
        "#     \"dataset\": \"genes_to_phenotypes_iric.tsv\",\n",
        "#     \"epochs\": epochs,\n",
        "#     'hidden_channels' : hidden_channels,\n",
        "#     'batch_size' : batch_size\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liA7PxgDac4i",
        "outputId": "a38fc686-f884-4147-8f6e-3101f3cc41ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>predicate</th>\n",
              "      <th>object</th>\n",
              "      <th>mapped_subject</th>\n",
              "      <th>mapped_predicate</th>\n",
              "      <th>mapped_object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0031267</td>\n",
              "      <td>8201</td>\n",
              "      <td>0</td>\n",
              "      <td>6566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0006886</td>\n",
              "      <td>8201</td>\n",
              "      <td>0</td>\n",
              "      <td>20154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005622</td>\n",
              "      <td>8201</td>\n",
              "      <td>0</td>\n",
              "      <td>20826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005623</td>\n",
              "      <td>8201</td>\n",
              "      <td>0</td>\n",
              "      <td>10373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OsNippo01g010050</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0090630</td>\n",
              "      <td>8201</td>\n",
              "      <td>0</td>\n",
              "      <td>2733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169243</th>\n",
              "      <td>OsNippo12g248550</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0009409</td>\n",
              "      <td>20245</td>\n",
              "      <td>0</td>\n",
              "      <td>12440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169244</th>\n",
              "      <td>OsNippo12g248550</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0001666</td>\n",
              "      <td>20245</td>\n",
              "      <td>0</td>\n",
              "      <td>4625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169245</th>\n",
              "      <td>OsNippo12g250550</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0008270</td>\n",
              "      <td>20383</td>\n",
              "      <td>0</td>\n",
              "      <td>15186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169246</th>\n",
              "      <td>OsNippo12g255100</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0005576</td>\n",
              "      <td>29052</td>\n",
              "      <td>0</td>\n",
              "      <td>8295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169247</th>\n",
              "      <td>OsNippo12g255100</td>\n",
              "      <td>gene ontology</td>\n",
              "      <td>GO:0006952</td>\n",
              "      <td>29052</td>\n",
              "      <td>0</td>\n",
              "      <td>7092</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>169248 rows Ã 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 subject      predicate      object  mapped_subject  \\\n",
              "0       OsNippo01g010050  gene ontology  GO:0031267            8201   \n",
              "1       OsNippo01g010050  gene ontology  GO:0006886            8201   \n",
              "2       OsNippo01g010050  gene ontology  GO:0005622            8201   \n",
              "3       OsNippo01g010050  gene ontology  GO:0005623            8201   \n",
              "4       OsNippo01g010050  gene ontology  GO:0090630            8201   \n",
              "...                  ...            ...         ...             ...   \n",
              "169243  OsNippo12g248550  gene ontology  GO:0009409           20245   \n",
              "169244  OsNippo12g248550  gene ontology  GO:0001666           20245   \n",
              "169245  OsNippo12g250550  gene ontology  GO:0008270           20383   \n",
              "169246  OsNippo12g255100  gene ontology  GO:0005576           29052   \n",
              "169247  OsNippo12g255100  gene ontology  GO:0006952           29052   \n",
              "\n",
              "        mapped_predicate  mapped_object  \n",
              "0                      0           6566  \n",
              "1                      0          20154  \n",
              "2                      0          20826  \n",
              "3                      0          10373  \n",
              "4                      0           2733  \n",
              "...                  ...            ...  \n",
              "169243                 0          12440  \n",
              "169244                 0           4625  \n",
              "169245                 0          15186  \n",
              "169246                 0           8295  \n",
              "169247                 0           7092  \n",
              "\n",
              "[169248 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mapped_iric = pd.read_csv(mapped_iric_path, sep = '\\t')\n",
        "display(mapped_iric)\n",
        "\n",
        "GO_to_map = {row['object']: row['mapped_object'] for index, row in mapped_iric.iterrows()}\n",
        "map_to_GO = {key: value for key, value in GO_to_map.items()}\n",
        "\n",
        "print('Dict looks ok :', bool(GO_to_map[mapped_iric['object'][0]]==mapped_iric['mapped_object'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datatsets look OK ? (val, train, test) : True True True\n",
            "Data(edge_index=[2, 135400], edge_attr=[135400], num_nodes=30396, edge_label=[33848], edge_label_index=[2, 33848])\n",
            "Data(edge_index=[2, 152324], edge_attr=[152324], num_nodes=30396, edge_label=[33848], edge_label_index=[2, 33848])\n",
            "Data(edge_index=[2, 135400], edge_attr=[135400], num_nodes=30396, edge_label=[135400], edge_label_index=[2, 135400])\n"
          ]
        }
      ],
      "source": [
        "val_data = torch.load(val_path)\n",
        "test_data = torch.load(test_path)\n",
        "train_data = torch.load(train_path)\n",
        "\n",
        "print(\"Datatsets look OK ? (val, train, test) :\",\n",
        "val_data.validate(),\n",
        "test_data.validate(),\n",
        "train_data.validate())\n",
        "\n",
        "print(val_data)\n",
        "print(test_data)\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmfPJcuTi3Ul"
      },
      "source": [
        "## Iniating models and loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cQ3Oi0vsQjRr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loader type : <class 'torch_geometric.nn.kge.loader.KGTripletLoader'>\n",
            "4096\n"
          ]
        }
      ],
      "source": [
        "#Â Initiating models\n",
        "\n",
        "complex_model = ComplEx(\n",
        "    num_nodes=train_data.num_nodes,\n",
        "    num_relations = train_data.edge_index.size()[1],\n",
        "    hidden_channels=hidden_channels,\n",
        ").to(device)\n",
        "\n",
        "#Â Initiaing loader\n",
        "head_index = train_data.edge_index[0]\n",
        "tail_index = train_data.edge_index[1]\n",
        "rel_type = train_data.edge_attr\n",
        "\n",
        "loader = complex_model.loader(\n",
        "    head_index = head_index,\n",
        "    tail_index = tail_index,\n",
        "    rel_type = rel_type,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "print(\"Loader type :\", type(loader))\n",
        "\n",
        "# initiating optimizers\n",
        "complex_optimizer = optim.Adam(complex_model.parameters())\n",
        "\n",
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(data, model):\n",
        "    model.eval()\n",
        "    return model.test(\n",
        "        head_index=data.edge_index[0],\n",
        "        tail_index=data.edge_index[1],\n",
        "        rel_type=data.edge_attr,\n",
        "        batch_size=batch_size,\n",
        "        k=10, #The k in Hit@k\n",
        "    )\n",
        "\n",
        "def train(loader, model, optimizer):\n",
        "    model.train()\n",
        "    total_loss = total_examples = 0\n",
        "    for head_index, rel_type, tail_index in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(head_index, rel_type, tail_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * head_index.numel()\n",
        "        total_examples += head_index.numel()\n",
        "    return total_loss / total_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "for a in range(0,20):\n",
        "    print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv6TIchaQjRr",
        "outputId": "12dd55ba-9b78-4b6a-d457-d4bb97cc8688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Loss: 0.6931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 152324/152324 [15:32<00:00, 163.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Val Mean Rank: 8673.97 Val MRR: 0.0084, Val Hits@10: 0.0152\n",
            "Epoch: 001, Loss: 0.6931\n",
            "Epoch: 002, Loss: 0.6919\n",
            "Epoch: 003, Loss: 0.6790\n",
            "Epoch: 004, Loss: 0.6337\n",
            "Epoch: 005, Loss: 0.5650\n",
            "Epoch: 006, Loss: 0.5067\n",
            "Epoch: 007, Loss: 0.4716\n",
            "Epoch: 008, Loss: 0.4505\n",
            "Epoch: 009, Loss: 0.4338\n",
            "Epoch: 010, Loss: 0.4214\n",
            "Epoch: 011, Loss: 0.4078\n",
            "Epoch: 012, Loss: 0.3963\n",
            "Epoch: 013, Loss: 0.3798\n",
            "Epoch: 014, Loss: 0.3634\n",
            "Epoch: 015, Loss: 0.3454\n",
            "Epoch: 016, Loss: 0.3260\n",
            "Epoch: 017, Loss: 0.3041\n",
            "Epoch: 018, Loss: 0.2823\n",
            "Epoch: 019, Loss: 0.2609\n",
            "Epoch: 020, Loss: 0.2403\n",
            "Epoch: 021, Loss: 0.2203\n",
            "Epoch: 022, Loss: 0.2039\n",
            "Epoch: 023, Loss: 0.1869\n",
            "Epoch: 024, Loss: 0.1727\n",
            "Epoch: 025, Loss: 0.1618\n",
            "Epoch: 026, Loss: 0.1496\n",
            "Epoch: 027, Loss: 0.1387\n",
            "Epoch: 028, Loss: 0.1305\n",
            "Epoch: 029, Loss: 0.1216\n",
            "Epoch: 030, Loss: 0.1159\n",
            "Epoch: 031, Loss: 0.1104\n",
            "Epoch: 032, Loss: 0.1032\n",
            "Epoch: 033, Loss: 0.0997\n",
            "Epoch: 034, Loss: 0.0942\n",
            "Epoch: 035, Loss: 0.0915\n",
            "Epoch: 036, Loss: 0.0873\n",
            "Epoch: 037, Loss: 0.0825\n",
            "Epoch: 038, Loss: 0.0806\n",
            "Epoch: 039, Loss: 0.0781\n",
            "Epoch: 040, Loss: 0.0760\n",
            "Epoch: 041, Loss: 0.0736\n",
            "Epoch: 042, Loss: 0.0713\n",
            "Epoch: 043, Loss: 0.0698\n",
            "Epoch: 044, Loss: 0.0681\n",
            "Epoch: 045, Loss: 0.0657\n",
            "Epoch: 046, Loss: 0.0663\n",
            "Epoch: 047, Loss: 0.0647\n",
            "Epoch: 048, Loss: 0.0631\n",
            "Epoch: 049, Loss: 0.0619\n",
            "Epoch: 050, Loss: 0.0602\n",
            "Epoch: 051, Loss: 0.0598\n",
            "Epoch: 052, Loss: 0.0577\n",
            "Epoch: 053, Loss: 0.0579\n",
            "Epoch: 054, Loss: 0.0575\n",
            "Epoch: 055, Loss: 0.0555\n",
            "Epoch: 056, Loss: 0.0551\n",
            "Epoch: 057, Loss: 0.0548\n",
            "Epoch: 058, Loss: 0.0542\n",
            "Epoch: 059, Loss: 0.0535\n",
            "Epoch: 060, Loss: 0.0527\n",
            "Epoch: 061, Loss: 0.0529\n",
            "Epoch: 062, Loss: 0.0519\n",
            "Epoch: 063, Loss: 0.0513\n",
            "Epoch: 064, Loss: 0.0512\n",
            "Epoch: 065, Loss: 0.0504\n",
            "Epoch: 066, Loss: 0.0503\n",
            "Epoch: 067, Loss: 0.0501\n",
            "Epoch: 068, Loss: 0.0488\n",
            "Epoch: 069, Loss: 0.0496\n",
            "Epoch: 070, Loss: 0.0480\n",
            "Epoch: 071, Loss: 0.0475\n",
            "Epoch: 072, Loss: 0.0484\n",
            "Epoch: 073, Loss: 0.0486\n",
            "Epoch: 074, Loss: 0.0462\n",
            "Epoch: 075, Loss: 0.0468\n",
            "Epoch: 076, Loss: 0.0452\n",
            "Epoch: 077, Loss: 0.0470\n",
            "Epoch: 078, Loss: 0.0472\n",
            "Epoch: 079, Loss: 0.0463\n",
            "Epoch: 080, Loss: 0.0466\n",
            "Epoch: 081, Loss: 0.0460\n",
            "Epoch: 082, Loss: 0.0446\n",
            "Epoch: 083, Loss: 0.0445\n",
            "Epoch: 084, Loss: 0.0448\n",
            "Epoch: 085, Loss: 0.0459\n",
            "Epoch: 086, Loss: 0.0451\n",
            "Epoch: 087, Loss: 0.0449\n",
            "Epoch: 088, Loss: 0.0464\n",
            "Epoch: 089, Loss: 0.0442\n",
            "Epoch: 090, Loss: 0.0444\n",
            "Epoch: 091, Loss: 0.0439\n",
            "Epoch: 092, Loss: 0.0436\n",
            "Epoch: 093, Loss: 0.0430\n",
            "Epoch: 094, Loss: 0.0431\n",
            "Epoch: 095, Loss: 0.0429\n",
            "Epoch: 096, Loss: 0.0432\n",
            "Epoch: 097, Loss: 0.0432\n",
            "Epoch: 098, Loss: 0.0430\n",
            "Epoch: 099, Loss: 0.0428\n",
            "Epoch: 100, Loss: 0.0424\n",
            "Epoch: 101, Loss: 0.0425\n",
            "Epoch: 102, Loss: 0.0414\n",
            "Epoch: 103, Loss: 0.0414\n",
            "Epoch: 104, Loss: 0.0414\n",
            "Epoch: 105, Loss: 0.0440\n",
            "Epoch: 106, Loss: 0.0418\n",
            "Epoch: 107, Loss: 0.0414\n",
            "Epoch: 108, Loss: 0.0419\n",
            "Epoch: 109, Loss: 0.0413\n",
            "Epoch: 110, Loss: 0.0409\n",
            "Epoch: 111, Loss: 0.0412\n",
            "Epoch: 112, Loss: 0.0417\n",
            "Epoch: 113, Loss: 0.0421\n",
            "Epoch: 114, Loss: 0.0427\n",
            "Epoch: 115, Loss: 0.0416\n",
            "Epoch: 116, Loss: 0.0415\n",
            "Epoch: 117, Loss: 0.0412\n",
            "Epoch: 118, Loss: 0.0411\n",
            "Epoch: 119, Loss: 0.0405\n",
            "Epoch: 120, Loss: 0.0427\n",
            "Epoch: 121, Loss: 0.0397\n",
            "Epoch: 122, Loss: 0.0420\n",
            "Epoch: 123, Loss: 0.0396\n",
            "Epoch: 124, Loss: 0.0405\n",
            "Epoch: 125, Loss: 0.0403\n",
            "Epoch: 126, Loss: 0.0397\n",
            "Epoch: 127, Loss: 0.0399\n",
            "Epoch: 128, Loss: 0.0402\n",
            "Epoch: 129, Loss: 0.0401\n",
            "Epoch: 130, Loss: 0.0399\n",
            "Epoch: 131, Loss: 0.0420\n",
            "Epoch: 132, Loss: 0.0394\n",
            "Epoch: 133, Loss: 0.0410\n",
            "Epoch: 134, Loss: 0.0401\n",
            "Epoch: 135, Loss: 0.0410\n",
            "Epoch: 136, Loss: 0.0418\n",
            "Epoch: 137, Loss: 0.0395\n",
            "Epoch: 138, Loss: 0.0403\n",
            "Epoch: 139, Loss: 0.0399\n",
            "Epoch: 140, Loss: 0.0396\n",
            "Epoch: 141, Loss: 0.0395\n",
            "Epoch: 142, Loss: 0.0398\n",
            "Epoch: 143, Loss: 0.0396\n",
            "Epoch: 144, Loss: 0.0395\n",
            "Epoch: 145, Loss: 0.0398\n",
            "Epoch: 146, Loss: 0.0394\n",
            "Epoch: 147, Loss: 0.0408\n",
            "Epoch: 148, Loss: 0.0393\n",
            "Epoch: 149, Loss: 0.0385\n",
            "Epoch: 150, Loss: 0.0385\n",
            "Epoch: 151, Loss: 0.0394\n",
            "Epoch: 152, Loss: 0.0391\n",
            "Epoch: 153, Loss: 0.0386\n",
            "Epoch: 154, Loss: 0.0383\n",
            "Epoch: 155, Loss: 0.0392\n",
            "Epoch: 156, Loss: 0.0395\n",
            "Epoch: 157, Loss: 0.0392\n",
            "Epoch: 158, Loss: 0.0390\n",
            "Epoch: 159, Loss: 0.0381\n",
            "Epoch: 160, Loss: 0.0380\n",
            "Epoch: 161, Loss: 0.0393\n",
            "Epoch: 162, Loss: 0.0394\n",
            "Epoch: 163, Loss: 0.0386\n",
            "Epoch: 164, Loss: 0.0395\n",
            "Epoch: 165, Loss: 0.0365\n",
            "Epoch: 166, Loss: 0.0391\n",
            "Epoch: 167, Loss: 0.0392\n",
            "Epoch: 168, Loss: 0.0388\n",
            "Epoch: 169, Loss: 0.0389\n",
            "Epoch: 170, Loss: 0.0385\n",
            "Epoch: 171, Loss: 0.0396\n",
            "Epoch: 172, Loss: 0.0379\n",
            "Epoch: 173, Loss: 0.0381\n",
            "Epoch: 174, Loss: 0.0380\n",
            "Epoch: 175, Loss: 0.0388\n",
            "Epoch: 176, Loss: 0.0383\n",
            "Epoch: 177, Loss: 0.0380\n",
            "Epoch: 178, Loss: 0.0384\n",
            "Epoch: 179, Loss: 0.0390\n",
            "Epoch: 180, Loss: 0.0381\n",
            "Epoch: 181, Loss: 0.0380\n",
            "Epoch: 182, Loss: 0.0376\n",
            "Epoch: 183, Loss: 0.0387\n",
            "Epoch: 184, Loss: 0.0374\n",
            "Epoch: 185, Loss: 0.0390\n",
            "Epoch: 186, Loss: 0.0378\n",
            "Epoch: 187, Loss: 0.0383\n",
            "Epoch: 188, Loss: 0.0396\n",
            "Epoch: 189, Loss: 0.0384\n",
            "Epoch: 190, Loss: 0.0387\n",
            "Epoch: 191, Loss: 0.0381\n",
            "Epoch: 192, Loss: 0.0381\n",
            "Epoch: 193, Loss: 0.0382\n",
            "Epoch: 194, Loss: 0.0368\n",
            "Epoch: 195, Loss: 0.0384\n",
            "Epoch: 196, Loss: 0.0379\n",
            "Epoch: 197, Loss: 0.0385\n",
            "Epoch: 198, Loss: 0.0376\n",
            "Epoch: 199, Loss: 0.0375\n",
            "Epoch: 200, Loss: 0.0375\n",
            "Epoch: 201, Loss: 0.0382\n",
            "Epoch: 202, Loss: 0.0374\n",
            "Epoch: 203, Loss: 0.0363\n",
            "Epoch: 204, Loss: 0.0381\n",
            "Epoch: 205, Loss: 0.0379\n",
            "Epoch: 206, Loss: 0.0373\n",
            "Epoch: 207, Loss: 0.0368\n",
            "Epoch: 208, Loss: 0.0379\n",
            "Epoch: 209, Loss: 0.0383\n",
            "Epoch: 210, Loss: 0.0376\n",
            "Epoch: 211, Loss: 0.0372\n",
            "Epoch: 212, Loss: 0.0367\n",
            "Epoch: 213, Loss: 0.0378\n",
            "Epoch: 214, Loss: 0.0363\n",
            "Epoch: 215, Loss: 0.0373\n",
            "Epoch: 216, Loss: 0.0382\n",
            "Epoch: 217, Loss: 0.0372\n",
            "Epoch: 218, Loss: 0.0383\n",
            "Epoch: 219, Loss: 0.0376\n",
            "Epoch: 220, Loss: 0.0368\n",
            "Epoch: 221, Loss: 0.0379\n",
            "Epoch: 222, Loss: 0.0365\n",
            "Epoch: 223, Loss: 0.0363\n",
            "Epoch: 224, Loss: 0.0372\n",
            "Epoch: 225, Loss: 0.0374\n",
            "Epoch: 226, Loss: 0.0370\n",
            "Epoch: 227, Loss: 0.0373\n",
            "Epoch: 228, Loss: 0.0380\n",
            "Epoch: 229, Loss: 0.0364\n",
            "Epoch: 230, Loss: 0.0378\n",
            "Epoch: 231, Loss: 0.0363\n",
            "Epoch: 232, Loss: 0.0362\n",
            "Epoch: 233, Loss: 0.0364\n",
            "Epoch: 234, Loss: 0.0380\n",
            "Epoch: 235, Loss: 0.0368\n",
            "Epoch: 236, Loss: 0.0369\n",
            "Epoch: 237, Loss: 0.0374\n",
            "Epoch: 238, Loss: 0.0371\n",
            "Epoch: 239, Loss: 0.0369\n",
            "Epoch: 240, Loss: 0.0373\n",
            "Epoch: 241, Loss: 0.0371\n",
            "Epoch: 242, Loss: 0.0380\n",
            "Epoch: 243, Loss: 0.0378\n",
            "Epoch: 244, Loss: 0.0360\n",
            "Epoch: 245, Loss: 0.0359\n",
            "Epoch: 246, Loss: 0.0363\n",
            "Epoch: 247, Loss: 0.0391\n",
            "Epoch: 248, Loss: 0.0371\n",
            "Epoch: 249, Loss: 0.0374\n",
            "Epoch: 250, Loss: 0.0372\n",
            "Epoch: 251, Loss: 0.0372\n",
            "Epoch: 252, Loss: 0.0369\n",
            "Epoch: 253, Loss: 0.0364\n",
            "Epoch: 254, Loss: 0.0361\n",
            "Epoch: 255, Loss: 0.0373\n",
            "Epoch: 256, Loss: 0.0354\n",
            "Epoch: 257, Loss: 0.0373\n",
            "Epoch: 258, Loss: 0.0364\n",
            "Epoch: 259, Loss: 0.0372\n",
            "Epoch: 260, Loss: 0.0372\n",
            "Epoch: 261, Loss: 0.0362\n",
            "Epoch: 262, Loss: 0.0372\n",
            "Epoch: 263, Loss: 0.0376\n",
            "Epoch: 264, Loss: 0.0366\n",
            "Epoch: 265, Loss: 0.0364\n",
            "Epoch: 266, Loss: 0.0338\n",
            "Epoch: 267, Loss: 0.0362\n",
            "Epoch: 268, Loss: 0.0356\n",
            "Epoch: 269, Loss: 0.0374\n",
            "Epoch: 270, Loss: 0.0367\n",
            "Epoch: 271, Loss: 0.0371\n",
            "Epoch: 272, Loss: 0.0366\n",
            "Epoch: 273, Loss: 0.0376\n",
            "Epoch: 274, Loss: 0.0373\n",
            "Epoch: 275, Loss: 0.0370\n",
            "Epoch: 276, Loss: 0.0366\n",
            "Epoch: 277, Loss: 0.0373\n",
            "Epoch: 278, Loss: 0.0364\n",
            "Epoch: 279, Loss: 0.0366\n",
            "Epoch: 280, Loss: 0.0363\n",
            "Epoch: 281, Loss: 0.0367\n",
            "Epoch: 282, Loss: 0.0376\n",
            "Epoch: 283, Loss: 0.0373\n",
            "Epoch: 284, Loss: 0.0369\n",
            "Epoch: 285, Loss: 0.0351\n",
            "Epoch: 286, Loss: 0.0364\n",
            "Epoch: 287, Loss: 0.0356\n",
            "Epoch: 288, Loss: 0.0349\n",
            "Epoch: 289, Loss: 0.0367\n",
            "Epoch: 290, Loss: 0.0370\n",
            "Epoch: 291, Loss: 0.0366\n",
            "Epoch: 292, Loss: 0.0353\n",
            "Epoch: 293, Loss: 0.0362\n",
            "Epoch: 294, Loss: 0.0356\n",
            "Epoch: 295, Loss: 0.0366\n",
            "Epoch: 296, Loss: 0.0358\n",
            "Epoch: 297, Loss: 0.0371\n",
            "Epoch: 298, Loss: 0.0366\n",
            "Epoch: 299, Loss: 0.0354\n",
            "Epoch: 300, Loss: 0.0368\n",
            "Epoch: 301, Loss: 0.0367\n",
            "Epoch: 302, Loss: 0.0366\n",
            "Epoch: 303, Loss: 0.0369\n",
            "Epoch: 304, Loss: 0.0368\n",
            "Epoch: 305, Loss: 0.0368\n",
            "Epoch: 306, Loss: 0.0375\n",
            "Epoch: 307, Loss: 0.0358\n",
            "Epoch: 308, Loss: 0.0361\n",
            "Epoch: 309, Loss: 0.0370\n",
            "Epoch: 310, Loss: 0.0364\n",
            "Epoch: 311, Loss: 0.0357\n",
            "Epoch: 312, Loss: 0.0357\n",
            "Epoch: 313, Loss: 0.0371\n",
            "Epoch: 314, Loss: 0.0358\n",
            "Epoch: 315, Loss: 0.0367\n",
            "Epoch: 316, Loss: 0.0363\n",
            "Epoch: 317, Loss: 0.0357\n",
            "Epoch: 318, Loss: 0.0367\n",
            "Epoch: 319, Loss: 0.0354\n",
            "Epoch: 320, Loss: 0.0358\n",
            "Epoch: 321, Loss: 0.0369\n",
            "Epoch: 322, Loss: 0.0364\n",
            "Epoch: 323, Loss: 0.0362\n",
            "Epoch: 324, Loss: 0.0355\n",
            "Epoch: 325, Loss: 0.0365\n",
            "Epoch: 326, Loss: 0.0357\n",
            "Epoch: 327, Loss: 0.0359\n",
            "Epoch: 328, Loss: 0.0356\n",
            "Epoch: 329, Loss: 0.0368\n",
            "Epoch: 330, Loss: 0.0356\n",
            "Epoch: 331, Loss: 0.0367\n",
            "Epoch: 332, Loss: 0.0349\n",
            "Epoch: 333, Loss: 0.0365\n",
            "Epoch: 334, Loss: 0.0367\n",
            "Epoch: 335, Loss: 0.0359\n",
            "Epoch: 336, Loss: 0.0353\n",
            "Epoch: 337, Loss: 0.0348\n",
            "Epoch: 338, Loss: 0.0371\n",
            "Epoch: 339, Loss: 0.0349\n",
            "Epoch: 340, Loss: 0.0361\n",
            "Epoch: 341, Loss: 0.0356\n",
            "Epoch: 342, Loss: 0.0353\n",
            "Epoch: 343, Loss: 0.0374\n",
            "Epoch: 344, Loss: 0.0364\n",
            "Epoch: 345, Loss: 0.0361\n",
            "Epoch: 346, Loss: 0.0362\n",
            "Epoch: 347, Loss: 0.0353\n",
            "Epoch: 348, Loss: 0.0366\n",
            "Epoch: 349, Loss: 0.0368\n",
            "Epoch: 350, Loss: 0.0362\n",
            "Epoch: 351, Loss: 0.0358\n",
            "Epoch: 352, Loss: 0.0360\n",
            "Epoch: 353, Loss: 0.0368\n",
            "Epoch: 354, Loss: 0.0348\n",
            "Epoch: 355, Loss: 0.0359\n",
            "Epoch: 356, Loss: 0.0353\n",
            "Epoch: 357, Loss: 0.0371\n",
            "Epoch: 358, Loss: 0.0367\n",
            "Epoch: 359, Loss: 0.0362\n",
            "Epoch: 360, Loss: 0.0354\n",
            "Epoch: 361, Loss: 0.0359\n",
            "Epoch: 362, Loss: 0.0362\n",
            "Epoch: 363, Loss: 0.0362\n",
            "Epoch: 364, Loss: 0.0363\n",
            "Epoch: 365, Loss: 0.0355\n",
            "Epoch: 366, Loss: 0.0371\n",
            "Epoch: 367, Loss: 0.0360\n",
            "Epoch: 368, Loss: 0.0363\n",
            "Epoch: 369, Loss: 0.0364\n",
            "Epoch: 370, Loss: 0.0353\n",
            "Epoch: 371, Loss: 0.0361\n",
            "Epoch: 372, Loss: 0.0360\n",
            "Epoch: 373, Loss: 0.0349\n",
            "Epoch: 374, Loss: 0.0353\n",
            "Epoch: 375, Loss: 0.0365\n",
            "Epoch: 376, Loss: 0.0359\n",
            "Epoch: 377, Loss: 0.0361\n",
            "Epoch: 378, Loss: 0.0355\n",
            "Epoch: 379, Loss: 0.0358\n",
            "Epoch: 380, Loss: 0.0359\n",
            "Epoch: 381, Loss: 0.0364\n",
            "Epoch: 382, Loss: 0.0360\n",
            "Epoch: 383, Loss: 0.0354\n",
            "Epoch: 384, Loss: 0.0366\n",
            "Epoch: 385, Loss: 0.0343\n",
            "Epoch: 386, Loss: 0.0362\n",
            "Epoch: 387, Loss: 0.0360\n",
            "Epoch: 388, Loss: 0.0352\n",
            "Epoch: 389, Loss: 0.0357\n",
            "Epoch: 390, Loss: 0.0363\n",
            "Epoch: 391, Loss: 0.0365\n",
            "Epoch: 392, Loss: 0.0343\n",
            "Epoch: 393, Loss: 0.0367\n",
            "Epoch: 394, Loss: 0.0358\n",
            "Epoch: 395, Loss: 0.0351\n",
            "Epoch: 396, Loss: 0.0364\n",
            "Epoch: 397, Loss: 0.0357\n",
            "Epoch: 398, Loss: 0.0355\n",
            "Epoch: 399, Loss: 0.0358\n",
            "Epoch: 400, Loss: 0.0356\n",
            "Epoch: 401, Loss: 0.0358\n",
            "Epoch: 402, Loss: 0.0354\n",
            "Epoch: 403, Loss: 0.0360\n",
            "Epoch: 404, Loss: 0.0378\n",
            "Epoch: 405, Loss: 0.0357\n",
            "Epoch: 406, Loss: 0.0361\n",
            "Epoch: 407, Loss: 0.0346\n",
            "Epoch: 408, Loss: 0.0362\n",
            "Epoch: 409, Loss: 0.0349\n",
            "Epoch: 410, Loss: 0.0370\n",
            "Epoch: 411, Loss: 0.0360\n",
            "Epoch: 412, Loss: 0.0350\n",
            "Epoch: 413, Loss: 0.0354\n",
            "Epoch: 414, Loss: 0.0356\n",
            "Epoch: 415, Loss: 0.0351\n",
            "Epoch: 416, Loss: 0.0352\n",
            "Epoch: 417, Loss: 0.0359\n",
            "Epoch: 418, Loss: 0.0359\n",
            "Epoch: 419, Loss: 0.0365\n",
            "Epoch: 420, Loss: 0.0356\n",
            "Epoch: 421, Loss: 0.0362\n",
            "Epoch: 422, Loss: 0.0362\n",
            "Epoch: 423, Loss: 0.0352\n",
            "Epoch: 424, Loss: 0.0364\n",
            "Epoch: 425, Loss: 0.0356\n",
            "Epoch: 426, Loss: 0.0355\n",
            "Epoch: 427, Loss: 0.0350\n",
            "Epoch: 428, Loss: 0.0364\n",
            "Epoch: 429, Loss: 0.0358\n",
            "Epoch: 430, Loss: 0.0357\n",
            "Epoch: 431, Loss: 0.0368\n",
            "Epoch: 432, Loss: 0.0353\n",
            "Epoch: 433, Loss: 0.0359\n",
            "Epoch: 434, Loss: 0.0360\n",
            "Epoch: 435, Loss: 0.0358\n",
            "Epoch: 436, Loss: 0.0352\n",
            "Epoch: 437, Loss: 0.0361\n",
            "Epoch: 438, Loss: 0.0355\n",
            "Epoch: 439, Loss: 0.0353\n",
            "Epoch: 440, Loss: 0.0355\n",
            "Epoch: 441, Loss: 0.0354\n",
            "Epoch: 442, Loss: 0.0356\n",
            "Epoch: 443, Loss: 0.0356\n",
            "Epoch: 444, Loss: 0.0365\n",
            "Epoch: 445, Loss: 0.0358\n",
            "Epoch: 446, Loss: 0.0354\n",
            "Epoch: 447, Loss: 0.0357\n",
            "Epoch: 448, Loss: 0.0364\n",
            "Epoch: 449, Loss: 0.0358\n",
            "Epoch: 450, Loss: 0.0360\n",
            "Epoch: 451, Loss: 0.0350\n",
            "Epoch: 452, Loss: 0.0359\n",
            "Epoch: 453, Loss: 0.0361\n",
            "Epoch: 454, Loss: 0.0349\n",
            "Epoch: 455, Loss: 0.0354\n",
            "Epoch: 456, Loss: 0.0352\n",
            "Epoch: 457, Loss: 0.0349\n",
            "Epoch: 458, Loss: 0.0354\n",
            "Epoch: 459, Loss: 0.0354\n",
            "Epoch: 460, Loss: 0.0355\n",
            "Epoch: 461, Loss: 0.0358\n",
            "Epoch: 462, Loss: 0.0347\n",
            "Epoch: 463, Loss: 0.0354\n",
            "Epoch: 464, Loss: 0.0365\n",
            "Epoch: 465, Loss: 0.0358\n",
            "Epoch: 466, Loss: 0.0355\n",
            "Epoch: 467, Loss: 0.0361\n",
            "Epoch: 468, Loss: 0.0345\n",
            "Epoch: 469, Loss: 0.0349\n",
            "Epoch: 470, Loss: 0.0357\n",
            "Epoch: 471, Loss: 0.0356\n",
            "Epoch: 472, Loss: 0.0358\n",
            "Epoch: 473, Loss: 0.0358\n",
            "Epoch: 474, Loss: 0.0360\n",
            "Epoch: 475, Loss: 0.0351\n",
            "Epoch: 476, Loss: 0.0343\n",
            "Epoch: 477, Loss: 0.0342\n",
            "Epoch: 478, Loss: 0.0348\n",
            "Epoch: 479, Loss: 0.0357\n",
            "Epoch: 480, Loss: 0.0360\n",
            "Epoch: 481, Loss: 0.0352\n",
            "Epoch: 482, Loss: 0.0343\n",
            "Epoch: 483, Loss: 0.0359\n",
            "Epoch: 484, Loss: 0.0352\n",
            "Epoch: 485, Loss: 0.0359\n",
            "Epoch: 486, Loss: 0.0356\n",
            "Epoch: 487, Loss: 0.0351\n",
            "Epoch: 488, Loss: 0.0358\n",
            "Epoch: 489, Loss: 0.0351\n",
            "Epoch: 490, Loss: 0.0355\n",
            "Epoch: 491, Loss: 0.0350\n",
            "Epoch: 492, Loss: 0.0351\n",
            "Epoch: 493, Loss: 0.0355\n",
            "Epoch: 494, Loss: 0.0344\n",
            "Epoch: 495, Loss: 0.0344\n",
            "Epoch: 496, Loss: 0.0349\n",
            "Epoch: 497, Loss: 0.0355\n",
            "Epoch: 498, Loss: 0.0343\n",
            "Epoch: 499, Loss: 0.0353\n",
            "Epoch: 500, Loss: 0.0346\n",
            "Epoch: 501, Loss: 0.0355\n",
            "Epoch: 502, Loss: 0.0357\n",
            "Epoch: 503, Loss: 0.0351\n",
            "Epoch: 504, Loss: 0.0352\n",
            "Epoch: 505, Loss: 0.0358\n",
            "Epoch: 506, Loss: 0.0343\n",
            "Epoch: 507, Loss: 0.0359\n",
            "Epoch: 508, Loss: 0.0369\n",
            "Epoch: 509, Loss: 0.0353\n",
            "Epoch: 510, Loss: 0.0348\n",
            "Epoch: 511, Loss: 0.0352\n",
            "Epoch: 512, Loss: 0.0359\n",
            "Epoch: 513, Loss: 0.0366\n",
            "Epoch: 514, Loss: 0.0358\n",
            "Epoch: 515, Loss: 0.0364\n",
            "Epoch: 516, Loss: 0.0349\n",
            "Epoch: 517, Loss: 0.0349\n",
            "Epoch: 518, Loss: 0.0361\n",
            "Epoch: 519, Loss: 0.0356\n",
            "Epoch: 520, Loss: 0.0344\n",
            "Epoch: 521, Loss: 0.0353\n",
            "Epoch: 522, Loss: 0.0345\n",
            "Epoch: 523, Loss: 0.0351\n",
            "Epoch: 524, Loss: 0.0351\n",
            "Epoch: 525, Loss: 0.0355\n",
            "Epoch: 526, Loss: 0.0353\n",
            "Epoch: 527, Loss: 0.0345\n",
            "Epoch: 528, Loss: 0.0348\n",
            "Epoch: 529, Loss: 0.0347\n",
            "Epoch: 530, Loss: 0.0347\n",
            "Epoch: 531, Loss: 0.0351\n",
            "Epoch: 532, Loss: 0.0350\n",
            "Epoch: 533, Loss: 0.0345\n",
            "Epoch: 534, Loss: 0.0355\n",
            "Epoch: 535, Loss: 0.0352\n",
            "Epoch: 536, Loss: 0.0342\n",
            "Epoch: 537, Loss: 0.0344\n",
            "Epoch: 538, Loss: 0.0350\n",
            "Epoch: 539, Loss: 0.0355\n",
            "Epoch: 540, Loss: 0.0351\n",
            "Epoch: 541, Loss: 0.0354\n",
            "Epoch: 542, Loss: 0.0344\n",
            "Epoch: 543, Loss: 0.0347\n",
            "Epoch: 544, Loss: 0.0363\n",
            "Epoch: 545, Loss: 0.0355\n",
            "Epoch: 546, Loss: 0.0359\n",
            "Epoch: 547, Loss: 0.0360\n",
            "Epoch: 548, Loss: 0.0353\n",
            "Epoch: 549, Loss: 0.0366\n",
            "Epoch: 550, Loss: 0.0352\n",
            "Epoch: 551, Loss: 0.0356\n",
            "Epoch: 552, Loss: 0.0348\n",
            "Epoch: 553, Loss: 0.0340\n",
            "Epoch: 554, Loss: 0.0339\n",
            "Epoch: 555, Loss: 0.0349\n",
            "Epoch: 556, Loss: 0.0352\n",
            "Epoch: 557, Loss: 0.0353\n",
            "Epoch: 558, Loss: 0.0347\n",
            "Epoch: 559, Loss: 0.0353\n",
            "Epoch: 560, Loss: 0.0347\n",
            "Epoch: 561, Loss: 0.0346\n",
            "Epoch: 562, Loss: 0.0353\n",
            "Epoch: 563, Loss: 0.0354\n",
            "Epoch: 564, Loss: 0.0349\n",
            "Epoch: 565, Loss: 0.0357\n",
            "Epoch: 566, Loss: 0.0351\n",
            "Epoch: 567, Loss: 0.0360\n",
            "Epoch: 568, Loss: 0.0350\n",
            "Epoch: 569, Loss: 0.0348\n",
            "Epoch: 570, Loss: 0.0356\n",
            "Epoch: 571, Loss: 0.0365\n",
            "Epoch: 572, Loss: 0.0346\n",
            "Epoch: 573, Loss: 0.0358\n",
            "Epoch: 574, Loss: 0.0352\n",
            "Epoch: 575, Loss: 0.0355\n",
            "Epoch: 576, Loss: 0.0349\n",
            "Epoch: 577, Loss: 0.0339\n",
            "Epoch: 578, Loss: 0.0349\n",
            "Epoch: 579, Loss: 0.0346\n",
            "Epoch: 580, Loss: 0.0367\n",
            "Epoch: 581, Loss: 0.0348\n",
            "Epoch: 582, Loss: 0.0344\n",
            "Epoch: 583, Loss: 0.0343\n",
            "Epoch: 584, Loss: 0.0358\n",
            "Epoch: 585, Loss: 0.0348\n",
            "Epoch: 586, Loss: 0.0357\n",
            "Epoch: 587, Loss: 0.0350\n",
            "Epoch: 588, Loss: 0.0351\n",
            "Epoch: 589, Loss: 0.0345\n",
            "Epoch: 590, Loss: 0.0369\n",
            "Epoch: 591, Loss: 0.0358\n",
            "Epoch: 592, Loss: 0.0345\n",
            "Epoch: 593, Loss: 0.0352\n",
            "Epoch: 594, Loss: 0.0350\n",
            "Epoch: 595, Loss: 0.0365\n",
            "Epoch: 596, Loss: 0.0348\n",
            "Epoch: 597, Loss: 0.0346\n",
            "Epoch: 598, Loss: 0.0339\n",
            "Epoch: 599, Loss: 0.0350\n",
            "Epoch: 600, Loss: 0.0353\n",
            "Epoch: 601, Loss: 0.0347\n",
            "Epoch: 602, Loss: 0.0354\n",
            "Epoch: 603, Loss: 0.0342\n",
            "Epoch: 604, Loss: 0.0361\n",
            "Epoch: 605, Loss: 0.0355\n",
            "Epoch: 606, Loss: 0.0352\n",
            "Epoch: 607, Loss: 0.0354\n",
            "Epoch: 608, Loss: 0.0356\n",
            "Epoch: 609, Loss: 0.0339\n",
            "Epoch: 610, Loss: 0.0349\n",
            "Epoch: 611, Loss: 0.0346\n",
            "Epoch: 612, Loss: 0.0355\n",
            "Epoch: 613, Loss: 0.0350\n",
            "Epoch: 614, Loss: 0.0346\n",
            "Epoch: 615, Loss: 0.0345\n",
            "Epoch: 616, Loss: 0.0349\n",
            "Epoch: 617, Loss: 0.0353\n",
            "Epoch: 618, Loss: 0.0353\n",
            "Epoch: 619, Loss: 0.0342\n",
            "Epoch: 620, Loss: 0.0349\n",
            "Epoch: 621, Loss: 0.0343\n",
            "Epoch: 622, Loss: 0.0347\n",
            "Epoch: 623, Loss: 0.0358\n",
            "Epoch: 624, Loss: 0.0345\n",
            "Epoch: 625, Loss: 0.0348\n",
            "Epoch: 626, Loss: 0.0338\n",
            "Epoch: 627, Loss: 0.0343\n",
            "Epoch: 628, Loss: 0.0354\n",
            "Epoch: 629, Loss: 0.0349\n",
            "Epoch: 630, Loss: 0.0349\n",
            "Epoch: 631, Loss: 0.0353\n",
            "Epoch: 632, Loss: 0.0344\n",
            "Epoch: 633, Loss: 0.0345\n",
            "Epoch: 634, Loss: 0.0336\n",
            "Epoch: 635, Loss: 0.0360\n",
            "Epoch: 636, Loss: 0.0360\n",
            "Epoch: 637, Loss: 0.0352\n",
            "Epoch: 638, Loss: 0.0352\n",
            "Epoch: 639, Loss: 0.0351\n",
            "Epoch: 640, Loss: 0.0350\n",
            "Epoch: 641, Loss: 0.0342\n",
            "Epoch: 642, Loss: 0.0356\n",
            "Epoch: 643, Loss: 0.0338\n",
            "Epoch: 644, Loss: 0.0352\n",
            "Epoch: 645, Loss: 0.0349\n",
            "Epoch: 646, Loss: 0.0353\n",
            "Epoch: 647, Loss: 0.0348\n",
            "Epoch: 648, Loss: 0.0359\n",
            "Epoch: 649, Loss: 0.0342\n",
            "Epoch: 650, Loss: 0.0358\n",
            "Epoch: 651, Loss: 0.0355\n",
            "Epoch: 652, Loss: 0.0353\n",
            "Epoch: 653, Loss: 0.0349\n",
            "Epoch: 654, Loss: 0.0355\n",
            "Epoch: 655, Loss: 0.0345\n",
            "Epoch: 656, Loss: 0.0346\n",
            "Epoch: 657, Loss: 0.0342\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     10\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss})\n",
            "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m head_index\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     21\u001b[0m     total_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m head_index\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m total_examples\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "complex_model.reset_parameters()\n",
        "complex_model.to(device)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(1, epochs+1):\n",
        "    loss = train(model=complex_model, optimizer=complex_optimizer)\n",
        "    losses.append(loss)\n",
        "    wandb.log({\"loss\": loss})\n",
        "\n",
        "    if epoch%10 == 0:\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "\n",
        "    if epoch % epochs%500 == 0:\n",
        "        rank, mrr, hits = test(val_data, model=complex_model)\n",
        "        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}',\n",
        "              f'Val MRR: {mrr:.4f}, Val Hits@10: {hits:.4f}')\n",
        "        wandb.log({\"Val Mean Rank\" : rank, \"Val MRR\" : mrr, \"hits@10\": hits})\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val MRR</td><td>ââ</td></tr><tr><td>Val Mean Rank</td><td>ââ</td></tr><tr><td>hits@10</td><td>ââ</td></tr><tr><td>loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val MRR</td><td>0.16405</td></tr><tr><td>Val Mean Rank</td><td>230.30151</td></tr><tr><td>hits@10</td><td>0.36778</td></tr><tr><td>loss</td><td>0.06995</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-blaze-6</strong> at: <a href='https://wandb.ai/esl2024/ComplEx%20on%20Os_to_GO_iric/runs/x3btqtqw/workspace' target=\"_blank\">https://wandb.ai/esl2024/ComplEx%20on%20Os_to_GO_iric/runs/x3btqtqw/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240327_163330-x3btqtqw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WandB finished.\n",
            "Model saved at /home/elliot/Documents/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/PARAMS_ComplEx_HC_6_times_5_on_full_Os_GO\n"
          ]
        }
      ],
      "source": [
        "wandb.finish()\n",
        "print(\"WandB finished.\")\n",
        "\n",
        "torch.save(complex_model.state_dict(), params_save_path)\n",
        "print(\"Model saved at\", params_save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
