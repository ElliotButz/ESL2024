{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), 2)\n"
     ]
    }
   ],
   "source": [
    "t1 = (0,1)\n",
    "t2 = (2)\n",
    "t3= (t1,t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imports...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebutz/miniconda3/envs/pyg/lib/python3.11/site-packages/torch_geometric/typing.py:54: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/ebutz/miniconda3/envs/pyg/lib/python3.11/site-packages/libpyg.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImports...\")\n",
    "import sys\n",
    "sys.path.append( '/home/ebutz/ESL2024/code/utils' )\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( '/home/ebutz/ESL2024/code/utils' )\n",
    "import play_with_complex as pwc\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ComplEx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nxontology.imports import from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0,1])\n",
    "b = torch.tensor([2,3])\n",
    "c = torch.cat([a,b])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cuda check...\n",
      "\n",
      "Loading iric...\n",
      "            subject      predicate      object  mapped_subject  \\\n",
      "0  OsNippo01g010050  gene ontology  GO:0031267           29269   \n",
      "1  OsNippo01g010050  gene ontology  GO:0006886           29269   \n",
      "2  OsNippo01g010050  gene ontology  GO:0005622           29269   \n",
      "3  OsNippo01g010050  gene ontology  GO:0005623           29269   \n",
      "4  OsNippo01g010050  gene ontology  GO:0090630           29269   \n",
      "\n",
      "   mapped_predicate  mapped_object                            mapped_alt_tails  \n",
      "0                 0           8105  [20179, 25142, 11107, 11397, 25543, 24490]  \n",
      "1                 0          20179   [8105, 25142, 11107, 11397, 25543, 24490]  \n",
      "2                 0          25142   [8105, 20179, 11107, 11397, 25543, 24490]  \n",
      "3                 0          11107   [8105, 20179, 25142, 11397, 25543, 24490]  \n",
      "4                 0          11397   [8105, 20179, 25142, 11107, 25543, 24490]  \n",
      "mapped_alt_tails type : <class 'numpy.ndarray'>\n",
      "Alternative tails dict (first key-value pair): ((29269, 0), [11107, 11397, 25543, 8105, 24490, 20179, 25142])\n",
      "\n",
      "Making datasets...\n",
      "Data(edge_index=[2, 169248], edge_attr=[169248], num_nodes=30396)\n",
      "\n",
      "Dataset looks valid : True\n",
      "Dataset splits look valid (train, val, test): True True True\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------- Paths ------------- #\n",
    "\n",
    "# Iric\n",
    "mapped_iric_path  = '/home/ebutz/ESL2024/data/Os_to_GO_iric/altailed_Os_to_GO_iric.pickle'\n",
    "altails_dict_path = '/home/ebutz/ESL2024/data/Os_to_GO_iric/DICT_altailed_Os_to_GO_iric.pickle'\n",
    "\n",
    "# Model to train :\n",
    "hidden_channels = 176\n",
    "batch_size      = 4096\n",
    "epochs          = 200\n",
    "eval_period     = 2\n",
    "lin_factor      = 1\n",
    "\n",
    "use_wandb  = True\n",
    "\n",
    "params_save_name = f\"PARAMS_ComplEx_6_times_{hidden_channels}_HC_{epochs}_epochs_{batch_size}_BS_on_full_iric\"\n",
    "model_parameters_path = \"/home/ebutz/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
    "\n",
    "# Ontology\n",
    "ontology_path = \"/home/ebutz/ESL2024/data/go-basic.json.gz\"\n",
    "check_dicts = False\n",
    "\n",
    "# ------------- Cuda ------------- #\n",
    "\n",
    "print(\"\\nCuda check...\")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# ------------- Loading datas ------------- #\n",
    "\n",
    "print(\"\\nLoading iric...\")\n",
    "mapped_iric = pd.read_pickle(mapped_iric_path)\n",
    "print(mapped_iric.head())\n",
    "print('mapped_alt_tails type :', type(mapped_iric.iloc[0]['mapped_alt_tails']))\n",
    "\n",
    "GO_to_map = mapped_iric.set_index('object')['mapped_object'].to_dict()\n",
    "map_to_GO = {value: key for key, value in GO_to_map.items()}\n",
    "\n",
    "# if check_dicts:\n",
    "#     looks_ok: bool = True\n",
    "#     for i in tqdm(range(len(list(mapped_iric['object']))), desc = \"Checking GO to MAP dict\"):\n",
    "#         if GO_to_map[mapped_iric['object'][i]]!=mapped_iric['mapped_object'][i] :\n",
    "#             looks_ok = False\n",
    "#     print('GO - Mapping dicts looks ok :', looks_ok)\n",
    "\n",
    "with open(altails_dict_path, 'rb') as handle:\n",
    "    mapped_alt_tails = pickle.load(handle)\n",
    "print(\"Alternative tails dict (first key-value pair):\", list(mapped_alt_tails.items())[0])\n",
    "\n",
    "# # ------------- Making datasets ------------- #\n",
    "\n",
    "print(\"\\nMaking datasets...\")\n",
    "# Edges index :\n",
    "heads = list(mapped_iric['mapped_subject'])\n",
    "tails = list(mapped_iric['mapped_object'])\n",
    "edge_index = torch.tensor([heads,tails], dtype=torch.long)\n",
    "# edges attributes :\n",
    "edge_attributes = torch.tensor(mapped_iric['mapped_predicate'])\n",
    "\n",
    "\n",
    "iric_pyg = Data(\n",
    "                num_nodes = len(set(mapped_iric['object']).union(set(mapped_iric['subject']))),\n",
    "                edge_index = edge_index,\n",
    "                edge_attr = edge_attributes\n",
    "                )\n",
    "\n",
    "print(iric_pyg)\n",
    "\n",
    "print(\"\\nDataset looks valid :\",iric_pyg.validate(raise_on_error=True))\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "                            num_val = 0.1,\n",
    "                            num_test = 0.1,\n",
    "                            is_undirected = False,\n",
    "                            add_negative_train_samples = False,\n",
    "                            )\n",
    "\n",
    "train_set, val_set, test_set = transform(iric_pyg)\n",
    "print('Dataset splits look valid (train, val, test):',train_set.validate(raise_on_error = True),\n",
    "                                                      val_set.validate(raise_on_error   = True),\n",
    "                                                      test_set.validate(raise_on_error  = True))\n",
    "\n",
    "# ------------- Loading ontology ------------- #\n",
    "\n",
    "# print(\"\\nLoading ontology...\")\n",
    "# nxo = from_file(ontology_path)\n",
    "# nxo.freeze()\n",
    "# pwc.nxo              = nxo\n",
    "\n",
    "# ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "pwc.map_to_GO        = map_to_GO\n",
    "pwc.mapped_alt_tails = mapped_alt_tails\n",
    "pwc.device           = device\n",
    "\n",
    "\n",
    "print('All done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc.map_to_GO        = map_to_GO\n",
    "pwc.mapped_alt_tails = mapped_alt_tails\n",
    "pwc.device           = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 9642,  2583, 14253,  ..., 15481,  5034, 13213]), tensor([0, 0, 0,  ..., 0, 0, 0]), tensor([17406,  5748,  4763,  ..., 24599, 13425, 10747]))\n",
      "added additional negatives!\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8192])\n",
      "torch.Size([8192])\n",
      "torch.Size([8192])\n",
      "(tensor([ 9642,  2583, 14253,  ..., 15481,  5034, 13213]), tensor([0, 0, 0,  ..., 0, 0, 0]), tensor([17406,  5748,  4763,  ..., 24599, 13425, 10747]))\n",
      "(tensor([ 9642,  2583, 14253,  ..., 15481,  5034, 13213]), tensor([0, 0, 0,  ..., 0, 0, 0]), tensor([28205, 14535, 11938,  ..., 27341, 16445, 24599]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.4860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to1 = pwc.tail_only_ComplEx(num_nodes                            = train_set.num_nodes,\n",
    "                               num_relations                     = train_set.edge_index.size()[1],\n",
    "                               hidden_channels                   = hidden_channels,\n",
    "                               additionnal_negative_per_positive = 2\n",
    "                               ).to(device)\n",
    "\n",
    "loader = to1.loader(head_index = test_set.edge_index[0], tail_index = test_set.edge_index[1], rel_type   = test_set.edge_attr,batch_size = batch_size, shuffle    = True)\n",
    "batchy = next(iter(loader))\n",
    "print(batchy)\n",
    "to1.loss(*batchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imports...\n",
      "\n",
      "Cuda check...\n",
      "Device: 'cuda'\n",
      "Could reach GPU : True\n",
      "\n",
      "Loading iric...\n",
      "            subject      predicate      object  mapped_subject  \\\n",
      "0  OsNippo01g010050  gene ontology  GO:0031267           29269   \n",
      "1  OsNippo01g010050  gene ontology  GO:0006886           29269   \n",
      "2  OsNippo01g010050  gene ontology  GO:0005622           29269   \n",
      "3  OsNippo01g010050  gene ontology  GO:0005623           29269   \n",
      "4  OsNippo01g010050  gene ontology  GO:0090630           29269   \n",
      "\n",
      "   mapped_predicate  mapped_object                            mapped_alt_tails  \n",
      "0                 0           8105  [20179, 25142, 11107, 11397, 25543, 24490]  \n",
      "1                 0          20179   [8105, 25142, 11107, 11397, 25543, 24490]  \n",
      "2                 0          25142   [8105, 20179, 11107, 11397, 25543, 24490]  \n",
      "3                 0          11107   [8105, 20179, 25142, 11397, 25543, 24490]  \n",
      "4                 0          11397   [8105, 20179, 25142, 11107, 25543, 24490]  \n",
      "mapped_alt_tails type : <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking GO to MAP dict: 100%|██████████| 169248/169248 [00:01<00:00, 135224.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO - Mapping dicts looks ok : True\n",
      "Alternative tails dict (first key-value pair): ((29269, 0), [11107, 11397, 25543, 8105, 24490, 20179, 25142])\n",
      "\n",
      "Making datasets...\n",
      "Data(edge_index=[2, 169248], edge_attr=[169248], num_nodes=30396)\n",
      "\n",
      "Dataset looks valid : True\n",
      "Dataset splits look valid (train, val, test): True True True\n",
      "\n",
      "Loading ontology...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ------------- Loading ontology ------------- #\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading ontology...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m nxo \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m nxo\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# ------------- Making global variables accessibles to pwc ------------- #\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/nxontology/imports.py:83\u001b[0m, in \u001b[0;36mfrom_file\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(handle: BinaryIO \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m PathLike[AnyStr]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NXOntology[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Read ontology in OBO, OWL, or JSON (OBO Graphs) format via pronto.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        that contains a serialized version of the ontology.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     onto \u001b[38;5;241m=\u001b[39m \u001b[43mProntology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pronto_to_nxontology(onto)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pronto/ontology.py:283\u001b[0m, in \u001b[0;36mOntology.__init__\u001b[0;34m(self, handle, import_depth, timeout, threads)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m BaseParser\u001b[38;5;241m.\u001b[39m__subclasses__():\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_parse(typing\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath), buffer):\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pronto/parsers/obojson.py:18\u001b[0m, in \u001b[0;36mOboJSONParser.parse_from\u001b[0;34m(self, handle, threads)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, handle, threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Load the OBO graph into a syntax tree using fastobo\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mfastobo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompact_ids()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract metadata from the OBO header\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m typechecked\u001b[38;5;241m.\u001b[39mdisabled():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"\\nImports...\")\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# import pickle\n",
    "# import os\n",
    "# import sys\n",
    "# sys.path.append( '/home/ebutz/ESL2024/code/utils' )\n",
    "# import play_with_complex as pwc\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch_geometric\n",
    "# from torch_geometric.nn import ComplEx\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.transforms import RandomLinkSplit\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from nxontology.imports import from_file\n",
    "\n",
    "# # ------------- Paths ------------- #\n",
    "\n",
    "# # Iric\n",
    "# mapped_iric_path  = '/home/ebutz/ESL2024/data/Os_to_GO_iric/altailed_Os_to_GO_iric.pickle'\n",
    "# altails_dict_path = '/home/ebutz/ESL2024/data/Os_to_GO_iric/DICT_altailed_Os_to_GO_iric.pickle'\n",
    "\n",
    "# # Model to train :\n",
    "# hidden_channels = 176\n",
    "# batch_size      = 4096\n",
    "# epochs          = 1000\n",
    "# eval_period     = 4\n",
    "# lin_factor      = 1\n",
    "\n",
    "# use_wandb  = True\n",
    "\n",
    "# params_save_name = f\"PARAMS_ComplEx_6_times_{hidden_channels}_HC_{epochs}_epochs_{batch_size}_BS_on_full_iric\"\n",
    "# model_parameters_path = \"/home/ebutz/ESL2024/data/mapping_datasets_and_model_for_genes_to_phenotypes_iric/\"+params_save_name\n",
    "\n",
    "# # Ontology\n",
    "# ontology_path = \"/home/ebutz/ESL2024/data/go-basic.json.gz\"\n",
    "# check_dicts = True\n",
    "\n",
    "# # ------------- Cuda ------------- #\n",
    "\n",
    "# print(\"\\nCuda check...\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Device: '{device}'\")\n",
    "# print(\"Could reach GPU :\", torch.Tensor([0,1]).to(device).is_cuda)\n",
    "\n",
    "\n",
    "# # ------------- Loading datas ------------- #\n",
    "\n",
    "# print(\"\\nLoading iric...\")\n",
    "# mapped_iric = pd.read_pickle(mapped_iric_path)\n",
    "# print(mapped_iric.head())\n",
    "# print('mapped_alt_tails type :', type(mapped_iric.iloc[0]['mapped_alt_tails']))\n",
    "\n",
    "# GO_to_map = mapped_iric.set_index('object')['mapped_object'].to_dict()\n",
    "# map_to_GO = {value: key for key, value in GO_to_map.items()}\n",
    "\n",
    "# if check_dicts:\n",
    "#     looks_ok: bool = True\n",
    "#     for i in tqdm(range(len(list(mapped_iric['object']))), desc = \"Checking GO to MAP dict\"):\n",
    "#         if GO_to_map[mapped_iric['object'][i]]!=mapped_iric['mapped_object'][i] :\n",
    "#             looks_ok = False\n",
    "#     print('GO - Mapping dicts looks ok :', looks_ok)\n",
    "\n",
    "# with open(altails_dict_path, 'rb') as handle:\n",
    "#     mapped_alt_tails = pickle.load(handle)\n",
    "# print(\"Alternative tails dict (first key-value pair):\", list(mapped_alt_tails.items())[0])\n",
    "\n",
    "# # # ------------- Making datasets ------------- #\n",
    "\n",
    "# print(\"\\nMaking datasets...\")\n",
    "# # Edges index :\n",
    "# heads = list(mapped_iric['mapped_subject'])\n",
    "# tails = list(mapped_iric['mapped_object'])\n",
    "# edge_index = torch.tensor([heads,tails], dtype=torch.long)\n",
    "# # edges attributes :\n",
    "# edge_attributes = torch.tensor(mapped_iric['mapped_predicate'])\n",
    "\n",
    "\n",
    "# iric_pyg = Data(\n",
    "#                 num_nodes = len(set(mapped_iric['object']).union(set(mapped_iric['subject']))),\n",
    "#                 edge_index = edge_index,\n",
    "#                 edge_attr = edge_attributes\n",
    "#                 )\n",
    "\n",
    "# print(iric_pyg)\n",
    "\n",
    "# print(\"\\nDataset looks valid :\",iric_pyg.validate(raise_on_error=True))\n",
    "\n",
    "# transform = RandomLinkSplit(\n",
    "#                             num_val = 0.1,\n",
    "#                             num_test = 0.1,\n",
    "#                             is_undirected = False,\n",
    "#                             add_negative_train_samples = False,\n",
    "#                             )\n",
    "\n",
    "# train_set, val_set, test_set = transform(iric_pyg)\n",
    "# print('Dataset splits look valid (train, val, test):',train_set.validate(raise_on_error = True),\n",
    "#                                                       val_set.validate(raise_on_error   = True),\n",
    "#                                                       test_set.validate(raise_on_error  = True))\n",
    "\n",
    "# # ------------- Loading ontology ------------- #\n",
    "\n",
    "# print(\"\\nLoading ontology...\")\n",
    "# nxo = from_file(ontology_path)\n",
    "# nxo.freeze()\n",
    "\n",
    "# # ------------- Making global variables accessibles to pwc ------------- #\n",
    "\n",
    "# pwc.map_to_GO        = map_to_GO\n",
    "# pwc.nxo              = nxo\n",
    "# pwc.mapped_alt_tails = mapped_alt_tails\n",
    "# pwc.device           = device\n",
    "\n",
    "\n",
    "# epochs = 2\n",
    "# eval_period = 1\n",
    "# # ------------- Init model ------------- #\n",
    "# # 250, 15, 75, 15, 176, 250, 350\n",
    "# for hidden_channels in [5]:\n",
    "#     xp_name = f\"Lin VS Baseline with {hidden_channels}*6 HC on {epochs} epochs\"\n",
    "\n",
    "#     pwc.train_model(model_name='tail_only_ComplEx', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "    \n",
    "#     pwc.train_model(model_name='ComplEx_L_labels', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "    \n",
    "#     pwc.train_model(model_name='ComplEx_L_FRL_labels', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "    \n",
    "#     pwc.train_model(model_name='ComplEx_FRL_U_labels', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "\n",
    "#     pwc.train_model(model_name='ComplEx_UGN_labels', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "    \n",
    "#     pwc.train_model(model_name='ComplEx_LGN_labels', hidden_channels_list=[hidden_channels], epochs=epochs, eval_period=eval_period,\n",
    "#                 device='cpu', use_wandb=True, xp_name=f'Lin or noise with {hidden_channels} HC',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "    \n",
    "\n",
    "# print('Finished !')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwc.train_model(model_name='tail_only_ComplEx', hidden_channels_list=[1000], epochs=150, eval_period=10,\n",
    "#                 device='cpu', use_wandb=True, xp_name='1000 HC !',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "\n",
    "# pwc.train_model(model_name='ComplEx_LGN_labels', hidden_channels_list=[1000], epochs=150, eval_period=5,\n",
    "#                 device='cpu', use_wandb=True, xp_name='1000 HC !',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "\n",
    "# pwc.train_model(model_name='ComplEx_GN_U_labels', hidden_channels_list=[1000], epochs=150, eval_period=5,\n",
    "#                 device='cpu', use_wandb=True, xp_name='1000 HC !',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "\n",
    "# pwc.train_model(model_name='ComplEx_L_labels', hidden_channels_list=[1000], epochs=150, eval_period=5,\n",
    "#                 device='cpu', use_wandb=True, xp_name='1000 HC !',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)\n",
    "\n",
    "\n",
    "\n",
    "# pwc.train_model(model_name='ComplEx_UGN_labels', hidden_channels_list=[1000], epochs=150, eval_period=5,\n",
    "#                 device='cpu', use_wandb=True, xp_name='1000 HC !',\n",
    "#                 train_set=train_set, test_set=test_set,\n",
    "#                 file_import = pwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3]\n",
    "l.append(4)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_channels = 15\n",
    "# xp_name = \"Full randomised labels with 15*6 HC\"\n",
    "\n",
    "# GN_U = pwc.ComplEx_GN_U_labels(num_nodes       = train_set.num_nodes,\n",
    "#                                num_relations   = train_set.edge_index.size()[1],\n",
    "#                                hidden_channels = hidden_channels,\n",
    "#                                ).to(device)\n",
    "\n",
    "# loader = GN_U.loader(\n",
    "#                           head_index = test_set.edge_index[0],\n",
    "#                           tail_index = test_set.edge_index[1],\n",
    "#                           rel_type   = test_set.edge_attr,\n",
    "#                           batch_size = batch_size,\n",
    "#                           shuffle    = True)\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# print(batch)\n",
    "# GN_U.test(*batch, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GN_U = pwc.ComplEx_GN_U_labels(num_nodes       = train_set.num_nodes,\n",
    "#                                num_relations   = train_set.edge_index.size()[1],\n",
    "#                                hidden_channels = hidden_channels,\n",
    "#                                ).to(device)\n",
    "# pwc.train_and_test_complex(model      = GN_U,\n",
    "#                             train_data = train_set,\n",
    "#                             test_data  = test_set,\n",
    "#                             device     = device,\n",
    "#                             use_wandb  = False,\n",
    "#                             xp_name    = xp_name,\n",
    "#                             run_name   = 'GN_U_labels',\n",
    "#                             eval_period= eval_period,\n",
    "#                             epochs = epochs\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 936437.60it/s]\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test.\n",
      "Model type : <class 'play_with_complex.ComplEx_LGN_labels'>\n",
      "Hidden channels : 6 * 176\n",
      "Epochs : 200\n",
      "Eval period : 2\n",
      "Use WanDB : False\n",
      "Device : cpu Could reach GPU : False\n",
      "Evaluation each 2 epochs.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'nxontology' has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m LGN \u001b[38;5;241m=\u001b[39m pwc\u001b[38;5;241m.\u001b[39mComplEx_LGN_labels(num_nodes       \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39mnum_nodes,\n\u001b[1;32m      2\u001b[0m                                  num_relations   \u001b[38;5;241m=\u001b[39m train_set\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                  hidden_channels \u001b[38;5;241m=\u001b[39m hidden_channels,\n\u001b[1;32m      4\u001b[0m                                  )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mpwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_test_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLGN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mxp_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLGN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43meval_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:173\u001b[0m, in \u001b[0;36mtrain_and_test_complex\u001b[0;34m(model, train_data, test_data, xp_name, run_name, use_wandb, epochs, batch_size, eval_period, reset_parameters, params_save_path, device, dataset_name, k)\u001b[0m\n\u001b[1;32m    171\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m get_test_loss(\n\u001b[1;32m    178\u001b[0m                         loader \u001b[38;5;241m=\u001b[39m test_loader,\n\u001b[1;32m    179\u001b[0m                         model  \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m    180\u001b[0m                         device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    182\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, model, optimizer, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m head_index, rel_type, tail_index \u001b[38;5;241m=\u001b[39m head_index\u001b[38;5;241m.\u001b[39mto(device), rel_type\u001b[38;5;241m.\u001b[39mto(device), tail_index\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:502\u001b[0m, in \u001b[0;36mComplEx_LGN_labels.loss\u001b[0;34m(self, head_index, rel_type, tail_index)\u001b[0m\n\u001b[1;32m    499\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pos_score, neg_score], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    501\u001b[0m pos_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(pos_score)\n\u001b[0;32m--> 502\u001b[0m lin_neg_target \u001b[38;5;241m=\u001b[39m \u001b[43mlin_sims_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    504\u001b[0m gn_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    505\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pos_target, lin_neg_target], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:272\u001b[0m, in \u001b[0;36mlin_sims_for_batch\u001b[0;34m(term1, term2)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlin_sims_for_batch\u001b[39m(term1: torch\u001b[38;5;241m.\u001b[39mTensor, term2: torch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    268\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mstack((term1, term2)),\n\u001b[1;32m    269\u001b[0m                                          \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m                         )\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlin_sim_on_mapped_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped_term1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mmapped_term2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/core/frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10360\u001b[0m )\n\u001b[0;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyg/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:272\u001b[0m, in \u001b[0;36mlin_sims_for_batch.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlin_sims_for_batch\u001b[39m(term1: torch\u001b[38;5;241m.\u001b[39mTensor, term2: torch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    268\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mstack((term1, term2)),\n\u001b[1;32m    269\u001b[0m                                          \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m                         )\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(batch\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row : \u001b[43mlin_sim_on_mapped_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped_term1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mmapped_term2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    275\u001b[0m                                     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/ESL2024/code/utils/play_with_complex.py:233\u001b[0m, in \u001b[0;36mlin_sim_on_mapped_terms\u001b[0;34m(mapped_term1, mapped_term2)\u001b[0m\n\u001b[1;32m    231\u001b[0m term1 \u001b[38;5;241m=\u001b[39m map_to_GO[mapped_term1]\n\u001b[1;32m    232\u001b[0m term2 \u001b[38;5;241m=\u001b[39m map_to_GO[mapped_term2]\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (term1 \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnxo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39m_node \u001b[38;5;129;01mand\u001b[39;00m term2 \u001b[38;5;129;01min\u001b[39;00m nxo\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_node):\n\u001b[1;32m    234\u001b[0m     sim \u001b[38;5;241m=\u001b[39m nxo\u001b[38;5;241m.\u001b[39msimilarity(term1, term2)\u001b[38;5;241m.\u001b[39mlin \n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sim\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nxontology' has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "LGN = pwc.ComplEx_LGN_labels(num_nodes       = train_set.num_nodes,\n",
    "                                 num_relations   = train_set.edge_index.size()[1],\n",
    "                                 hidden_channels = hidden_channels,\n",
    "                                 ).to(device)\n",
    "pwc.train_and_test_complex(model       = LGN,\n",
    "                            train_data = train_set,\n",
    "                            test_data  = test_set,\n",
    "                            device     = device,\n",
    "                            use_wandb  = False,\n",
    "                            xp_name    = 'nay',\n",
    "                            run_name   = 'LGN',\n",
    "                            eval_period= eval_period,\n",
    "                            epochs = epochs\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 15\n",
    "xp_name = \"Full randomised labels with 15*6 HC\"\n",
    "\n",
    "\n",
    "\n",
    "# BLS_U = pwc.ComplEx_BLS_U_labels(num_nodes       = train_set.num_nodes,\n",
    "#                                  num_relations   = train_set.edge_index.size()[1],\n",
    "#                                  hidden_channels = hidden_channels,\n",
    "#                                  ).to(device)\n",
    "# pwc.train_and_test_complex(model       = BLS_U,\n",
    "#                             train_data = train_set,\n",
    "#                             test_data  = test_set,\n",
    "#                             device     = device,\n",
    "#                             use_wandb  = False,\n",
    "#                             xp_name    = xp_name,\n",
    "#                             run_name   = 'BLS_U_labels',\n",
    "#                             eval_period= eval_period,\n",
    "#                             epochs = epochs\n",
    "#                             )\n",
    "\n",
    "# FRBLS_U = pwc.ComplEx_FRBLS_U_labels(num_nodes      = train_set.num_nodes,\n",
    "#                                     num_relations   = train_set.edge_index.size()[1],\n",
    "#                                     hidden_channels = hidden_channels,\n",
    "#                                     ).to(device)\n",
    "# pwc.train_and_test_complex(model       = FRBLS_U,\n",
    "#                             train_data = train_set,\n",
    "#                             test_data  = test_set,\n",
    "#                             device     = device,\n",
    "#                             use_wandb  = True,\n",
    "#                             xp_name    = xp_name,\n",
    "#                             run_name   = 'FRBLS_U_labels',\n",
    "#                             eval_period= eval_period,\n",
    "#                             epochs = epochs\n",
    "#                             )\n",
    "\n",
    "\n",
    "# print('Finished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1, 12, 23])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0,0,12,1,23,1])\n",
    "print(a.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
